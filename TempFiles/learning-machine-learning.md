# 目录
- 决策树、bagging、boosting、adaboost
- 手推逻辑回归
- k-means的k选择、二分k-means
- PCA与word embedding
- SVM
- SVD与推荐

---

### 线性模型

- 基于均方误差最小化进行模型求解的方法成为“最小二乘法”，在线性回归中，最小二乘法就是找到一条直线，使所有样本到直线上的欧氏距离之和最小。
- LDA：线性判别分析

- 线性回归是指使用一条线，尽量拟合所有的数据点，也就是形成
    >y = w*x + b

    对w和b进行求解，即找到这么一条曲线。

- 对于分类任务，可以将线性回归得到的y_i输入到sigmoid函数中，根据输出的0or1完成分类。

- 对于多分类任务，可以有以下三种方式：

    假设有m个样本，N个类别

    方式 | 训练 | 测试 |
    ---| ---| ---|
    One vs One | 将N个类别两两配对，产生N（N-1）/2个分类任务 | 新样本同时给所有分类器进行投票
    One vs Rest | 某一类作为正例，其他的类作为反例 | 若仅有一个分类器预测为正，那么就是这类；如果多个预测为正，那么根据置信度考虑
    Many vs Many | 若干个类作为正例，若干个类作为反例 | **没说**

- 对于类别不平衡的问题，有以下方法

    方法 | 解释
    ---| ---|
    欠采样 | 去除数量多的那个类使数据平衡
    过采样 | 增加数量少的类的样本数
    阈值移动 | 有点复杂，见西瓜书P67，公式3.48
    
---

### 决策树

- 信息量：

    即对信息的度量，越小概率的事件信息量越大（指在某个概率分布之下，某个概率值对应的信息量的公式）
    >refer: http://mamicode.com/info-detail-2228412.html


    假设不相关事件x, y，则：

        h(x,y) = h(x) + h(y)
        p(x,y) = p(x)*p(y)
        →
        h(x) = -log2p(x)

- 信息熵：

    信息量考虑事件发生以后带来的信息数量，信息熵考虑事件发生前可能产生的信息量的期望。(所有可能取值的信息量的期望就称为熵)
    
    refer: 

    - https://www.cnblogs.com/llhthinker/p/7287029.html
    - https://zhuanlan.zhihu.com/p/26486223

- 条件熵：
    
    条件熵中X也是一个变量，意思是在一个变量X的条件下（变量X的每个值都会取），另一个变量Y熵对X的期望。
    
    refer:
    - https://zhuanlan.zhihu.com/p/26551798

- 信息增益：
    
    信息熵 — 条件熵，即在一个条件下，信息复杂度（不确定性）减少的程度。信息增益与互信息实际上是一件事情。

    我们把特征A对数据集D的信息增益记为 G(D, A) ，定义集合D的信息熵为 (D)，定义集合D在特征A条件下的条件熵为 (D|A)，那么信息增益为：

        G(D, A) = H(D) — H(D|A)
        
    >即原本的信息熵减去给定信息A后信息熵，其差值即为信息增益
