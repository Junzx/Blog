{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/kiddochan/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/jquery.fancybox.js","path":"fancybox/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/CoveredByYourGrace-webfont.woff2","path":"font/CoveredByYourGrace-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/FontAwesome.otf","path":"font/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/ReenieBeanie.eot","path":"font/ReenieBeanie.eot","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/ReenieBeanie.ttf","path":"font/ReenieBeanie.ttf","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/ReenieBeanie.woff","path":"font/ReenieBeanie.woff","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/ReenieBeanie.woff2","path":"font/ReenieBeanie.woff2","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.eot","path":"font/chalkboardse-bold-webfont.eot","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.woff","path":"font/chalkboardse-bold-webfont.woff","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.woff2","path":"font/chalkboardse-bold-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/coveredbyyourgrace-webfont.eot","path":"font/coveredbyyourgrace-webfont.eot","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/coveredbyyourgrace-webfont.ttf","path":"font/coveredbyyourgrace-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/coveredbyyourgrace-webfont.woff","path":"font/coveredbyyourgrace-webfont.woff","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.eot","path":"font/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.woff","path":"font/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontdiao.eot","path":"font/fontdiao.eot","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontdiao.ttf","path":"font/fontdiao.ttf","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontdiao.woff","path":"font/fontdiao.woff","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontdiao.woff2","path":"font/fontdiao.woff2","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/author.JPG","path":"img/author.JPG","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/cc-by-nc-nd.svg","path":"img/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/cc-by-nc-sa.svg","path":"img/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/cc-by-nc.svg","path":"img/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/cc-by-nd.svg","path":"img/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/cc-by-sa.svg","path":"img/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/cc-by.svg","path":"img/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/cc-zero.svg","path":"img/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/icon.ico","path":"img/icon.ico","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/scrollup.png","path":"img/scrollup.png","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/js/gallery.js","path":"js/gallery.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/js/jquery.qrcode-0.12.0.min.js","path":"js/jquery.qrcode-0.12.0.min.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/js/totop.js","path":"js/totop.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.ttf","path":"font/chalkboardse-bold-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.ttf","path":"font/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/js/jquery-2.0.3.min.js","path":"js/jquery-2.0.3.min.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/helpers/fancybox_buttons.png","path":"fancybox/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-buttons.css","path":"fancybox/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-buttons.js","path":"fancybox/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-media.js","path":"fancybox/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-thumbs.css","path":"fancybox/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-thumbs.js","path":"fancybox/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.svg","path":"font/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/banner.jpg","path":"img/banner.jpg","modified":1,"renderable":1},{"_id":"themes/kiddochan/source/img/IMG_2984.JPG","path":"img/IMG_2984.JPG","modified":1,"renderable":1},{"_id":"source/about/index/4.png","path":"about/index/4.png","modified":1,"renderable":0},{"_id":"source/about/index/IMG_8752.JPG","path":"about/index/IMG_8752.JPG","modified":1,"renderable":0},{"_id":"themes/kiddochan/source/img/IMG_8752.JPG","path":"img/IMG_8752.JPG","modified":1,"renderable":1}],"Cache":[{"_id":"source/boostnote.json","hash":"1d8c7454808e612994ea23361819e2f544c21b99","modified":1548133164980},{"_id":"themes/kiddochan/.gitignore","hash":"7d65523f2a5afb69d76824dd1dfa62a34faa3197","modified":1548133164988},{"_id":"themes/kiddochan/LICENSE","hash":"88d02ae601d76b0b7e8335b5162d4130343783c8","modified":1548133164988},{"_id":"themes/kiddochan/README.md","hash":"1d3a05b447d37448e707073aa3c751a99ccd67c4","modified":1548133164988},{"_id":"themes/kiddochan/README_zh.md","hash":"b880e65dc41893f3f27a67f3cdf20717d42d57a5","modified":1548133164988},{"_id":"themes/kiddochan/_config.yml","hash":"ed9922ea2371be34ed8f09ed4ec40aecb1c1b8ab","modified":1548133164988},{"_id":"source/_posts/2018-work-schedule.md","hash":"48d770791f10e08995c0c512e4529080f32f02db","modified":1548133164924},{"_id":"source/_posts/2019-work-schedule.md","hash":"ced20e3c90d4fcda292b5de0f430b3c21dd32d54","modified":1548133164924},{"_id":"source/_posts/Baidu_tieba_spider.md","hash":"852f0588026635015dd2601c6c9f112af66feb50","modified":1548133164924},{"_id":"source/_posts/CorefNLP相关-资料&笔记.md","hash":"e2c2d0706e14f3d59d66a24e59a87bd8d1d2bcdf","modified":1548133164924},{"_id":"source/_posts/Java学习笔记.md","hash":"38edaa119de3b0f9b5cecc35c0152837bed8acc5","modified":1548133164924},{"_id":"source/_posts/algorithm-learning-sort.md","hash":"8898bb338f0504ceb77ffb8ea60d0f4eeba17ad6","modified":1548133164924},{"_id":"source/_posts/cnn.md","hash":"6950d2460f19e17b2461abccf23314149021ff34","modified":1548133164928},{"_id":"source/_posts/conll数据格式.md","hash":"b81e01d8a234c37b6a6e58f83230b2cbcd6aba20","modified":1548133164932},{"_id":"source/_posts/end2end-coreference-resolution.md","hash":"cf886b94ee1dc66b71e51348a796acb0dc375dfa","modified":1548133164932},{"_id":"source/_posts/how-to-use-python-to-build-markdown-calc.md","hash":"48989799155c356ac5a4d4cb82ab46428b64582b","modified":1548133164952},{"_id":"source/_posts/learning-tensorflow-linear-regression.md","hash":"b82dbaf3c367a1da9706366a1156031f34c0d01d","modified":1548133164952},{"_id":"source/_posts/python操作excel.md","hash":"481e1efd47f6ea6de2decafe9ece1f9d024609e5","modified":1548133164956},{"_id":"source/_posts/spacy-leaning.md","hash":"3736bd545c152f80b81d4315df7bb38d77ef16c3","modified":1548133164956},{"_id":"source/_posts/tree.md","hash":"b0b395f347c14745f1216e7513fe76ddf41bb392","modified":1548133164956},{"_id":"source/_posts/year-plan.md","hash":"46539a706437eeb4d101335d1036d5f68882bd4e","modified":1548133164956},{"_id":"source/_posts/组会分享.md","hash":"0dc5b9bd269e2b321dd384da042073cc5f2effc6","modified":1548133164956},{"_id":"source/_posts/论文笔记.md","hash":"f73afdcc3ed027d679e91c210728ab05429d6471","modified":1548133164960},{"_id":"source/about/index.md","hash":"b77d30006134eacb1b53487ae61a6ea028ad822f","modified":1548133164960},{"_id":"themes/kiddochan/languages/default.yml","hash":"eea72d6138497287c0b3f4bd93e4f6f62b7aff37","modified":1548133164988},{"_id":"themes/kiddochan/languages/zh-CN.yml","hash":"ac510cb258b0d08aafd05e8f9f6eec597e12eb9b","modified":1548133164988},{"_id":"themes/kiddochan/languages/zh-TW.yml","hash":"cd2a252be1aa3f8830b76e63b6bea9290b14c2b2","modified":1548133164988},{"_id":"themes/kiddochan/layout/archive.ejs","hash":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1548133164988},{"_id":"themes/kiddochan/layout/category.ejs","hash":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1548133164988},{"_id":"themes/kiddochan/layout/index.ejs","hash":"75cef2172c286994af412e11ab7f4f5a0daaf1f5","modified":1548133164988},{"_id":"themes/kiddochan/layout/layout.ejs","hash":"5b4289a4526899809b9c2facea535367ff51ba2b","modified":1548133164988},{"_id":"themes/kiddochan/layout/page.ejs","hash":"bd6bbf2ea8e183bd835867ff617dc6366b56748c","modified":1548133164988},{"_id":"themes/kiddochan/layout/post.ejs","hash":"3114134775bdde5a83cf14feb019606fa2b2b2be","modified":1548133164988},{"_id":"themes/kiddochan/layout/tag.ejs","hash":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1548133164988},{"_id":"themes/kiddochan/scripts/fancybox.js","hash":"aa411cd072399df1ddc8e2181a3204678a5177d9","modified":1548133165004},{"_id":"source/_posts/2018-work-schedule/1.JPG","hash":"5a4423d2a7c1fb20d16c3585aa79d51f3468f15e","modified":1548133164924},{"_id":"source/_posts/Baidu_tieba_spider/1.png","hash":"af6ab2f8de6a92aea282cbe250cd55e321584bff","modified":1548133164924},{"_id":"source/_posts/Baidu_tieba_spider/psb2.jpg","hash":"d87c984ce6b5fffbd86be703e4a7f08346cb8977","modified":1548133164924},{"_id":"source/_posts/algorithm-learning-sort/1.JPG","hash":"5a4423d2a7c1fb20d16c3585aa79d51f3468f15e","modified":1548133164924},{"_id":"source/_posts/cnn/data.JPG","hash":"4d420eed35d65f46fa669128b44f16f3092dcb30","modified":1548133164928},{"_id":"source/_posts/cnn/vocab.JPG","hash":"c3d505f976c88161219f92629edd471241e5e486","modified":1548133164928},{"_id":"source/_posts/how-to-use-python-to-build-markdown-calc/1.png","hash":"af6ab2f8de6a92aea282cbe250cd55e321584bff","modified":1548133164952},{"_id":"source/_posts/how-to-use-python-to-build-markdown-calc/2.JPG","hash":"338f76229ac31cee03c0d08472c80adf86cd8ef7","modified":1548133164952},{"_id":"source/_posts/images/1.png","hash":"af6ab2f8de6a92aea282cbe250cd55e321584bff","modified":1548133164952},{"_id":"source/_posts/images/psb2.jpg","hash":"d87c984ce6b5fffbd86be703e4a7f08346cb8977","modified":1548133164952},{"_id":"source/_posts/learning-tensorflow-linear-regression/1.png","hash":"7daa3b3cd8ea9287ce3137899434169f5123188e","modified":1548133164956},{"_id":"source/_posts/learning-tensorflow-linear-regression/2.JPG","hash":"7e45ede4125c790b0a0a8e471e488bf86cb3c409","modified":1548133164956},{"_id":"source/_posts/learning-tensorflow-linear-regression/3.png","hash":"5af1b8ecdbb528b4be13453a5c2e1ba632c1dff0","modified":1548133164956},{"_id":"source/_posts/组会分享/clipboard.png","hash":"9b8590d42f3239f082f3b00281597d1f22e7a3a6","modified":1548133164956},{"_id":"source/_posts/组会分享/clipboard_1.png","hash":"8ae5662ef01488516cfc8ddbf66db34cac7e83ea","modified":1548133164956},{"_id":"source/_posts/组会分享/clipboard_2.png","hash":"41056b8308440132658a5b9dc836fe0275d08b2b","modified":1548133164956},{"_id":"source/_posts/组会分享/clipboard_3.png","hash":"0eb0c2a36d968398fc241701f12e171c2cde0a76","modified":1548133164956},{"_id":"themes/kiddochan/layout/_partial/after_footer.ejs","hash":"c703b0c25139b8a5f8f9d24a334a07905e2b7987","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/analytics.ejs","hash":"697601996220fe0a0f9cd628be67dec3c86ae2aa","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/archive.ejs","hash":"2c7395e7563fe016521712a645c28a13f952d52a","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/article.ejs","hash":"261ecacb8456f4cb972632b6a9103860fa63b9a3","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/article_row.ejs","hash":"4cb855d91ece7f67b2ca0992fffa55472d0b9c93","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/categories.ejs","hash":"8a52d0344d5bce1925cf586ed73c11192925209b","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/footer.ejs","hash":"81413d48505abd3a378089fb2d73228ce515099a","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/head.ejs","hash":"761941be4922cd3c177c8130296b909bf7db5c09","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/header.ejs","hash":"18515612344ff048b9372b91b7eef6f3b143801f","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/mathjax.ejs","hash":"d42994ac696f52ba99c1cbac382cd76d5b04a3e8","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/pagination.ejs","hash":"6146ac37dfb4f8613090bc52b3fc8cfa911a186a","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/search.ejs","hash":"17ab7ff6d3ff86ccbbfb076bd3153c4e25f2ba99","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/sidebar.ejs","hash":"6fe72121d8db4ab40fbedb945ca45755a2ce8421","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/tags.ejs","hash":"b33b2b5d08f1d53a8de25a95f660f7f1cea7b3cb","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/tinysou_search.ejs","hash":"06ecddc8a9d40b480fe2e958af1dab857a9d5441","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/totop.ejs","hash":"bea5bb7cb9350b8af7d97a8d223af63a5b30ab78","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/archive.ejs","hash":"39ea6b7888406fbd1b4cf236ebd718e881493374","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/category.ejs","hash":"c1fae96b5053da021bcc04ab2ce5c2c8d30de8a2","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/douban.ejs","hash":"e3820c36169e88663e6c9177666b2904c1ce47e6","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/github-card.ejs","hash":"5c759b6ea214bac56a393247de27e67ce73fb33f","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/links.ejs","hash":"e49868063439c2092cdf9a8ec82cc295b0e42f66","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/rss.ejs","hash":"0a4b5f2a2e36a1d504fe2e7c6c8372cbb4628aab","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/tag.ejs","hash":"7e82ad9c916b9ce871b2f65ce8f283c5ba47947b","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/tagcloud.ejs","hash":"10a1001189d5c28ce6d42494563b9637c302b454","modified":1548133164988},{"_id":"themes/kiddochan/layout/_widget/weibo.ejs","hash":"a31c2b223d0feb2a227e203cac9e5d13b7d328a8","modified":1548133164988},{"_id":"themes/kiddochan/screenshots/example1.png","hash":"879b27b28950ce225268e46087b7250da3d8e0e6","modified":1548133164992},{"_id":"themes/kiddochan/screenshots/example2.png","hash":"bbacebf8609ada4ed7ed5304c08cfb7aa817dd24","modified":1548133164992},{"_id":"themes/kiddochan/screenshots/example3.png","hash":"52456d3201b730cae88a2fb8e38c93c63a29bf29","modified":1548133164992},{"_id":"themes/kiddochan/screenshots/example5.png","hash":"ca735d47f85f8050d47f9f57312bd00dc1b4ab95","modified":1548133164996},{"_id":"themes/kiddochan/screenshots/kiddochan_pad2.png","hash":"dc290a46806116589db57d16e32d11f396ec8c8a","modified":1548133165000},{"_id":"themes/kiddochan/screenshots/kiddochan_phone1.png","hash":"401e3d026a885880ed1b1e2581f09ad47bf5132f","modified":1548133165004},{"_id":"themes/kiddochan/screenshots/kiddochan_phone3.png","hash":"e07d3005e426b60c9a7bf63f190d0750a2009e41","modified":1548133165004},{"_id":"themes/kiddochan/source/css/style.styl","hash":"a0a45af186a72ae68979bf26f2a5d0d2303189ca","modified":1548133165004},{"_id":"themes/kiddochan/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1548133165004},{"_id":"themes/kiddochan/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1548133165004},{"_id":"themes/kiddochan/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1548133165008},{"_id":"themes/kiddochan/source/font/CoveredByYourGrace-webfont.woff2","hash":"fe5fce8733d35a38e127d59592c2201e046da478","modified":1548133165008},{"_id":"themes/kiddochan/source/font/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1548133165008},{"_id":"themes/kiddochan/source/font/ReenieBeanie.eot","hash":"b8a6114536ce1cde347339b299924ec33a992396","modified":1548133165008},{"_id":"themes/kiddochan/source/font/ReenieBeanie.ttf","hash":"5d52ff55cbdfb46a293d6bd4ab6f98807942456b","modified":1548133165008},{"_id":"themes/kiddochan/source/font/ReenieBeanie.woff","hash":"95a3376a44b1e208e2d3b4b81c62eac392911d17","modified":1548133165008},{"_id":"themes/kiddochan/source/font/ReenieBeanie.woff2","hash":"d42df9377c1393c3347334772ef38fbaf11fee6f","modified":1548133165008},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.eot","hash":"032c99cb605b8dab399d94baec39ed44bc936bb2","modified":1548133165008},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.woff","hash":"de8c126b031046f309badfed30f166e7b96420e7","modified":1548133165008},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.woff2","hash":"312b79586406e09f4964bdeb9c0969ff9303d9a7","modified":1548133165008},{"_id":"themes/kiddochan/source/font/coveredbyyourgrace-webfont.eot","hash":"a17d0f10534303e40f210c506ebb8703fa23b7de","modified":1548133165008},{"_id":"themes/kiddochan/source/font/coveredbyyourgrace-webfont.ttf","hash":"194ccb4acf77a03dc25bcc174edb266143704fec","modified":1548133165008},{"_id":"themes/kiddochan/source/font/coveredbyyourgrace-webfont.woff","hash":"c6f8dc1a2f6ce914f120e80a876b8fd77b98888e","modified":1548133165008},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.eot","hash":"9c94e02de6099219e219ddc7050053f24799b455","modified":1548133165008},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1548133165012},{"_id":"themes/kiddochan/source/font/fontdiao.eot","hash":"9fd63e687d8fd01f792c46b2c508c847515a2b20","modified":1548133165012},{"_id":"themes/kiddochan/source/font/fontdiao.ttf","hash":"f919729fc0dc7219c6de57fcebf86c54edefd964","modified":1548133165012},{"_id":"themes/kiddochan/source/font/fontdiao.woff","hash":"66db191655c6ee0f8e476910558ba7a291977573","modified":1548133165012},{"_id":"themes/kiddochan/source/font/fontdiao.woff2","hash":"45d7ee083bbc68a386fb95dd7788bf868a22b993","modified":1548133165012},{"_id":"themes/kiddochan/source/img/author.JPG","hash":"4501d2bed52e399bded28759653d1691091e544a","modified":1548133165028},{"_id":"themes/kiddochan/source/img/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1548133165028},{"_id":"themes/kiddochan/source/img/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1548133165028},{"_id":"themes/kiddochan/source/img/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1548133165028},{"_id":"themes/kiddochan/source/img/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1548133165028},{"_id":"themes/kiddochan/source/img/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1548133165028},{"_id":"themes/kiddochan/source/img/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1548133165028},{"_id":"themes/kiddochan/source/img/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1548133165028},{"_id":"themes/kiddochan/source/img/icon.ico","hash":"53fef85a38a122c065b5e8b9510c845184db85b9","modified":1548133165028},{"_id":"themes/kiddochan/source/img/scrollup.png","hash":"2137d4f1739aa8aa3fcb0348c3ddf1e41d62f2e3","modified":1548133165028},{"_id":"themes/kiddochan/source/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1548133165028},{"_id":"themes/kiddochan/source/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1548133165028},{"_id":"themes/kiddochan/source/js/jquery.qrcode-0.12.0.min.js","hash":"57c3987166a26415a71292162690e82c21e315ad","modified":1548133165028},{"_id":"themes/kiddochan/source/js/totop.js","hash":"cad23c5ea7163d1e5c05a0fd3ef9233469da10cb","modified":1548133165028},{"_id":"source/_posts/algorithm-learning-sort/insert-sort-1.jpg","hash":"867fac442a77c6383e831a756783445fa39e81bf","modified":1548133164924},{"_id":"source/_posts/algorithm-learning-sort/select-sort-1.jpg","hash":"21ea0cc4a93aeaa67469effe310e482a4e730cb1","modified":1548133164928},{"_id":"source/_posts/组会分享/clipboard_5.png","hash":"c695a8c991d38ed26271dc3d1d1d169e33a45e55","modified":1548133164960},{"_id":"themes/kiddochan/screenshots/kiddochan_pad.png","hash":"5971989f2caea6bc33a1e058cd78e922051849bd","modified":1548133165000},{"_id":"themes/kiddochan/screenshots/kiddochan_pc2.png","hash":"21c15ce7f244a01e42adfa7434402a08d4c94024","modified":1548133165004},{"_id":"themes/kiddochan/screenshots/kiddochan_phone2.png","hash":"4e2dfd6489e1af94a65d8650a3001c8f2eec03b6","modified":1548133165004},{"_id":"themes/kiddochan/source/font/chalkboardse-bold-webfont.ttf","hash":"8ae80892d452643c69c7740640402f9d97d046d4","modified":1548133165008},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1548133165012},{"_id":"themes/kiddochan/source/js/jquery-2.0.3.min.js","hash":"a0ae3697b0ab8c0e8bd3186c80db42abd6d97a8d","modified":1548133165028},{"_id":"source/_posts/algorithm-learning-sort/insert-sort-2.jpg","hash":"3b0fb85d9ec4bfb30fb32a38e5e3db7dd0b612f3","modified":1548133164928},{"_id":"source/_posts/algorithm-learning-sort/select-sort-2.jpg","hash":"8c7bf5f200ab6b4e1cba2f24ce5650d651d6d512","modified":1548133164928},{"_id":"source/_posts/组会分享/tree.png","hash":"aa5f4c2769757b7a8b2fbec574aa42012f18282f","modified":1548133164960},{"_id":"themes/kiddochan/layout/_partial/post/article.ejs","hash":"b09e3acea7076e1f01dfe0c2295e19951ea09437","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/post/catetags.ejs","hash":"0e37bababc8f4659f5b59a552a946b46d89e4158","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/post/comment.ejs","hash":"c88bc8f5805173920a5fdd7e9234a850e3d8e151","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/post/footer.ejs","hash":"25bc301ce21cfa39195b45537c0cb0e3ec8f9f58","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/post/gallery.ejs","hash":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/post/header.ejs","hash":"36a705942b691abe0d643ea8afa339981b32f6f2","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/post/jiathis.ejs","hash":"d7f5960039ac74924559ab6ba03c64457b8f0966","modified":1548133164988},{"_id":"themes/kiddochan/layout/_partial/post/pagination.ejs","hash":"f4c42f1268f1afaa91f41540b4f6b204933f6438","modified":1548133164988},{"_id":"themes/kiddochan/screenshots/example.png","hash":"1b7d9c48bb97983cbe39da5d5090107fb3368264","modified":1548133164992},{"_id":"themes/kiddochan/source/css/_base/font.styl","hash":"14cb317eca998abcddd3176ed9dbeff27bcd2532","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_base/public.styl","hash":"f016180726019927b9a835ed01e04d153f27a149","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_base/variable.styl","hash":"d36ada15fb697ecd4ac8d87166cd8ba51933be69","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/article.styl","hash":"c69641b4a34a8c62986b335414413dbde26de25e","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/aside.styl","hash":"a899aeacd86f9334489c3bb7921b8872efb7275f","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/duoshuo.styl","hash":"e85f1192283f043115c272a9deb3cb6ced793990","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/footer.styl","hash":"6621ae567131e1ed0d1df9f5966689a9ccbc2e2e","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/gallery.styl","hash":"7246809f4ce3166ec1b259bf475cae1a48e29aad","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/header.styl","hash":"fec1c0feaa1c2cb068e58bb76abd2d6dd2e4105c","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/helper.styl","hash":"1136600932b97534b88465bf05ef313630b2de3d","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/index.styl","hash":"a72ff14effd276015264f870f47ed8f8413bf5d3","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_partial/totop.styl","hash":"96363d7c5aaed5f649667fc0752a62620a67e872","modified":1548133165004},{"_id":"themes/kiddochan/source/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1548133165008},{"_id":"themes/kiddochan/source/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1548133165008},{"_id":"source/_posts/Baidu_tieba_spider/psb.png","hash":"dce74636acaa3068953c4ccd01a8a37219c2e4b1","modified":1548133164924},{"_id":"source/_posts/algorithm-learning-sort/swap-sort-1.jpg","hash":"07c16b72f6b51b035bb39235007d607c403defc4","modified":1548133164928},{"_id":"source/_posts/end2end-coreference-resolution/3.png","hash":"6477f5d7cb808be1f7a1af2d881ae7e65470711a","modified":1548133164936},{"_id":"source/_posts/images/psb.png","hash":"dce74636acaa3068953c4ccd01a8a37219c2e4b1","modified":1548133164952},{"_id":"source/_posts/组会分享/clipboard_4.png","hash":"f549d6682bccdd3f786fd8c8bd6b1b4dea0e3f03","modified":1548133164956},{"_id":"source/_posts/cnn/3-4.png","hash":"653f28a43e8759008889de2e01a27f5197255084","modified":1548133164928},{"_id":"source/_posts/end2end-coreference-resolution/6.png","hash":"1c072e98c507b300c42445366c0b47b74af5a0ff","modified":1548133164944},{"_id":"themes/kiddochan/screenshots/kiddochan_pc.png","hash":"615fa03c4bb7dc3121518f29821ee5c78ea0a5a8","modified":1548133165000},{"_id":"themes/kiddochan/source/css/_base/highlight/highlight.styl","hash":"91b62bfc58390b0d5db782a75be6965ee3665eb3","modified":1548133165004},{"_id":"themes/kiddochan/source/css/_base/highlight/theme.styl","hash":"e3a59bd427ba37a54ead9eeba9a5356b3f720a48","modified":1548133165004},{"_id":"themes/kiddochan/source/font/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1548133165012},{"_id":"themes/kiddochan/source/img/banner.jpg","hash":"5104860c4f8b2e84ef734ba6c37fe7a288bf0d74","modified":1548133165028},{"_id":"source/_posts/end2end-coreference-resolution/5.png","hash":"14ba531085def0d9f97d52d8579dfc78e781d464","modified":1548133164944},{"_id":"source/_posts/end2end-coreference-resolution/7.png","hash":"f581e13adda87494c686a28611ddbed65dd1be44","modified":1548133164948},{"_id":"themes/kiddochan/screenshots/example4.png","hash":"97b1b828adf1aa5f083a2865405e090ec8a6cc92","modified":1548133164996},{"_id":"themes/kiddochan/screenshots/google_custom_search.png","hash":"9c6e624eb2343004a0ba17c7cc2a0638cc770aec","modified":1548133164996},{"_id":"source/_posts/tree/1.jpg","hash":"c78ad854f331c6aa9a2b8cd0a4f28506ee7b4d5c","modified":1548133164956},{"_id":"source/_posts/end2end-coreference-resolution/9.png","hash":"ad7cbef5664f3c11e4323ba280520f9114ca45ea","modified":1548133164952},{"_id":"source/_posts/组会分享/mention_and_entity.png","hash":"e98cdc1a8e0d748144ee64bd07159842ea31b2c6","modified":1548133164960},{"_id":"source/_posts/end2end-coreference-resolution/2.png","hash":"a4d0ca0fbfade924a5ccb2a9ba48a3dce68e668e","modified":1548133164936},{"_id":"source/_posts/end2end-coreference-resolution/8.png","hash":"e10a235321ea1882b637d33d7ee8c4f2e11ed408","modified":1548133164948},{"_id":"themes/kiddochan/source/img/IMG_2984.JPG","hash":"786e2bb74a8e97c6c51b8d0064dfa1228d4ddf6d","modified":1548133165016},{"_id":"source/_posts/end2end-coreference-resolution/10.png","hash":"2fc7427cd711098e755c9a2060e294addd099f0d","modified":1548133164932},{"_id":"source/_posts/end2end-coreference-resolution/4.png","hash":"eee7f95b7d5123a609daa311e4d0b665027180b3","modified":1548133164944},{"_id":"source/about/index/4.png","hash":"8e293c496bc53a38c59c67b288fc271668c3ab29","modified":1548133164968},{"_id":"source/about/index/IMG_8752.JPG","hash":"2ffad2e8faf36f7f7eb6798ca4ff68974df5c3b4","modified":1548133164980},{"_id":"themes/kiddochan/source/img/IMG_8752.JPG","hash":"2ffad2e8faf36f7f7eb6798ca4ff68974df5c3b4","modified":1548133165024},{"_id":"public/boostnote.json","hash":"b5b79bd5f31a89933a540aa726550b7587fa6c7c","modified":1548133237992},{"_id":"public/about/index.html","hash":"1e471aed3470971f0adfb53ee51864791a1632ae","modified":1548133238582},{"_id":"public/2019/01/22/spacy-leaning/index.html","hash":"71838e9e0926e0bcfd5cb193b293f680a4f3c876","modified":1548133238582},{"_id":"public/2019/01/02/2019-work-schedule/index.html","hash":"67eaf94ae99f11a9202933c9d0c8c8e64d18cc8e","modified":1548133238582},{"_id":"public/2019/01/02/year-plan/index.html","hash":"4aaedc94d8b2a74b48a84ce4ddbb4fe82b649e76","modified":1548133238582},{"_id":"public/2018/12/29/end2end-coreference-resolution/index.html","hash":"d108d19c84a94e53289d983c751ff6732309c84a","modified":1548133238582},{"_id":"public/2018/12/10/learning-tensorflow-linear-regression/index.html","hash":"f1b54389ddd82624c4b1a44f91a556765f08c13f","modified":1548133238582},{"_id":"public/2018/12/06/tree/index.html","hash":"299cc93c46c7bb7fef3cf4658523e8e24ab14744","modified":1548133238582},{"_id":"public/2018/10/14/python操作excel/index.html","hash":"6eaeaf5607038d2982c2a609c72e41ed3553868c","modified":1548133238582},{"_id":"public/archives/index.html","hash":"835d8eb9a840f03fc37f334b53b01813d9c9c756","modified":1548133238582},{"_id":"public/archives/page/2/index.html","hash":"d03647d81dfac01bfd009ac7c636c0fc25278a45","modified":1548133238582},{"_id":"public/archives/2015/index.html","hash":"5e11e7e9ec31c106063a41c191122e106bfbe4d5","modified":1548133238582},{"_id":"public/archives/2015/07/index.html","hash":"0f6483d16a83e4f7e93ee5581f44d59d9922cb8c","modified":1548133238582},{"_id":"public/archives/2016/index.html","hash":"b9905c674c8d9c88c52948286a9fd41d659d1dda","modified":1548133238583},{"_id":"public/archives/2016/11/index.html","hash":"dcbf155cf4644dcc62069ca09c65af907595db56","modified":1548133238583},{"_id":"public/archives/2017/index.html","hash":"8cca4ef2f46a561b2276c6c24c23d8305d3e2baf","modified":1548133238583},{"_id":"public/archives/2017/08/index.html","hash":"4f79f345c0e6c678ca81fb5045c26f3bcbb89577","modified":1548133238583},{"_id":"public/archives/2018/index.html","hash":"4273e4bf978aebad35d0ce6fc45f397074db7ad5","modified":1548133238583},{"_id":"public/archives/2018/04/index.html","hash":"7ab2362e7539e1e60d1224d4cda40a878604df24","modified":1548133238583},{"_id":"public/archives/2018/07/index.html","hash":"81af713e4ce7492f33d5c66398c0b2a9e968049e","modified":1548133238583},{"_id":"public/archives/2018/10/index.html","hash":"acfdb53b24f0ad8fff774da0347db0238b452d78","modified":1548133238583},{"_id":"public/archives/2018/11/index.html","hash":"b1725004c80b3a6bb5f39428f8fe1bb0a24d302b","modified":1548133238583},{"_id":"public/archives/2018/12/index.html","hash":"29034d880eb08c8763cbaf79223697c1382128ad","modified":1548133238583},{"_id":"public/archives/2019/index.html","hash":"0491ccc2f3498535f4f7d84b09e974498b511aef","modified":1548133238583},{"_id":"public/archives/2019/01/index.html","hash":"052cc21724addc6e745f3ccc70aa11ddd46bd6e3","modified":1548133238583},{"_id":"public/index.html","hash":"abff82d4cffdd594c91fdc22639b5f9e3b1a2f28","modified":1548133238583},{"_id":"public/page/2/index.html","hash":"f193d652788c846638845deb4ed0a66d370fa80f","modified":1548133238584},{"_id":"public/tags/随笔/index.html","hash":"cf6c9d6b6bd2d696f9afe4700e13c935b0e8b16a","modified":1548133238584},{"_id":"public/tags/爬虫/index.html","hash":"9e1554a62f043a0f06d145ccd9df5e2e9ffc15d0","modified":1548133238584},{"_id":"public/tags/学习/index.html","hash":"fab0baa615c6f3f7b93ac87c887cd07924c10d46","modified":1548133238584},{"_id":"public/tags/CoreNLP/index.html","hash":"f2f1ed7297818424306f1f5cfb1aafc942a96905","modified":1548133238585},{"_id":"public/tags/NLP/index.html","hash":"b8d0f8fb1f8098f4ac8cb4c0b4c531da18c2dd1c","modified":1548133238585},{"_id":"public/tags/笔记/index.html","hash":"d146d4d98c199e5fa6f9d0e2507b0fa02ec4bfee","modified":1548133238585},{"_id":"public/tags/Java/index.html","hash":"d92c3e8c753d40d442d81d60db4b64c944c24a52","modified":1548133238585},{"_id":"public/tags/CR/index.html","hash":"d38deb4a89d52bd198417d20cddda23a8da74adb","modified":1548133238585},{"_id":"public/tags/python/index.html","hash":"6b10161ff9e28ebf41497ea7680e62b344ecd341","modified":1548133238585},{"_id":"public/tags/Tensorflow/index.html","hash":"9a18ed6511e8829b1c6ceb8d23ccb569c0dbc701","modified":1548133238586},{"_id":"public/tags/Algorithm/index.html","hash":"f28edba8ebb86f703a79a9700219ffd030f5a3ad","modified":1548133238586},{"_id":"public/2018/12/12/algorithm-learning-sort/index.html","hash":"a5fd157c343bcfcad5721a566d15cb21c2f67012","modified":1548133238586},{"_id":"public/2018/12/06/cnn/index.html","hash":"efe0338b982b3a9737df2342daf8ede325c8aef8","modified":1548133238586},{"_id":"public/2018/12/05/how-to-use-python-to-build-markdown-calc/index.html","hash":"3248ce36724a6de0ff31dadde31eacbf1adb4b45","modified":1548133238586},{"_id":"public/2018/11/29/2018-work-schedule/index.html","hash":"5d5bdfcaa93e9f1ca19a280c93307e45dbe8de6f","modified":1548133238586},{"_id":"public/2018/07/14/论文笔记/index.html","hash":"6c78b2c35893defc3dbbeee58850efc8a75a2171","modified":1548133238586},{"_id":"public/2018/04/24/组会分享/index.html","hash":"2f3383745860f2b060fe11b34d6d2b781dbcf02b","modified":1548133238586},{"_id":"public/2017/08/31/Java学习笔记/index.html","hash":"5dbbfafc7ba6f6e17a4a18d1529c133ae583927c","modified":1548133238586},{"_id":"public/2017/08/24/conll数据格式/index.html","hash":"eab6a846742420f48904cedd6947eefff6d0ed96","modified":1548133238586},{"_id":"public/2016/11/19/CorefNLP相关-资料&笔记/index.html","hash":"8d00cc46fa89c85f8dd8224d8b9a67d9f5e52dd6","modified":1548133238586},{"_id":"public/2015/07/01/Baidu_tieba_spider/index.html","hash":"cb8a46824b1eb37e1033943d955e619638f17da7","modified":1548133238586},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1548133238605},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1548133238605},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1548133238605},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1548133238605},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1548133238605},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1548133238605},{"_id":"public/font/CoveredByYourGrace-webfont.woff2","hash":"fe5fce8733d35a38e127d59592c2201e046da478","modified":1548133238605},{"_id":"public/font/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1548133238605},{"_id":"public/font/ReenieBeanie.eot","hash":"b8a6114536ce1cde347339b299924ec33a992396","modified":1548133238605},{"_id":"public/font/ReenieBeanie.ttf","hash":"5d52ff55cbdfb46a293d6bd4ab6f98807942456b","modified":1548133238605},{"_id":"public/font/ReenieBeanie.woff","hash":"95a3376a44b1e208e2d3b4b81c62eac392911d17","modified":1548133238605},{"_id":"public/font/ReenieBeanie.woff2","hash":"d42df9377c1393c3347334772ef38fbaf11fee6f","modified":1548133238605},{"_id":"public/font/chalkboardse-bold-webfont.eot","hash":"032c99cb605b8dab399d94baec39ed44bc936bb2","modified":1548133238605},{"_id":"public/font/chalkboardse-bold-webfont.woff","hash":"de8c126b031046f309badfed30f166e7b96420e7","modified":1548133238606},{"_id":"public/font/chalkboardse-bold-webfont.woff2","hash":"312b79586406e09f4964bdeb9c0969ff9303d9a7","modified":1548133238606},{"_id":"public/font/coveredbyyourgrace-webfont.eot","hash":"a17d0f10534303e40f210c506ebb8703fa23b7de","modified":1548133238606},{"_id":"public/font/coveredbyyourgrace-webfont.ttf","hash":"194ccb4acf77a03dc25bcc174edb266143704fec","modified":1548133238606},{"_id":"public/font/coveredbyyourgrace-webfont.woff","hash":"c6f8dc1a2f6ce914f120e80a876b8fd77b98888e","modified":1548133238606},{"_id":"public/font/fontawesome-webfont.eot","hash":"9c94e02de6099219e219ddc7050053f24799b455","modified":1548133238606},{"_id":"public/font/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1548133238606},{"_id":"public/font/fontdiao.eot","hash":"9fd63e687d8fd01f792c46b2c508c847515a2b20","modified":1548133238606},{"_id":"public/font/fontdiao.ttf","hash":"f919729fc0dc7219c6de57fcebf86c54edefd964","modified":1548133238606},{"_id":"public/font/fontdiao.woff","hash":"66db191655c6ee0f8e476910558ba7a291977573","modified":1548133238606},{"_id":"public/font/fontdiao.woff2","hash":"45d7ee083bbc68a386fb95dd7788bf868a22b993","modified":1548133238606},{"_id":"public/img/author.JPG","hash":"4501d2bed52e399bded28759653d1691091e544a","modified":1548133238606},{"_id":"public/img/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1548133238606},{"_id":"public/img/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1548133238606},{"_id":"public/img/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1548133238607},{"_id":"public/img/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1548133238607},{"_id":"public/img/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1548133238607},{"_id":"public/img/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1548133238607},{"_id":"public/img/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1548133238607},{"_id":"public/img/icon.ico","hash":"53fef85a38a122c065b5e8b9510c845184db85b9","modified":1548133238607},{"_id":"public/img/scrollup.png","hash":"2137d4f1739aa8aa3fcb0348c3ddf1e41d62f2e3","modified":1548133238607},{"_id":"public/fancybox/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1548133238607},{"_id":"public/2018/12/05/how-to-use-python-to-build-markdown-calc/1.png","hash":"af6ab2f8de6a92aea282cbe250cd55e321584bff","modified":1548133238607},{"_id":"public/2018/12/05/how-to-use-python-to-build-markdown-calc/2.JPG","hash":"338f76229ac31cee03c0d08472c80adf86cd8ef7","modified":1548133238607},{"_id":"public/2015/07/01/Baidu_tieba_spider/1.png","hash":"af6ab2f8de6a92aea282cbe250cd55e321584bff","modified":1548133238607},{"_id":"public/2015/07/01/Baidu_tieba_spider/psb2.jpg","hash":"d87c984ce6b5fffbd86be703e4a7f08346cb8977","modified":1548133238607},{"_id":"public/2018/12/10/learning-tensorflow-linear-regression/1.png","hash":"7daa3b3cd8ea9287ce3137899434169f5123188e","modified":1548133238607},{"_id":"public/2018/12/10/learning-tensorflow-linear-regression/2.JPG","hash":"7e45ede4125c790b0a0a8e471e488bf86cb3c409","modified":1548133238608},{"_id":"public/2018/12/10/learning-tensorflow-linear-regression/3.png","hash":"5af1b8ecdbb528b4be13453a5c2e1ba632c1dff0","modified":1548133238608},{"_id":"public/2018/11/29/2018-work-schedule/1.JPG","hash":"5a4423d2a7c1fb20d16c3585aa79d51f3468f15e","modified":1548133238608},{"_id":"public/2018/12/06/cnn/data.JPG","hash":"4d420eed35d65f46fa669128b44f16f3092dcb30","modified":1548133238608},{"_id":"public/2018/12/06/cnn/vocab.JPG","hash":"c3d505f976c88161219f92629edd471241e5e486","modified":1548133238608},{"_id":"public/2018/12/12/algorithm-learning-sort/1.JPG","hash":"5a4423d2a7c1fb20d16c3585aa79d51f3468f15e","modified":1548133238608},{"_id":"public/2018/04/24/组会分享/clipboard.png","hash":"9b8590d42f3239f082f3b00281597d1f22e7a3a6","modified":1548133238608},{"_id":"public/2018/04/24/组会分享/clipboard_1.png","hash":"8ae5662ef01488516cfc8ddbf66db34cac7e83ea","modified":1548133238608},{"_id":"public/2018/04/24/组会分享/clipboard_2.png","hash":"41056b8308440132658a5b9dc836fe0275d08b2b","modified":1548133238608},{"_id":"public/2018/04/24/组会分享/clipboard_3.png","hash":"0eb0c2a36d968398fc241701f12e171c2cde0a76","modified":1548133238608},{"_id":"public/font/chalkboardse-bold-webfont.ttf","hash":"8ae80892d452643c69c7740640402f9d97d046d4","modified":1548133239268},{"_id":"public/font/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1548133239269},{"_id":"public/2018/12/12/algorithm-learning-sort/insert-sort-1.jpg","hash":"867fac442a77c6383e831a756783445fa39e81bf","modified":1548133239269},{"_id":"public/2018/12/12/algorithm-learning-sort/insert-sort-2.jpg","hash":"3b0fb85d9ec4bfb30fb32a38e5e3db7dd0b612f3","modified":1548133239270},{"_id":"public/2018/12/12/algorithm-learning-sort/select-sort-1.jpg","hash":"21ea0cc4a93aeaa67469effe310e482a4e730cb1","modified":1548133239270},{"_id":"public/2018/04/24/组会分享/clipboard_5.png","hash":"c695a8c991d38ed26271dc3d1d1d169e33a45e55","modified":1548133239270},{"_id":"public/fancybox/jquery.fancybox.css","hash":"aaa582fb9eb4b7092dc69fcb2d5b1c20cca58ab6","modified":1548133239279},{"_id":"public/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1548133239279},{"_id":"public/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1548133239279},{"_id":"public/js/totop.js","hash":"cad23c5ea7163d1e5c05a0fd3ef9233469da10cb","modified":1548133239279},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1548133239279},{"_id":"public/fancybox/helpers/jquery.fancybox-buttons.js","hash":"dc3645529a4bf72983a39fa34c1eb9146e082019","modified":1548133239279},{"_id":"public/fancybox/helpers/jquery.fancybox-media.js","hash":"294420f9ff20f4e3584d212b0c262a00a96ecdb3","modified":1548133239279},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1548133239279},{"_id":"public/fancybox/helpers/jquery.fancybox-thumbs.js","hash":"47da1ae5401c24b5c17cc18e2730780f5c1a7a0c","modified":1548133239280},{"_id":"public/css/style.css","hash":"1bef8bf3745cccbd2c5fe2bc7b65d27c0682f1ca","modified":1548133239280},{"_id":"public/2018/12/12/algorithm-learning-sort/swap-sort-1.jpg","hash":"07c16b72f6b51b035bb39235007d607c403defc4","modified":1548133239280},{"_id":"public/font/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1548133239290},{"_id":"public/img/banner.jpg","hash":"5104860c4f8b2e84ef734ba6c37fe7a288bf0d74","modified":1548133239291},{"_id":"public/2015/07/01/Baidu_tieba_spider/psb.png","hash":"dce74636acaa3068953c4ccd01a8a37219c2e4b1","modified":1548133239291},{"_id":"public/2018/12/29/end2end-coreference-resolution/3.png","hash":"6477f5d7cb808be1f7a1af2d881ae7e65470711a","modified":1548133239291},{"_id":"public/2018/04/24/组会分享/clipboard_4.png","hash":"f549d6682bccdd3f786fd8c8bd6b1b4dea0e3f03","modified":1548133239292},{"_id":"public/2018/12/12/algorithm-learning-sort/select-sort-2.jpg","hash":"8c7bf5f200ab6b4e1cba2f24ce5650d651d6d512","modified":1548133239292},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"9e0d51ca1dbe66f6c0c7aefd552dc8122e694a6e","modified":1548133239302},{"_id":"public/js/jquery.qrcode-0.12.0.min.js","hash":"57c3987166a26415a71292162690e82c21e315ad","modified":1548133239302},{"_id":"public/2018/12/06/cnn/3-4.png","hash":"653f28a43e8759008889de2e01a27f5197255084","modified":1548133239302},{"_id":"public/2018/12/29/end2end-coreference-resolution/5.png","hash":"14ba531085def0d9f97d52d8579dfc78e781d464","modified":1548133239302},{"_id":"public/2018/04/24/组会分享/tree.png","hash":"aa5f4c2769757b7a8b2fbec574aa42012f18282f","modified":1548133239302},{"_id":"public/2018/12/29/end2end-coreference-resolution/6.png","hash":"1c072e98c507b300c42445366c0b47b74af5a0ff","modified":1548133239308},{"_id":"public/2018/04/24/组会分享/mention_and_entity.png","hash":"e98cdc1a8e0d748144ee64bd07159842ea31b2c6","modified":1548133239318},{"_id":"public/fancybox/jquery.fancybox.js","hash":"d08b03a42d5c4ba456ef8ba33116fdbb7a9cabed","modified":1548133239324},{"_id":"public/2018/12/06/tree/1.jpg","hash":"c78ad854f331c6aa9a2b8cd0a4f28506ee7b4d5c","modified":1548133239324},{"_id":"public/2018/12/29/end2end-coreference-resolution/2.png","hash":"a4d0ca0fbfade924a5ccb2a9ba48a3dce68e668e","modified":1548133239324},{"_id":"public/2018/12/29/end2end-coreference-resolution/7.png","hash":"f581e13adda87494c686a28611ddbed65dd1be44","modified":1548133239324},{"_id":"public/2018/12/29/end2end-coreference-resolution/9.png","hash":"ad7cbef5664f3c11e4323ba280520f9114ca45ea","modified":1548133239325},{"_id":"public/2018/12/29/end2end-coreference-resolution/8.png","hash":"e10a235321ea1882b637d33d7ee8c4f2e11ed408","modified":1548133239331},{"_id":"public/js/jquery-2.0.3.min.js","hash":"a0ae3697b0ab8c0e8bd3186c80db42abd6d97a8d","modified":1548133239338},{"_id":"public/2018/12/29/end2end-coreference-resolution/10.png","hash":"2fc7427cd711098e755c9a2060e294addd099f0d","modified":1548133239340},{"_id":"public/img/IMG_2984.JPG","hash":"786e2bb74a8e97c6c51b8d0064dfa1228d4ddf6d","modified":1548133239343},{"_id":"public/2018/12/29/end2end-coreference-resolution/4.png","hash":"eee7f95b7d5123a609daa311e4d0b665027180b3","modified":1548133239344},{"_id":"public/about/index/4.png","hash":"8e293c496bc53a38c59c67b288fc271668c3ab29","modified":1548133239348},{"_id":"public/img/IMG_8752.JPG","hash":"2ffad2e8faf36f7f7eb6798ca4ff68974df5c3b4","modified":1548133239354},{"_id":"public/about/index/IMG_8752.JPG","hash":"2ffad2e8faf36f7f7eb6798ca4ff68974df5c3b4","modified":1548133239354}],"Category":[],"Data":[],"Page":[{"_content":"{\n  \"folders\": [],\n  \"version\": \"1.0\"\n}\n","source":"boostnote.json","raw":"{\n  \"folders\": [],\n  \"version\": \"1.0\"\n}\n","date":"2019-01-22T04:59:24.980Z","updated":"2019-01-22T04:59:24.980Z","path":"boostnote.json","layout":"false","title":"","comments":1,"_id":"cjr7ak2jo0000scftrk61ypq4","content":"{\"folders\":[],\"version\":\"1.0\"}","site":{"data":{}},"excerpt":"","more":"{\"folders\":[],\"version\":\"1.0\"}"},{"title":"about","date":"2018-11-29T03:30:30.000Z","_content":"\nZyq，男，94年生人，现居北京。\n\n本科智能科学与技术专业毕业，硕士为软件工程智能方向。\n\n本科自己学习过推荐系统的一些知识，研究生阶段主要研究方向为自然语言处理(指代消解)。\n\n以第一作者发表EI检索的国际会议论文三篇，软件著作权一篇。\n> Zhu Y. A book recommendation algorithm based on collaborative filtering[C]. international conference on computer science and network technology, 2016: 286-289.\n> Zhu Y, Song W, Liu L, et al. Collaborative Filtering Recommender Algorithm Based on Comments and Score[C]. international symposium on computational intelligence and design, 2017.\n> Improving Anaphora Resolution by Animacy Identification. （检索中）\n\n\n日常爱好游泳看书，会拍几张照片，休息时候会玩儿Minecraft\n\n---\n![image](/index/IMG_8752.JPG)\n\n\n\n\n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2018-11-29 11:30:30\n---\n\nZyq，男，94年生人，现居北京。\n\n本科智能科学与技术专业毕业，硕士为软件工程智能方向。\n\n本科自己学习过推荐系统的一些知识，研究生阶段主要研究方向为自然语言处理(指代消解)。\n\n以第一作者发表EI检索的国际会议论文三篇，软件著作权一篇。\n> Zhu Y. A book recommendation algorithm based on collaborative filtering[C]. international conference on computer science and network technology, 2016: 286-289.\n> Zhu Y, Song W, Liu L, et al. Collaborative Filtering Recommender Algorithm Based on Comments and Score[C]. international symposium on computational intelligence and design, 2017.\n> Improving Anaphora Resolution by Animacy Identification. （检索中）\n\n\n日常爱好游泳看书，会拍几张照片，休息时候会玩儿Minecraft\n\n---\n![image](/index/IMG_8752.JPG)\n\n\n\n\n\n","updated":"2019-01-22T04:59:24.960Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjr7ak2lw0002scftueb7elv6","content":"<p>Zyq，男，94年生人，现居北京。</p>\n<p>本科智能科学与技术专业毕业，硕士为软件工程智能方向。</p>\n<p>本科自己学习过推荐系统的一些知识，研究生阶段主要研究方向为自然语言处理(指代消解)。</p>\n<p>以第一作者发表EI检索的国际会议论文三篇，软件著作权一篇。</p>\n<blockquote>\n<p>Zhu Y. A book recommendation algorithm based on collaborative filtering[C]. international conference on computer science and network technology, 2016: 286-289.<br>Zhu Y, Song W, Liu L, et al. Collaborative Filtering Recommender Algorithm Based on Comments and Score[C]. international symposium on computational intelligence and design, 2017.<br>Improving Anaphora Resolution by Animacy Identification. （检索中）</p>\n</blockquote>\n<p>日常爱好游泳看书，会拍几张照片，休息时候会玩儿Minecraft</p>\n<hr>\n<p><img src=\"/about/index/IMG_8752.JPG\" alt=\"image\"></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Zyq，男，94年生人，现居北京。</p>\n<p>本科智能科学与技术专业毕业，硕士为软件工程智能方向。</p>\n<p>本科自己学习过推荐系统的一些知识，研究生阶段主要研究方向为自然语言处理(指代消解)。</p>\n<p>以第一作者发表EI检索的国际会议论文三篇，软件著作权一篇。</p>\n<blockquote>\n<p>Zhu Y. A book recommendation algorithm based on collaborative filtering[C]. international conference on computer science and network technology, 2016: 286-289.<br>Zhu Y, Song W, Liu L, et al. Collaborative Filtering Recommender Algorithm Based on Comments and Score[C]. international symposium on computational intelligence and design, 2017.<br>Improving Anaphora Resolution by Animacy Identification. （检索中）</p>\n</blockquote>\n<p>日常爱好游泳看书，会拍几张照片，休息时候会玩儿Minecraft</p>\n<hr>\n<p><img src=\"/about/index/IMG_8752.JPG\" alt=\"image\"></p>\n"}],"Post":[{"title":"2019日记&日报","date":"2019-01-02T08:26:08.000Z","description":"用来记录每天的工作-2019","top":70,"_content":"\n\n###          January   2019          \nSun | Mon | Tue  | Wed | Thu | Fri | Sat \n---| ---| ---| ---| ---| ---| ---|\n  |  | 1 | [2](#12) | [3](#13) | 4 | 5 |\n 6 | [7](#17) | [8](#18) | [9](#19) | [10](#110) | 11 | 12 |\n [13](#113) | 14 | 15 | [16](#116) | [17](#117) | [18](#118) | [19](#119) |\n [20](#120) | [21](#121) | [22](#122) | [23](#123) | [24](#124) | [25](#125) | [26](#126) |\n [27](#127) | [28](#128) | [29](#129) | [30](#130) | [31](#131) |\n\n**本月主要任务：**\n\n- 完成毕设的所有实验程序\n- 完成毕设论文的开头部分\n- 更新简历、准备春招\n- 保证每天的打卡\n\n---\n **<span id=\"12\">1-2</span>**\n\n - 制定[19年计划](https://junzx.github.io/2019/01/02/year-plan/)\n\n---\n**<span id=\"13\">1-3</span>**\n\n- 由于博客搬家，暂时停更\n\n---\n**<span id=\"17\">1-7</span>**\n\n- 完成博客迁移的工作\n- 过了一遍论文\n\n---\n**<span id=\"18\">1-8</span>**\n\n- 收集到了几个有用的数据集\n- 完成前两个sieve\n- 明天预计完成所有的sieve\n\n---\n**<span id=\"19\">1-9</span>**\n\n- 除了sieve6之外都完成了\n- 明天预计解决已完成的中一些bug，打好log\n- 准备春招\n\n---\n**<span id=\"110\">1-10</span>**\n\n- Multisieve-bug-fix -关于get_modifier\n- 画set coref的逻辑图\n\n---\n**<span id=\"113\">1-13</span>**\n\n**下周计划**\n\n- 毕设前几章\n- 买无印日程本\n- 程序完成PRF评价部分\n- 程序完成剩余的sieve部分\n\n---\n**<span id=\"116\">1-16</span>**\n\n- 尝试使用BiLSTM构建一个mention detection的程序\n- 解决部分消极情绪\n\n---\n**<span id=\"117\">1-17</span>**\n\n- 买到了日程本，review之前的计划\n- 整理了之前写的排序的笔记，合并为：[算法入门——python实现部分排序](https://junzx.github.io/2018/12/12/algorithm-learning-sort/)\n\n---\n**<span id=\"118\">1-18</span>**\n\n- 添加调用计算prf的perl程序，增加调用接口函数\n\n---\n**<span id=\"120\">1-20</span>**\n\n今日成果\n- bug fix：解决将结果写入文件的问题\n\n明日计划\n- 完成论文第二章\n- 看剩下的几个sieve的论文\n\n---\n**<span id=\"121\">1-21</span>**\n\n今日成果\n- 解决linux下hexo博客更新的问题，之前的问题是g、s均正常，但是deploy不正常，后将Blog下.deplot_git手动清除后d成功\n- 完成论文第二章的一半，后面需要继续看论文然后完善\n- 完成调用scorer.pl的函数，实现绘图的工具\n- 增加了几个sieve，主要是采用之前项目的代码\n\n明日计划\n- 关注待学习的课程，制定相应的计划\n- 完成论文第三章\n\n---\n**<span id=\"122\">1-22</span>**\n\n- 训练\n","source":"_posts/2019-work-schedule.md","raw":"---\ntitle: 2019日记&日报\ndate: 2019-01-02 16:26:08\ntags:\n    - 随笔\ndescription: 用来记录每天的工作-2019\ntop: 70\n---\n\n\n###          January   2019          \nSun | Mon | Tue  | Wed | Thu | Fri | Sat \n---| ---| ---| ---| ---| ---| ---|\n  |  | 1 | [2](#12) | [3](#13) | 4 | 5 |\n 6 | [7](#17) | [8](#18) | [9](#19) | [10](#110) | 11 | 12 |\n [13](#113) | 14 | 15 | [16](#116) | [17](#117) | [18](#118) | [19](#119) |\n [20](#120) | [21](#121) | [22](#122) | [23](#123) | [24](#124) | [25](#125) | [26](#126) |\n [27](#127) | [28](#128) | [29](#129) | [30](#130) | [31](#131) |\n\n**本月主要任务：**\n\n- 完成毕设的所有实验程序\n- 完成毕设论文的开头部分\n- 更新简历、准备春招\n- 保证每天的打卡\n\n---\n **<span id=\"12\">1-2</span>**\n\n - 制定[19年计划](https://junzx.github.io/2019/01/02/year-plan/)\n\n---\n**<span id=\"13\">1-3</span>**\n\n- 由于博客搬家，暂时停更\n\n---\n**<span id=\"17\">1-7</span>**\n\n- 完成博客迁移的工作\n- 过了一遍论文\n\n---\n**<span id=\"18\">1-8</span>**\n\n- 收集到了几个有用的数据集\n- 完成前两个sieve\n- 明天预计完成所有的sieve\n\n---\n**<span id=\"19\">1-9</span>**\n\n- 除了sieve6之外都完成了\n- 明天预计解决已完成的中一些bug，打好log\n- 准备春招\n\n---\n**<span id=\"110\">1-10</span>**\n\n- Multisieve-bug-fix -关于get_modifier\n- 画set coref的逻辑图\n\n---\n**<span id=\"113\">1-13</span>**\n\n**下周计划**\n\n- 毕设前几章\n- 买无印日程本\n- 程序完成PRF评价部分\n- 程序完成剩余的sieve部分\n\n---\n**<span id=\"116\">1-16</span>**\n\n- 尝试使用BiLSTM构建一个mention detection的程序\n- 解决部分消极情绪\n\n---\n**<span id=\"117\">1-17</span>**\n\n- 买到了日程本，review之前的计划\n- 整理了之前写的排序的笔记，合并为：[算法入门——python实现部分排序](https://junzx.github.io/2018/12/12/algorithm-learning-sort/)\n\n---\n**<span id=\"118\">1-18</span>**\n\n- 添加调用计算prf的perl程序，增加调用接口函数\n\n---\n**<span id=\"120\">1-20</span>**\n\n今日成果\n- bug fix：解决将结果写入文件的问题\n\n明日计划\n- 完成论文第二章\n- 看剩下的几个sieve的论文\n\n---\n**<span id=\"121\">1-21</span>**\n\n今日成果\n- 解决linux下hexo博客更新的问题，之前的问题是g、s均正常，但是deploy不正常，后将Blog下.deplot_git手动清除后d成功\n- 完成论文第二章的一半，后面需要继续看论文然后完善\n- 完成调用scorer.pl的函数，实现绘图的工具\n- 增加了几个sieve，主要是采用之前项目的代码\n\n明日计划\n- 关注待学习的课程，制定相应的计划\n- 完成论文第三章\n\n---\n**<span id=\"122\">1-22</span>**\n\n- 训练\n","slug":"2019-work-schedule","published":1,"updated":"2019-01-22T04:59:24.924Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2lq0001scftgnf7z2vm","content":"<h3 id=\"January-2019\"><a href=\"#January-2019\" class=\"headerlink\" title=\"January   2019\"></a>January   2019</h3><table>\n<thead>\n<tr>\n<th>Sun</th>\n<th>Mon</th>\n<th>Tue</th>\n<th>Wed</th>\n<th>Thu</th>\n<th>Fri</th>\n<th>Sat </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td>1</td>\n<td><a href=\"#12\">2</a></td>\n<td><a href=\"#13\">3</a></td>\n<td>4</td>\n<td>5</td>\n<td></td>\n</tr>\n<tr>\n<td> 6</td>\n<td><a href=\"#17\">7</a></td>\n<td><a href=\"#18\">8</a></td>\n<td><a href=\"#19\">9</a></td>\n<td><a href=\"#110\">10</a></td>\n<td>11</td>\n<td>12</td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#113\">13</a></td>\n<td>14</td>\n<td>15</td>\n<td><a href=\"#116\">16</a></td>\n<td><a href=\"#117\">17</a></td>\n<td><a href=\"#118\">18</a></td>\n<td><a href=\"#119\">19</a></td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#120\">20</a></td>\n<td><a href=\"#121\">21</a></td>\n<td><a href=\"#122\">22</a></td>\n<td><a href=\"#123\">23</a></td>\n<td><a href=\"#124\">24</a></td>\n<td><a href=\"#125\">25</a></td>\n<td><a href=\"#126\">26</a></td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#127\">27</a></td>\n<td><a href=\"#128\">28</a></td>\n<td><a href=\"#129\">29</a></td>\n<td><a href=\"#130\">30</a></td>\n<td><a href=\"#131\">31</a></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>本月主要任务：</strong></p>\n<ul>\n<li>完成毕设的所有实验程序</li>\n<li>完成毕设论文的开头部分</li>\n<li>更新简历、准备春招</li>\n<li>保证每天的打卡</li>\n</ul>\n<hr>\n<p> <strong><span id=\"12\">1-2</span></strong></p>\n<ul>\n<li>制定<a href=\"https://junzx.github.io/2019/01/02/year-plan/\" target=\"_blank\" rel=\"noopener\">19年计划</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"13\">1-3</span></strong></p>\n<ul>\n<li>由于博客搬家，暂时停更</li>\n</ul>\n<hr>\n<p><strong><span id=\"17\">1-7</span></strong></p>\n<ul>\n<li>完成博客迁移的工作</li>\n<li>过了一遍论文</li>\n</ul>\n<hr>\n<p><strong><span id=\"18\">1-8</span></strong></p>\n<ul>\n<li>收集到了几个有用的数据集</li>\n<li>完成前两个sieve</li>\n<li>明天预计完成所有的sieve</li>\n</ul>\n<hr>\n<p><strong><span id=\"19\">1-9</span></strong></p>\n<ul>\n<li>除了sieve6之外都完成了</li>\n<li>明天预计解决已完成的中一些bug，打好log</li>\n<li>准备春招</li>\n</ul>\n<hr>\n<p><strong><span id=\"110\">1-10</span></strong></p>\n<ul>\n<li>Multisieve-bug-fix -关于get_modifier</li>\n<li>画set coref的逻辑图</li>\n</ul>\n<hr>\n<p><strong><span id=\"113\">1-13</span></strong></p>\n<p><strong>下周计划</strong></p>\n<ul>\n<li>毕设前几章</li>\n<li>买无印日程本</li>\n<li>程序完成PRF评价部分</li>\n<li>程序完成剩余的sieve部分</li>\n</ul>\n<hr>\n<p><strong><span id=\"116\">1-16</span></strong></p>\n<ul>\n<li>尝试使用BiLSTM构建一个mention detection的程序</li>\n<li>解决部分消极情绪</li>\n</ul>\n<hr>\n<p><strong><span id=\"117\">1-17</span></strong></p>\n<ul>\n<li>买到了日程本，review之前的计划</li>\n<li>整理了之前写的排序的笔记，合并为：<a href=\"https://junzx.github.io/2018/12/12/algorithm-learning-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——python实现部分排序</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"118\">1-18</span></strong></p>\n<ul>\n<li>添加调用计算prf的perl程序，增加调用接口函数</li>\n</ul>\n<hr>\n<p><strong><span id=\"120\">1-20</span></strong></p>\n<p>今日成果</p>\n<ul>\n<li>bug fix：解决将结果写入文件的问题</li>\n</ul>\n<p>明日计划</p>\n<ul>\n<li>完成论文第二章</li>\n<li>看剩下的几个sieve的论文</li>\n</ul>\n<hr>\n<p><strong><span id=\"121\">1-21</span></strong></p>\n<p>今日成果</p>\n<ul>\n<li>解决linux下hexo博客更新的问题，之前的问题是g、s均正常，但是deploy不正常，后将Blog下.deplot_git手动清除后d成功</li>\n<li>完成论文第二章的一半，后面需要继续看论文然后完善</li>\n<li>完成调用scorer.pl的函数，实现绘图的工具</li>\n<li>增加了几个sieve，主要是采用之前项目的代码</li>\n</ul>\n<p>明日计划</p>\n<ul>\n<li>关注待学习的课程，制定相应的计划</li>\n<li>完成论文第三章</li>\n</ul>\n<hr>\n<p><strong><span id=\"122\">1-22</span></strong></p>\n<ul>\n<li>训练</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"January-2019\"><a href=\"#January-2019\" class=\"headerlink\" title=\"January   2019\"></a>January   2019</h3><table>\n<thead>\n<tr>\n<th>Sun</th>\n<th>Mon</th>\n<th>Tue</th>\n<th>Wed</th>\n<th>Thu</th>\n<th>Fri</th>\n<th>Sat </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td>1</td>\n<td><a href=\"#12\">2</a></td>\n<td><a href=\"#13\">3</a></td>\n<td>4</td>\n<td>5</td>\n<td></td>\n</tr>\n<tr>\n<td> 6</td>\n<td><a href=\"#17\">7</a></td>\n<td><a href=\"#18\">8</a></td>\n<td><a href=\"#19\">9</a></td>\n<td><a href=\"#110\">10</a></td>\n<td>11</td>\n<td>12</td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#113\">13</a></td>\n<td>14</td>\n<td>15</td>\n<td><a href=\"#116\">16</a></td>\n<td><a href=\"#117\">17</a></td>\n<td><a href=\"#118\">18</a></td>\n<td><a href=\"#119\">19</a></td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#120\">20</a></td>\n<td><a href=\"#121\">21</a></td>\n<td><a href=\"#122\">22</a></td>\n<td><a href=\"#123\">23</a></td>\n<td><a href=\"#124\">24</a></td>\n<td><a href=\"#125\">25</a></td>\n<td><a href=\"#126\">26</a></td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#127\">27</a></td>\n<td><a href=\"#128\">28</a></td>\n<td><a href=\"#129\">29</a></td>\n<td><a href=\"#130\">30</a></td>\n<td><a href=\"#131\">31</a></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p><strong>本月主要任务：</strong></p>\n<ul>\n<li>完成毕设的所有实验程序</li>\n<li>完成毕设论文的开头部分</li>\n<li>更新简历、准备春招</li>\n<li>保证每天的打卡</li>\n</ul>\n<hr>\n<p> <strong><span id=\"12\">1-2</span></strong></p>\n<ul>\n<li>制定<a href=\"https://junzx.github.io/2019/01/02/year-plan/\" target=\"_blank\" rel=\"noopener\">19年计划</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"13\">1-3</span></strong></p>\n<ul>\n<li>由于博客搬家，暂时停更</li>\n</ul>\n<hr>\n<p><strong><span id=\"17\">1-7</span></strong></p>\n<ul>\n<li>完成博客迁移的工作</li>\n<li>过了一遍论文</li>\n</ul>\n<hr>\n<p><strong><span id=\"18\">1-8</span></strong></p>\n<ul>\n<li>收集到了几个有用的数据集</li>\n<li>完成前两个sieve</li>\n<li>明天预计完成所有的sieve</li>\n</ul>\n<hr>\n<p><strong><span id=\"19\">1-9</span></strong></p>\n<ul>\n<li>除了sieve6之外都完成了</li>\n<li>明天预计解决已完成的中一些bug，打好log</li>\n<li>准备春招</li>\n</ul>\n<hr>\n<p><strong><span id=\"110\">1-10</span></strong></p>\n<ul>\n<li>Multisieve-bug-fix -关于get_modifier</li>\n<li>画set coref的逻辑图</li>\n</ul>\n<hr>\n<p><strong><span id=\"113\">1-13</span></strong></p>\n<p><strong>下周计划</strong></p>\n<ul>\n<li>毕设前几章</li>\n<li>买无印日程本</li>\n<li>程序完成PRF评价部分</li>\n<li>程序完成剩余的sieve部分</li>\n</ul>\n<hr>\n<p><strong><span id=\"116\">1-16</span></strong></p>\n<ul>\n<li>尝试使用BiLSTM构建一个mention detection的程序</li>\n<li>解决部分消极情绪</li>\n</ul>\n<hr>\n<p><strong><span id=\"117\">1-17</span></strong></p>\n<ul>\n<li>买到了日程本，review之前的计划</li>\n<li>整理了之前写的排序的笔记，合并为：<a href=\"https://junzx.github.io/2018/12/12/algorithm-learning-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——python实现部分排序</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"118\">1-18</span></strong></p>\n<ul>\n<li>添加调用计算prf的perl程序，增加调用接口函数</li>\n</ul>\n<hr>\n<p><strong><span id=\"120\">1-20</span></strong></p>\n<p>今日成果</p>\n<ul>\n<li>bug fix：解决将结果写入文件的问题</li>\n</ul>\n<p>明日计划</p>\n<ul>\n<li>完成论文第二章</li>\n<li>看剩下的几个sieve的论文</li>\n</ul>\n<hr>\n<p><strong><span id=\"121\">1-21</span></strong></p>\n<p>今日成果</p>\n<ul>\n<li>解决linux下hexo博客更新的问题，之前的问题是g、s均正常，但是deploy不正常，后将Blog下.deplot_git手动清除后d成功</li>\n<li>完成论文第二章的一半，后面需要继续看论文然后完善</li>\n<li>完成调用scorer.pl的函数，实现绘图的工具</li>\n<li>增加了几个sieve，主要是采用之前项目的代码</li>\n</ul>\n<p>明日计划</p>\n<ul>\n<li>关注待学习的课程，制定相应的计划</li>\n<li>完成论文第三章</li>\n</ul>\n<hr>\n<p><strong><span id=\"122\">1-22</span></strong></p>\n<ul>\n<li>训练</li>\n</ul>\n"},{"title":"爬虫初探——爬取百度贴吧","date":"2015-06-30T16:00:00.000Z","top":1,"description":"仿照他人的代码爬取百度贴吧的文本。","_content":"\n\n```\n# -*- coding:utf-8 -*-\nimport re\nimport urllib2\nclass BDTB(object):\n    def __init__(self,urlIn,seeLz):\n        self.baseUrl=urlIn\n        self.seeLz='?see_Lz='+str(seeLz)\n    def getPage(self,page):\n        try:\n            self.baseUrl=self.baseUrl+self.seeLz+'&pn='+str(page)\n            req=urllib2.Request(self.baseUrl)\n            response=urllib2.urlopen(req)\n            #return response.read().decode('utf-8')     # Maybe Bugs\n            return response\n        except Exception,e:\n            print 'Open Error:',e\n    def getTitle(self):\n        try:\n            aPage=self.getPage(1)\n            pattern=re.compile(r'<h3 class=\"core_title_txt.*?\" title=\"(.*?)\" style.*?</h3>',re.S)\n            result=re.search(pattern,str(aPage.read()))\n            testWriteTool(result.group(1).strip())\n            return result.group(1).strip()      # return a title of page\n            #testWriteTool(aPage.read())\n        except Exception,e:\n            print 'Error in getTitle:',e\n    def getPageNum(self):\n        try:\n            aPage=self.getPage(1)\n            #pattern=re.compile(r'<li class=\"l_reply_num\".*?<span class=\"red\">.*?(\\d*)</font>',re.S)\n            pattern=re.compile(r'<li class=\"l_reply_num\".*><span class.*>(\\d+)</span>',re.S)\n            #pattern = re.compile('<li class=\"l_reply_num.*?</span>.*?<span.*?>(.*?)</span>',re.S)\n            result=re.search(pattern,str(aPage.read()))\n            #print result.group(1).strip()\n            return result.group(1).strip()\n        except Exception,e:\n            print 'Error in getPageNum:',e\n    def getContent(self,page=1):\n        try:\n            i=1\n            aPage=self.getPage(page)\n            pattern=re.compile('<div id=\"post_content_.*?>(.*?)</div>',re.S)\n            #pattern=re.compile('<div id=\"post_content_.*?>(.*?)</div>',re.S)\n            items=re.findall(pattern,str(aPage.read()))\n            #print 'we get ',len(items),'contents'\n            f=open('ans.txt','a+')\n            for item in items:\n                temp=str(i)+':'+item.strip()+'\\n'\n                f.writelines(temp)\n                i+=1\n                #print item\n            f.close()\n        except Exception,e:\n            print 'Error in getContent:',e\n        else:\n            print 'We Loading Txt....'\ndef testWriteTool(objStr):\n    f=open('ans.txt','a+')\n    f.writelines(objStr+'\\n')\n    f.close()\ndef startFunc(url):\n    f=open('ans.txt','a+')\n    f.truncate()\n    f.close()\n    #============= start clear file\n    test=BDTB(url,1)\n    if len(test.getPage(1).read())!=0:print 'yes we can conn'\n    print 'Title:',test.getTitle()\n    numOfPage=test.getPageNum()\n    print 'PageNum:',numOfPage\n    for i in range(1,int(numOfPage)+1):\n        test.getContent(i)\nif __name__ == '__main__':\n    testUrl1='http://tieba.baidu.com/p/3138733512'      #2014-07-01 16:22\n    testUrl2='http://tieba.baidu.com/p/3812772126'      #2015-06-07 20:53\n    testUrl3='http://tieba.baidu.com/p/3180989704'      #2014-07-22 19:05\n    \n    #=======================\n    #startFunc(testUrl1)\n    startFunc(testUrl2)\n    #startFunc(testUrl3)\n```\n\n参考：http://cuiqingcai.com/993.html\n写的挺详细的 虽然在实现的时候也出了这样那样的错吧\n\n## 运行时：\n\n![image](/Baidu_tieba_spider/psb2.jpg)\n\n\n\n## 运行结果：\n![image](/Baidu_tieba_spider/psb.png)\n\n","source":"_posts/Baidu_tieba_spider.md","raw":"---\ntitle: 爬虫初探——爬取百度贴吧\ndate: 2015-7-1\ntags: \n    - 爬虫\n    - 学习\ntop: 1\ndescription: 仿照他人的代码爬取百度贴吧的文本。\n---\n\n\n```\n# -*- coding:utf-8 -*-\nimport re\nimport urllib2\nclass BDTB(object):\n    def __init__(self,urlIn,seeLz):\n        self.baseUrl=urlIn\n        self.seeLz='?see_Lz='+str(seeLz)\n    def getPage(self,page):\n        try:\n            self.baseUrl=self.baseUrl+self.seeLz+'&pn='+str(page)\n            req=urllib2.Request(self.baseUrl)\n            response=urllib2.urlopen(req)\n            #return response.read().decode('utf-8')     # Maybe Bugs\n            return response\n        except Exception,e:\n            print 'Open Error:',e\n    def getTitle(self):\n        try:\n            aPage=self.getPage(1)\n            pattern=re.compile(r'<h3 class=\"core_title_txt.*?\" title=\"(.*?)\" style.*?</h3>',re.S)\n            result=re.search(pattern,str(aPage.read()))\n            testWriteTool(result.group(1).strip())\n            return result.group(1).strip()      # return a title of page\n            #testWriteTool(aPage.read())\n        except Exception,e:\n            print 'Error in getTitle:',e\n    def getPageNum(self):\n        try:\n            aPage=self.getPage(1)\n            #pattern=re.compile(r'<li class=\"l_reply_num\".*?<span class=\"red\">.*?(\\d*)</font>',re.S)\n            pattern=re.compile(r'<li class=\"l_reply_num\".*><span class.*>(\\d+)</span>',re.S)\n            #pattern = re.compile('<li class=\"l_reply_num.*?</span>.*?<span.*?>(.*?)</span>',re.S)\n            result=re.search(pattern,str(aPage.read()))\n            #print result.group(1).strip()\n            return result.group(1).strip()\n        except Exception,e:\n            print 'Error in getPageNum:',e\n    def getContent(self,page=1):\n        try:\n            i=1\n            aPage=self.getPage(page)\n            pattern=re.compile('<div id=\"post_content_.*?>(.*?)</div>',re.S)\n            #pattern=re.compile('<div id=\"post_content_.*?>(.*?)</div>',re.S)\n            items=re.findall(pattern,str(aPage.read()))\n            #print 'we get ',len(items),'contents'\n            f=open('ans.txt','a+')\n            for item in items:\n                temp=str(i)+':'+item.strip()+'\\n'\n                f.writelines(temp)\n                i+=1\n                #print item\n            f.close()\n        except Exception,e:\n            print 'Error in getContent:',e\n        else:\n            print 'We Loading Txt....'\ndef testWriteTool(objStr):\n    f=open('ans.txt','a+')\n    f.writelines(objStr+'\\n')\n    f.close()\ndef startFunc(url):\n    f=open('ans.txt','a+')\n    f.truncate()\n    f.close()\n    #============= start clear file\n    test=BDTB(url,1)\n    if len(test.getPage(1).read())!=0:print 'yes we can conn'\n    print 'Title:',test.getTitle()\n    numOfPage=test.getPageNum()\n    print 'PageNum:',numOfPage\n    for i in range(1,int(numOfPage)+1):\n        test.getContent(i)\nif __name__ == '__main__':\n    testUrl1='http://tieba.baidu.com/p/3138733512'      #2014-07-01 16:22\n    testUrl2='http://tieba.baidu.com/p/3812772126'      #2015-06-07 20:53\n    testUrl3='http://tieba.baidu.com/p/3180989704'      #2014-07-22 19:05\n    \n    #=======================\n    #startFunc(testUrl1)\n    startFunc(testUrl2)\n    #startFunc(testUrl3)\n```\n\n参考：http://cuiqingcai.com/993.html\n写的挺详细的 虽然在实现的时候也出了这样那样的错吧\n\n## 运行时：\n\n![image](/Baidu_tieba_spider/psb2.jpg)\n\n\n\n## 运行结果：\n![image](/Baidu_tieba_spider/psb.png)\n\n","slug":"Baidu_tieba_spider","published":1,"updated":"2019-01-22T04:59:24.924Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2lx0003scftus1voevv","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -*- coding:utf-8 -*-</span><br><span class=\"line\">import re</span><br><span class=\"line\">import urllib2</span><br><span class=\"line\">class BDTB(object):</span><br><span class=\"line\">    def __init__(self,urlIn,seeLz):</span><br><span class=\"line\">        self.baseUrl=urlIn</span><br><span class=\"line\">        self.seeLz=&apos;?see_Lz=&apos;+str(seeLz)</span><br><span class=\"line\">    def getPage(self,page):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            self.baseUrl=self.baseUrl+self.seeLz+&apos;&amp;pn=&apos;+str(page)</span><br><span class=\"line\">            req=urllib2.Request(self.baseUrl)</span><br><span class=\"line\">            response=urllib2.urlopen(req)</span><br><span class=\"line\">            #return response.read().decode(&apos;utf-8&apos;)     # Maybe Bugs</span><br><span class=\"line\">            return response</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Open Error:&apos;,e</span><br><span class=\"line\">    def getTitle(self):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            aPage=self.getPage(1)</span><br><span class=\"line\">            pattern=re.compile(r&apos;&lt;h3 class=&quot;core_title_txt.*?&quot; title=&quot;(.*?)&quot; style.*?&lt;/h3&gt;&apos;,re.S)</span><br><span class=\"line\">            result=re.search(pattern,str(aPage.read()))</span><br><span class=\"line\">            testWriteTool(result.group(1).strip())</span><br><span class=\"line\">            return result.group(1).strip()      # return a title of page</span><br><span class=\"line\">            #testWriteTool(aPage.read())</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Error in getTitle:&apos;,e</span><br><span class=\"line\">    def getPageNum(self):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            aPage=self.getPage(1)</span><br><span class=\"line\">            #pattern=re.compile(r&apos;&lt;li class=&quot;l_reply_num&quot;.*?&lt;span class=&quot;red&quot;&gt;.*?(\\d*)&lt;/font&gt;&apos;,re.S)</span><br><span class=\"line\">            pattern=re.compile(r&apos;&lt;li class=&quot;l_reply_num&quot;.*&gt;&lt;span class.*&gt;(\\d+)&lt;/span&gt;&apos;,re.S)</span><br><span class=\"line\">            #pattern = re.compile(&apos;&lt;li class=&quot;l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;&apos;,re.S)</span><br><span class=\"line\">            result=re.search(pattern,str(aPage.read()))</span><br><span class=\"line\">            #print result.group(1).strip()</span><br><span class=\"line\">            return result.group(1).strip()</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Error in getPageNum:&apos;,e</span><br><span class=\"line\">    def getContent(self,page=1):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            i=1</span><br><span class=\"line\">            aPage=self.getPage(page)</span><br><span class=\"line\">            pattern=re.compile(&apos;&lt;div id=&quot;post_content_.*?&gt;(.*?)&lt;/div&gt;&apos;,re.S)</span><br><span class=\"line\">            #pattern=re.compile(&apos;&lt;div id=&quot;post_content_.*?&gt;(.*?)&lt;/div&gt;&apos;,re.S)</span><br><span class=\"line\">            items=re.findall(pattern,str(aPage.read()))</span><br><span class=\"line\">            #print &apos;we get &apos;,len(items),&apos;contents&apos;</span><br><span class=\"line\">            f=open(&apos;ans.txt&apos;,&apos;a+&apos;)</span><br><span class=\"line\">            for item in items:</span><br><span class=\"line\">                temp=str(i)+&apos;:&apos;+item.strip()+&apos;\\n&apos;</span><br><span class=\"line\">                f.writelines(temp)</span><br><span class=\"line\">                i+=1</span><br><span class=\"line\">                #print item</span><br><span class=\"line\">            f.close()</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Error in getContent:&apos;,e</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            print &apos;We Loading Txt....&apos;</span><br><span class=\"line\">def testWriteTool(objStr):</span><br><span class=\"line\">    f=open(&apos;ans.txt&apos;,&apos;a+&apos;)</span><br><span class=\"line\">    f.writelines(objStr+&apos;\\n&apos;)</span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">def startFunc(url):</span><br><span class=\"line\">    f=open(&apos;ans.txt&apos;,&apos;a+&apos;)</span><br><span class=\"line\">    f.truncate()</span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    #============= start clear file</span><br><span class=\"line\">    test=BDTB(url,1)</span><br><span class=\"line\">    if len(test.getPage(1).read())!=0:print &apos;yes we can conn&apos;</span><br><span class=\"line\">    print &apos;Title:&apos;,test.getTitle()</span><br><span class=\"line\">    numOfPage=test.getPageNum()</span><br><span class=\"line\">    print &apos;PageNum:&apos;,numOfPage</span><br><span class=\"line\">    for i in range(1,int(numOfPage)+1):</span><br><span class=\"line\">        test.getContent(i)</span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    testUrl1=&apos;http://tieba.baidu.com/p/3138733512&apos;      #2014-07-01 16:22</span><br><span class=\"line\">    testUrl2=&apos;http://tieba.baidu.com/p/3812772126&apos;      #2015-06-07 20:53</span><br><span class=\"line\">    testUrl3=&apos;http://tieba.baidu.com/p/3180989704&apos;      #2014-07-22 19:05</span><br><span class=\"line\">    </span><br><span class=\"line\">    #=======================</span><br><span class=\"line\">    #startFunc(testUrl1)</span><br><span class=\"line\">    startFunc(testUrl2)</span><br><span class=\"line\">    #startFunc(testUrl3)</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"http://cuiqingcai.com/993.html\" target=\"_blank\" rel=\"noopener\">http://cuiqingcai.com/993.html</a><br>写的挺详细的 虽然在实现的时候也出了这样那样的错吧</p>\n<h2 id=\"运行时：\"><a href=\"#运行时：\" class=\"headerlink\" title=\"运行时：\"></a>运行时：</h2><p><img src=\"/2015/07/01/Baidu_tieba_spider/psb2.jpg\" alt=\"image\"></p>\n<h2 id=\"运行结果：\"><a href=\"#运行结果：\" class=\"headerlink\" title=\"运行结果：\"></a>运行结果：</h2><p><img src=\"/2015/07/01/Baidu_tieba_spider/psb.png\" alt=\"image\"></p>\n","site":{"data":{}},"excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># -*- coding:utf-8 -*-</span><br><span class=\"line\">import re</span><br><span class=\"line\">import urllib2</span><br><span class=\"line\">class BDTB(object):</span><br><span class=\"line\">    def __init__(self,urlIn,seeLz):</span><br><span class=\"line\">        self.baseUrl=urlIn</span><br><span class=\"line\">        self.seeLz=&apos;?see_Lz=&apos;+str(seeLz)</span><br><span class=\"line\">    def getPage(self,page):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            self.baseUrl=self.baseUrl+self.seeLz+&apos;&amp;pn=&apos;+str(page)</span><br><span class=\"line\">            req=urllib2.Request(self.baseUrl)</span><br><span class=\"line\">            response=urllib2.urlopen(req)</span><br><span class=\"line\">            #return response.read().decode(&apos;utf-8&apos;)     # Maybe Bugs</span><br><span class=\"line\">            return response</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Open Error:&apos;,e</span><br><span class=\"line\">    def getTitle(self):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            aPage=self.getPage(1)</span><br><span class=\"line\">            pattern=re.compile(r&apos;&lt;h3 class=&quot;core_title_txt.*?&quot; title=&quot;(.*?)&quot; style.*?&lt;/h3&gt;&apos;,re.S)</span><br><span class=\"line\">            result=re.search(pattern,str(aPage.read()))</span><br><span class=\"line\">            testWriteTool(result.group(1).strip())</span><br><span class=\"line\">            return result.group(1).strip()      # return a title of page</span><br><span class=\"line\">            #testWriteTool(aPage.read())</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Error in getTitle:&apos;,e</span><br><span class=\"line\">    def getPageNum(self):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            aPage=self.getPage(1)</span><br><span class=\"line\">            #pattern=re.compile(r&apos;&lt;li class=&quot;l_reply_num&quot;.*?&lt;span class=&quot;red&quot;&gt;.*?(\\d*)&lt;/font&gt;&apos;,re.S)</span><br><span class=\"line\">            pattern=re.compile(r&apos;&lt;li class=&quot;l_reply_num&quot;.*&gt;&lt;span class.*&gt;(\\d+)&lt;/span&gt;&apos;,re.S)</span><br><span class=\"line\">            #pattern = re.compile(&apos;&lt;li class=&quot;l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;&apos;,re.S)</span><br><span class=\"line\">            result=re.search(pattern,str(aPage.read()))</span><br><span class=\"line\">            #print result.group(1).strip()</span><br><span class=\"line\">            return result.group(1).strip()</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Error in getPageNum:&apos;,e</span><br><span class=\"line\">    def getContent(self,page=1):</span><br><span class=\"line\">        try:</span><br><span class=\"line\">            i=1</span><br><span class=\"line\">            aPage=self.getPage(page)</span><br><span class=\"line\">            pattern=re.compile(&apos;&lt;div id=&quot;post_content_.*?&gt;(.*?)&lt;/div&gt;&apos;,re.S)</span><br><span class=\"line\">            #pattern=re.compile(&apos;&lt;div id=&quot;post_content_.*?&gt;(.*?)&lt;/div&gt;&apos;,re.S)</span><br><span class=\"line\">            items=re.findall(pattern,str(aPage.read()))</span><br><span class=\"line\">            #print &apos;we get &apos;,len(items),&apos;contents&apos;</span><br><span class=\"line\">            f=open(&apos;ans.txt&apos;,&apos;a+&apos;)</span><br><span class=\"line\">            for item in items:</span><br><span class=\"line\">                temp=str(i)+&apos;:&apos;+item.strip()+&apos;\\n&apos;</span><br><span class=\"line\">                f.writelines(temp)</span><br><span class=\"line\">                i+=1</span><br><span class=\"line\">                #print item</span><br><span class=\"line\">            f.close()</span><br><span class=\"line\">        except Exception,e:</span><br><span class=\"line\">            print &apos;Error in getContent:&apos;,e</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            print &apos;We Loading Txt....&apos;</span><br><span class=\"line\">def testWriteTool(objStr):</span><br><span class=\"line\">    f=open(&apos;ans.txt&apos;,&apos;a+&apos;)</span><br><span class=\"line\">    f.writelines(objStr+&apos;\\n&apos;)</span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">def startFunc(url):</span><br><span class=\"line\">    f=open(&apos;ans.txt&apos;,&apos;a+&apos;)</span><br><span class=\"line\">    f.truncate()</span><br><span class=\"line\">    f.close()</span><br><span class=\"line\">    #============= start clear file</span><br><span class=\"line\">    test=BDTB(url,1)</span><br><span class=\"line\">    if len(test.getPage(1).read())!=0:print &apos;yes we can conn&apos;</span><br><span class=\"line\">    print &apos;Title:&apos;,test.getTitle()</span><br><span class=\"line\">    numOfPage=test.getPageNum()</span><br><span class=\"line\">    print &apos;PageNum:&apos;,numOfPage</span><br><span class=\"line\">    for i in range(1,int(numOfPage)+1):</span><br><span class=\"line\">        test.getContent(i)</span><br><span class=\"line\">if __name__ == &apos;__main__&apos;:</span><br><span class=\"line\">    testUrl1=&apos;http://tieba.baidu.com/p/3138733512&apos;      #2014-07-01 16:22</span><br><span class=\"line\">    testUrl2=&apos;http://tieba.baidu.com/p/3812772126&apos;      #2015-06-07 20:53</span><br><span class=\"line\">    testUrl3=&apos;http://tieba.baidu.com/p/3180989704&apos;      #2014-07-22 19:05</span><br><span class=\"line\">    </span><br><span class=\"line\">    #=======================</span><br><span class=\"line\">    #startFunc(testUrl1)</span><br><span class=\"line\">    startFunc(testUrl2)</span><br><span class=\"line\">    #startFunc(testUrl3)</span><br></pre></td></tr></table></figure>\n<p>参考：<a href=\"http://cuiqingcai.com/993.html\" target=\"_blank\" rel=\"noopener\">http://cuiqingcai.com/993.html</a><br>写的挺详细的 虽然在实现的时候也出了这样那样的错吧</p>\n<h2 id=\"运行时：\"><a href=\"#运行时：\" class=\"headerlink\" title=\"运行时：\"></a>运行时：</h2><p><img src=\"/2015/07/01/Baidu_tieba_spider/psb2.jpg\" alt=\"image\"></p>\n<h2 id=\"运行结果：\"><a href=\"#运行结果：\" class=\"headerlink\" title=\"运行结果：\"></a>运行结果：</h2><p><img src=\"/2015/07/01/Baidu_tieba_spider/psb.png\" alt=\"image\"></p>\n"},{"title":"CoreNLP资料","date":"2016-11-19T02:44:25.000Z","description":"CoreNLP相关笔记","top":1,"_content":"\n\n- [一个日语关于CoreNLP的讲解](http://lab.astamuse.co.jp/entry/corenlp1)\n- [coreNLP的使用](http://luchi007.iteye.com/blog/2285485)\n- [Stanford CoreNLP 3.6.0 中文指代消解模块调用失败的解决方案](http://www.cnblogs.com/zklidd/p/5081677.html)\n- [使用Stanford CoreNLP工具包处理中文](http://blog.csdn.net/jiangjingxuan/article/details/54906288)\n- [深入迁出CoreNLP、可能暂无用](https://toutiao.io/posts/366808/app_preview)\n- [Github上一个CoreNLP的demo](https://github.com/drewfarris/corenlp-examples/tree/master/src/main/java/drew/corenlp)\n- [Github上搜索corenlp](https://github.com/search?utf8=%E2%9C%93&q=corenlp&type=)\n- [Stanford Deterministic Coreference Resolution System](https://nlp.stanford.edu/software/dcoref.html)\n- [CoreNLP主页](https://stanfordnlp.github.io/CoreNLP/coref.html#overview)\n- [The Stanford CoreNLP Natural Language Processing Toolkit/ PDF](https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf)\n- [**Coref文档**](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/)\n- [Coref Github 代码](https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/coref)\n- [从命令行使用斯坦福 CoreNLP](https://kkbac.wordpress.com/2016/11/25/%E4%BB%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BD%BF%E7%94%A8%E6%96%AF%E5%9D%A6%E7%A6%8F-corenlp/)\n***\n***\n- 示范代码：\n 来源：https://stanfordnlp.github.io/CoreNLP/coref.html#api\n\n##### 输入：Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.\n\n##### 输出：\n```\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.7 sec].\n[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.3 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention\n[main] INFO edu.stanford.nlp.pipeline.MentionAnnotator - Using mention detector type: dependency\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [1.2 sec].\n---\ncoref chains\n\tCHAIN3-[\"Barack Obama\" in sentence 1, \"He\" in sentence 2, \"Obama\" in sentence 3]\n---\nmentions\n\tBarack Obama\n\tHawaii\n---\nmentions\n\tthe president\n\tHe\n---\nmentions\n\tObama\n\t2008\n\n```\n\n##### 代码：\n\n```\n//这个文件是原版的示例代码，没有任何的改动\nimport java.util.Properties;\n\nimport edu.stanford.nlp.coref.CorefCoreAnnotations;\nimport edu.stanford.nlp.coref.data.CorefChain;\nimport edu.stanford.nlp.coref.data.Mention;\nimport edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.util.CoreMap;\n\npublic class GoldCode {\n\t  public static void main(String[] args) throws Exception {\n\t    Annotation document = new Annotation(\"Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.\");\n\t    Properties props = new Properties();\n\t    props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,ner,parse,mention,coref\");\n\t    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n\t    pipeline.annotate(document);\n\t    System.out.println(\"---\");\n\t    System.out.println(\"coref chains\");\n\t    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {\n\t      System.out.println(\"\\t\" + cc);\n\t    }\n\t    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {\n\t      System.out.println(\"---\");\n\t      System.out.println(\"mentions\");\n\t      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {\n\t        System.out.println(\"\\t\" + m);\n       }\n    }\n  }\n}\n\n\n```\n\n\n### 注：以上代码只针对英文句子有效，中文句子无法成功。\n\n\n##### 需要调用的包：\n![image](http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/C871BA8A94E24C14B56D3D5B3C998330/244)\n\n\n---\n\n\n```\nimport edu.stanford.nlp.coref.CorefCoreAnnotations;\nimport edu.stanford.nlp.coref.data.CorefChain;\nimport edu.stanford.nlp.coref.data.Mention;\nimport edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.util.CoreMap;\n```\n\n\nThis is not a problem of StanfordCoreNlpDemo program because I ran that code in Netbeans before. The problem seems associated with classpath issue.\n\nSince the StanfordCoreNlpDemo.java file belongs to a package\n\n\n```\npackage package edu.stanford.nlp.pipeline.demo;\n\npublic class StanfordCoreNlpDemo {\n    public static final void main(String[] args) throws IOException {\n        // code goes here\n    } \n}\n```\n\nThen calling the following results in Error: Could not find or load main class TheClassName.\n\njava -cp . StanfordCoreNlpDemo\nIt must be called with its fully-qualified name:\n\njava -cp . edu.stanford.nlp.pipeline.demo.StanfordCoreNlpDemo\nAnd this edu.stanford.nlp.pipeline.demo directory must exist in the classpath. In this example, ., meaning the current directory, is the entirety of classpath. Therefore this particular example must be called from the directory in which edu.stanford.nlp.pipeline.demo exists.\n\nReference\n\nhttps://stackoverflow.com/a/29331827/5352399\n\nhttps://stackoverflow.com/a/18093929/5352399\n\n**StanfordCoreNLP的各个组件（annotator）按“tokenize（分词）, ssplit（断句）, pos（词性标注）, lemma（词元化）, ner（命名实体识别）, parse（语法分析）, dcoref（同义词分辨）”**","source":"_posts/CorefNLP相关-资料&笔记.md","raw":"---\ntitle: CoreNLP资料\ndate: 2016-11-19 10:44:25\ntags: \n\t- CoreNLP\n\t- NLP\ndescription: CoreNLP相关笔记\ntop: 1\n---\n\n\n- [一个日语关于CoreNLP的讲解](http://lab.astamuse.co.jp/entry/corenlp1)\n- [coreNLP的使用](http://luchi007.iteye.com/blog/2285485)\n- [Stanford CoreNLP 3.6.0 中文指代消解模块调用失败的解决方案](http://www.cnblogs.com/zklidd/p/5081677.html)\n- [使用Stanford CoreNLP工具包处理中文](http://blog.csdn.net/jiangjingxuan/article/details/54906288)\n- [深入迁出CoreNLP、可能暂无用](https://toutiao.io/posts/366808/app_preview)\n- [Github上一个CoreNLP的demo](https://github.com/drewfarris/corenlp-examples/tree/master/src/main/java/drew/corenlp)\n- [Github上搜索corenlp](https://github.com/search?utf8=%E2%9C%93&q=corenlp&type=)\n- [Stanford Deterministic Coreference Resolution System](https://nlp.stanford.edu/software/dcoref.html)\n- [CoreNLP主页](https://stanfordnlp.github.io/CoreNLP/coref.html#overview)\n- [The Stanford CoreNLP Natural Language Processing Toolkit/ PDF](https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf)\n- [**Coref文档**](https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/)\n- [Coref Github 代码](https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/coref)\n- [从命令行使用斯坦福 CoreNLP](https://kkbac.wordpress.com/2016/11/25/%E4%BB%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BD%BF%E7%94%A8%E6%96%AF%E5%9D%A6%E7%A6%8F-corenlp/)\n***\n***\n- 示范代码：\n 来源：https://stanfordnlp.github.io/CoreNLP/coref.html#api\n\n##### 输入：Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.\n\n##### 输出：\n```\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.7 sec].\n[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.3 sec].\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention\n[main] INFO edu.stanford.nlp.pipeline.MentionAnnotator - Using mention detector type: dependency\n[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref\n[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [1.2 sec].\n---\ncoref chains\n\tCHAIN3-[\"Barack Obama\" in sentence 1, \"He\" in sentence 2, \"Obama\" in sentence 3]\n---\nmentions\n\tBarack Obama\n\tHawaii\n---\nmentions\n\tthe president\n\tHe\n---\nmentions\n\tObama\n\t2008\n\n```\n\n##### 代码：\n\n```\n//这个文件是原版的示例代码，没有任何的改动\nimport java.util.Properties;\n\nimport edu.stanford.nlp.coref.CorefCoreAnnotations;\nimport edu.stanford.nlp.coref.data.CorefChain;\nimport edu.stanford.nlp.coref.data.Mention;\nimport edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.util.CoreMap;\n\npublic class GoldCode {\n\t  public static void main(String[] args) throws Exception {\n\t    Annotation document = new Annotation(\"Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.\");\n\t    Properties props = new Properties();\n\t    props.setProperty(\"annotators\", \"tokenize,ssplit,pos,lemma,ner,parse,mention,coref\");\n\t    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);\n\t    pipeline.annotate(document);\n\t    System.out.println(\"---\");\n\t    System.out.println(\"coref chains\");\n\t    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) {\n\t      System.out.println(\"\\t\" + cc);\n\t    }\n\t    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) {\n\t      System.out.println(\"---\");\n\t      System.out.println(\"mentions\");\n\t      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) {\n\t        System.out.println(\"\\t\" + m);\n       }\n    }\n  }\n}\n\n\n```\n\n\n### 注：以上代码只针对英文句子有效，中文句子无法成功。\n\n\n##### 需要调用的包：\n![image](http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/C871BA8A94E24C14B56D3D5B3C998330/244)\n\n\n---\n\n\n```\nimport edu.stanford.nlp.coref.CorefCoreAnnotations;\nimport edu.stanford.nlp.coref.data.CorefChain;\nimport edu.stanford.nlp.coref.data.Mention;\nimport edu.stanford.nlp.ling.CoreAnnotations;\nimport edu.stanford.nlp.pipeline.Annotation;\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP;\nimport edu.stanford.nlp.util.CoreMap;\n```\n\n\nThis is not a problem of StanfordCoreNlpDemo program because I ran that code in Netbeans before. The problem seems associated with classpath issue.\n\nSince the StanfordCoreNlpDemo.java file belongs to a package\n\n\n```\npackage package edu.stanford.nlp.pipeline.demo;\n\npublic class StanfordCoreNlpDemo {\n    public static final void main(String[] args) throws IOException {\n        // code goes here\n    } \n}\n```\n\nThen calling the following results in Error: Could not find or load main class TheClassName.\n\njava -cp . StanfordCoreNlpDemo\nIt must be called with its fully-qualified name:\n\njava -cp . edu.stanford.nlp.pipeline.demo.StanfordCoreNlpDemo\nAnd this edu.stanford.nlp.pipeline.demo directory must exist in the classpath. In this example, ., meaning the current directory, is the entirety of classpath. Therefore this particular example must be called from the directory in which edu.stanford.nlp.pipeline.demo exists.\n\nReference\n\nhttps://stackoverflow.com/a/29331827/5352399\n\nhttps://stackoverflow.com/a/18093929/5352399\n\n**StanfordCoreNLP的各个组件（annotator）按“tokenize（分词）, ssplit（断句）, pos（词性标注）, lemma（词元化）, ner（命名实体识别）, parse（语法分析）, dcoref（同义词分辨）”**","slug":"CorefNLP相关-资料&笔记","published":1,"updated":"2019-01-22T04:59:24.924Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2m30005scftitmw86lh","content":"<ul>\n<li><a href=\"http://lab.astamuse.co.jp/entry/corenlp1\" target=\"_blank\" rel=\"noopener\">一个日语关于CoreNLP的讲解</a></li>\n<li><a href=\"http://luchi007.iteye.com/blog/2285485\" target=\"_blank\" rel=\"noopener\">coreNLP的使用</a></li>\n<li><a href=\"http://www.cnblogs.com/zklidd/p/5081677.html\" target=\"_blank\" rel=\"noopener\">Stanford CoreNLP 3.6.0 中文指代消解模块调用失败的解决方案</a></li>\n<li><a href=\"http://blog.csdn.net/jiangjingxuan/article/details/54906288\" target=\"_blank\" rel=\"noopener\">使用Stanford CoreNLP工具包处理中文</a></li>\n<li><a href=\"https://toutiao.io/posts/366808/app_preview\" target=\"_blank\" rel=\"noopener\">深入迁出CoreNLP、可能暂无用</a></li>\n<li><a href=\"https://github.com/drewfarris/corenlp-examples/tree/master/src/main/java/drew/corenlp\" target=\"_blank\" rel=\"noopener\">Github上一个CoreNLP的demo</a></li>\n<li><a href=\"https://github.com/search?utf8=%E2%9C%93&amp;q=corenlp&amp;type=\" target=\"_blank\" rel=\"noopener\">Github上搜索corenlp</a></li>\n<li><a href=\"https://nlp.stanford.edu/software/dcoref.html\" target=\"_blank\" rel=\"noopener\">Stanford Deterministic Coreference Resolution System</a></li>\n<li><a href=\"https://stanfordnlp.github.io/CoreNLP/coref.html#overview\" target=\"_blank\" rel=\"noopener\">CoreNLP主页</a></li>\n<li><a href=\"https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf\" target=\"_blank\" rel=\"noopener\">The Stanford CoreNLP Natural Language Processing Toolkit/ PDF</a></li>\n<li><a href=\"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/\" target=\"_blank\" rel=\"noopener\"><strong>Coref文档</strong></a></li>\n<li><a href=\"https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/coref\" target=\"_blank\" rel=\"noopener\">Coref Github 代码</a></li>\n<li><a href=\"https://kkbac.wordpress.com/2016/11/25/%E4%BB%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BD%BF%E7%94%A8%E6%96%AF%E5%9D%A6%E7%A6%8F-corenlp/\" target=\"_blank\" rel=\"noopener\">从命令行使用斯坦福 CoreNLP</a></li>\n</ul>\n<hr>\n<hr>\n<ul>\n<li>示范代码：<br>来源：<a href=\"https://stanfordnlp.github.io/CoreNLP/coref.html#api\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/coref.html#api</a></li>\n</ul>\n<h5 id=\"输入：Barack-Obama-was-born-in-Hawaii-He-is-the-president-Obama-was-elected-in-2008\"><a href=\"#输入：Barack-Obama-was-born-in-Hawaii-He-is-the-president-Obama-was-elected-in-2008\" class=\"headerlink\" title=\"输入：Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.\"></a>输入：Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.</h5><h5 id=\"输出：\"><a href=\"#输出：\" class=\"headerlink\" title=\"输出：\"></a>输出：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.7 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.3 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.MentionAnnotator - Using mention detector type: dependency</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [1.2 sec].</span><br><span class=\"line\">---</span><br><span class=\"line\">coref chains</span><br><span class=\"line\">\tCHAIN3-[&quot;Barack Obama&quot; in sentence 1, &quot;He&quot; in sentence 2, &quot;Obama&quot; in sentence 3]</span><br><span class=\"line\">---</span><br><span class=\"line\">mentions</span><br><span class=\"line\">\tBarack Obama</span><br><span class=\"line\">\tHawaii</span><br><span class=\"line\">---</span><br><span class=\"line\">mentions</span><br><span class=\"line\">\tthe president</span><br><span class=\"line\">\tHe</span><br><span class=\"line\">---</span><br><span class=\"line\">mentions</span><br><span class=\"line\">\tObama</span><br><span class=\"line\">\t2008</span><br></pre></td></tr></table></figure>\n<h5 id=\"代码：\"><a href=\"#代码：\" class=\"headerlink\" title=\"代码：\"></a>代码：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//这个文件是原版的示例代码，没有任何的改动</span><br><span class=\"line\">import java.util.Properties;</span><br><span class=\"line\"></span><br><span class=\"line\">import edu.stanford.nlp.coref.CorefCoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.CorefChain;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.Mention;</span><br><span class=\"line\">import edu.stanford.nlp.ling.CoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.Annotation;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.StanfordCoreNLP;</span><br><span class=\"line\">import edu.stanford.nlp.util.CoreMap;</span><br><span class=\"line\"></span><br><span class=\"line\">public class GoldCode &#123;</span><br><span class=\"line\">\t  public static void main(String[] args) throws Exception &#123;</span><br><span class=\"line\">\t    Annotation document = new Annotation(&quot;Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.&quot;);</span><br><span class=\"line\">\t    Properties props = new Properties();</span><br><span class=\"line\">\t    props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,mention,coref&quot;);</span><br><span class=\"line\">\t    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);</span><br><span class=\"line\">\t    pipeline.annotate(document);</span><br><span class=\"line\">\t    System.out.println(&quot;---&quot;);</span><br><span class=\"line\">\t    System.out.println(&quot;coref chains&quot;);</span><br><span class=\"line\">\t    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) &#123;</span><br><span class=\"line\">\t      System.out.println(&quot;\\t&quot; + cc);</span><br><span class=\"line\">\t    &#125;</span><br><span class=\"line\">\t    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) &#123;</span><br><span class=\"line\">\t      System.out.println(&quot;---&quot;);</span><br><span class=\"line\">\t      System.out.println(&quot;mentions&quot;);</span><br><span class=\"line\">\t      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) &#123;</span><br><span class=\"line\">\t        System.out.println(&quot;\\t&quot; + m);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"注：以上代码只针对英文句子有效，中文句子无法成功。\"><a href=\"#注：以上代码只针对英文句子有效，中文句子无法成功。\" class=\"headerlink\" title=\"注：以上代码只针对英文句子有效，中文句子无法成功。\"></a>注：以上代码只针对英文句子有效，中文句子无法成功。</h3><h5 id=\"需要调用的包：\"><a href=\"#需要调用的包：\" class=\"headerlink\" title=\"需要调用的包：\"></a>需要调用的包：</h5><p><img src=\"http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/C871BA8A94E24C14B56D3D5B3C998330/244\" alt=\"image\"></p>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import edu.stanford.nlp.coref.CorefCoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.CorefChain;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.Mention;</span><br><span class=\"line\">import edu.stanford.nlp.ling.CoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.Annotation;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.StanfordCoreNLP;</span><br><span class=\"line\">import edu.stanford.nlp.util.CoreMap;</span><br></pre></td></tr></table></figure>\n<p>This is not a problem of StanfordCoreNlpDemo program because I ran that code in Netbeans before. The problem seems associated with classpath issue.</p>\n<p>Since the StanfordCoreNlpDemo.java file belongs to a package</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package package edu.stanford.nlp.pipeline.demo;</span><br><span class=\"line\"></span><br><span class=\"line\">public class StanfordCoreNlpDemo &#123;</span><br><span class=\"line\">    public static final void main(String[] args) throws IOException &#123;</span><br><span class=\"line\">        // code goes here</span><br><span class=\"line\">    &#125; </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Then calling the following results in Error: Could not find or load main class TheClassName.</p>\n<p>java -cp . StanfordCoreNlpDemo<br>It must be called with its fully-qualified name:</p>\n<p>java -cp . edu.stanford.nlp.pipeline.demo.StanfordCoreNlpDemo<br>And this edu.stanford.nlp.pipeline.demo directory must exist in the classpath. In this example, ., meaning the current directory, is the entirety of classpath. Therefore this particular example must be called from the directory in which edu.stanford.nlp.pipeline.demo exists.</p>\n<p>Reference</p>\n<p><a href=\"https://stackoverflow.com/a/29331827/5352399\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/a/29331827/5352399</a></p>\n<p><a href=\"https://stackoverflow.com/a/18093929/5352399\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/a/18093929/5352399</a></p>\n<p><strong>StanfordCoreNLP的各个组件（annotator）按“tokenize（分词）, ssplit（断句）, pos（词性标注）, lemma（词元化）, ner（命名实体识别）, parse（语法分析）, dcoref（同义词分辨）”</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li><a href=\"http://lab.astamuse.co.jp/entry/corenlp1\" target=\"_blank\" rel=\"noopener\">一个日语关于CoreNLP的讲解</a></li>\n<li><a href=\"http://luchi007.iteye.com/blog/2285485\" target=\"_blank\" rel=\"noopener\">coreNLP的使用</a></li>\n<li><a href=\"http://www.cnblogs.com/zklidd/p/5081677.html\" target=\"_blank\" rel=\"noopener\">Stanford CoreNLP 3.6.0 中文指代消解模块调用失败的解决方案</a></li>\n<li><a href=\"http://blog.csdn.net/jiangjingxuan/article/details/54906288\" target=\"_blank\" rel=\"noopener\">使用Stanford CoreNLP工具包处理中文</a></li>\n<li><a href=\"https://toutiao.io/posts/366808/app_preview\" target=\"_blank\" rel=\"noopener\">深入迁出CoreNLP、可能暂无用</a></li>\n<li><a href=\"https://github.com/drewfarris/corenlp-examples/tree/master/src/main/java/drew/corenlp\" target=\"_blank\" rel=\"noopener\">Github上一个CoreNLP的demo</a></li>\n<li><a href=\"https://github.com/search?utf8=%E2%9C%93&amp;q=corenlp&amp;type=\" target=\"_blank\" rel=\"noopener\">Github上搜索corenlp</a></li>\n<li><a href=\"https://nlp.stanford.edu/software/dcoref.html\" target=\"_blank\" rel=\"noopener\">Stanford Deterministic Coreference Resolution System</a></li>\n<li><a href=\"https://stanfordnlp.github.io/CoreNLP/coref.html#overview\" target=\"_blank\" rel=\"noopener\">CoreNLP主页</a></li>\n<li><a href=\"https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf\" target=\"_blank\" rel=\"noopener\">The Stanford CoreNLP Natural Language Processing Toolkit/ PDF</a></li>\n<li><a href=\"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/\" target=\"_blank\" rel=\"noopener\"><strong>Coref文档</strong></a></li>\n<li><a href=\"https://github.com/stanfordnlp/CoreNLP/tree/master/src/edu/stanford/nlp/coref\" target=\"_blank\" rel=\"noopener\">Coref Github 代码</a></li>\n<li><a href=\"https://kkbac.wordpress.com/2016/11/25/%E4%BB%8E%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BD%BF%E7%94%A8%E6%96%AF%E5%9D%A6%E7%A6%8F-corenlp/\" target=\"_blank\" rel=\"noopener\">从命令行使用斯坦福 CoreNLP</a></li>\n</ul>\n<hr>\n<hr>\n<ul>\n<li>示范代码：<br>来源：<a href=\"https://stanfordnlp.github.io/CoreNLP/coref.html#api\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/coref.html#api</a></li>\n</ul>\n<h5 id=\"输入：Barack-Obama-was-born-in-Hawaii-He-is-the-president-Obama-was-elected-in-2008\"><a href=\"#输入：Barack-Obama-was-born-in-Hawaii-He-is-the-president-Obama-was-elected-in-2008\" class=\"headerlink\" title=\"输入：Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.\"></a>输入：Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.</h5><h5 id=\"输出：\"><a href=\"#输出：\" class=\"headerlink\" title=\"输出：\"></a>输出：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.9 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [1.7 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.3 sec].</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator mention</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.MentionAnnotator - Using mention detector type: dependency</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator coref</span><br><span class=\"line\">[main] INFO edu.stanford.nlp.coref.statistical.SimpleLinearClassifier - Loading coref model edu/stanford/nlp/models/coref/statistical/ranking_model.ser.gz ... done [1.2 sec].</span><br><span class=\"line\">---</span><br><span class=\"line\">coref chains</span><br><span class=\"line\">\tCHAIN3-[&quot;Barack Obama&quot; in sentence 1, &quot;He&quot; in sentence 2, &quot;Obama&quot; in sentence 3]</span><br><span class=\"line\">---</span><br><span class=\"line\">mentions</span><br><span class=\"line\">\tBarack Obama</span><br><span class=\"line\">\tHawaii</span><br><span class=\"line\">---</span><br><span class=\"line\">mentions</span><br><span class=\"line\">\tthe president</span><br><span class=\"line\">\tHe</span><br><span class=\"line\">---</span><br><span class=\"line\">mentions</span><br><span class=\"line\">\tObama</span><br><span class=\"line\">\t2008</span><br></pre></td></tr></table></figure>\n<h5 id=\"代码：\"><a href=\"#代码：\" class=\"headerlink\" title=\"代码：\"></a>代码：</h5><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">//这个文件是原版的示例代码，没有任何的改动</span><br><span class=\"line\">import java.util.Properties;</span><br><span class=\"line\"></span><br><span class=\"line\">import edu.stanford.nlp.coref.CorefCoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.CorefChain;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.Mention;</span><br><span class=\"line\">import edu.stanford.nlp.ling.CoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.Annotation;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.StanfordCoreNLP;</span><br><span class=\"line\">import edu.stanford.nlp.util.CoreMap;</span><br><span class=\"line\"></span><br><span class=\"line\">public class GoldCode &#123;</span><br><span class=\"line\">\t  public static void main(String[] args) throws Exception &#123;</span><br><span class=\"line\">\t    Annotation document = new Annotation(&quot;Barack Obama was born in Hawaii.  He is the president. Obama was elected in 2008.&quot;);</span><br><span class=\"line\">\t    Properties props = new Properties();</span><br><span class=\"line\">\t    props.setProperty(&quot;annotators&quot;, &quot;tokenize,ssplit,pos,lemma,ner,parse,mention,coref&quot;);</span><br><span class=\"line\">\t    StanfordCoreNLP pipeline = new StanfordCoreNLP(props);</span><br><span class=\"line\">\t    pipeline.annotate(document);</span><br><span class=\"line\">\t    System.out.println(&quot;---&quot;);</span><br><span class=\"line\">\t    System.out.println(&quot;coref chains&quot;);</span><br><span class=\"line\">\t    for (CorefChain cc : document.get(CorefCoreAnnotations.CorefChainAnnotation.class).values()) &#123;</span><br><span class=\"line\">\t      System.out.println(&quot;\\t&quot; + cc);</span><br><span class=\"line\">\t    &#125;</span><br><span class=\"line\">\t    for (CoreMap sentence : document.get(CoreAnnotations.SentencesAnnotation.class)) &#123;</span><br><span class=\"line\">\t      System.out.println(&quot;---&quot;);</span><br><span class=\"line\">\t      System.out.println(&quot;mentions&quot;);</span><br><span class=\"line\">\t      for (Mention m : sentence.get(CorefCoreAnnotations.CorefMentionsAnnotation.class)) &#123;</span><br><span class=\"line\">\t        System.out.println(&quot;\\t&quot; + m);</span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"注：以上代码只针对英文句子有效，中文句子无法成功。\"><a href=\"#注：以上代码只针对英文句子有效，中文句子无法成功。\" class=\"headerlink\" title=\"注：以上代码只针对英文句子有效，中文句子无法成功。\"></a>注：以上代码只针对英文句子有效，中文句子无法成功。</h3><h5 id=\"需要调用的包：\"><a href=\"#需要调用的包：\" class=\"headerlink\" title=\"需要调用的包：\"></a>需要调用的包：</h5><p><img src=\"http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/C871BA8A94E24C14B56D3D5B3C998330/244\" alt=\"image\"></p>\n<hr>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import edu.stanford.nlp.coref.CorefCoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.CorefChain;</span><br><span class=\"line\">import edu.stanford.nlp.coref.data.Mention;</span><br><span class=\"line\">import edu.stanford.nlp.ling.CoreAnnotations;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.Annotation;</span><br><span class=\"line\">import edu.stanford.nlp.pipeline.StanfordCoreNLP;</span><br><span class=\"line\">import edu.stanford.nlp.util.CoreMap;</span><br></pre></td></tr></table></figure>\n<p>This is not a problem of StanfordCoreNlpDemo program because I ran that code in Netbeans before. The problem seems associated with classpath issue.</p>\n<p>Since the StanfordCoreNlpDemo.java file belongs to a package</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package package edu.stanford.nlp.pipeline.demo;</span><br><span class=\"line\"></span><br><span class=\"line\">public class StanfordCoreNlpDemo &#123;</span><br><span class=\"line\">    public static final void main(String[] args) throws IOException &#123;</span><br><span class=\"line\">        // code goes here</span><br><span class=\"line\">    &#125; </span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Then calling the following results in Error: Could not find or load main class TheClassName.</p>\n<p>java -cp . StanfordCoreNlpDemo<br>It must be called with its fully-qualified name:</p>\n<p>java -cp . edu.stanford.nlp.pipeline.demo.StanfordCoreNlpDemo<br>And this edu.stanford.nlp.pipeline.demo directory must exist in the classpath. In this example, ., meaning the current directory, is the entirety of classpath. Therefore this particular example must be called from the directory in which edu.stanford.nlp.pipeline.demo exists.</p>\n<p>Reference</p>\n<p><a href=\"https://stackoverflow.com/a/29331827/5352399\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/a/29331827/5352399</a></p>\n<p><a href=\"https://stackoverflow.com/a/18093929/5352399\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/a/18093929/5352399</a></p>\n<p><strong>StanfordCoreNLP的各个组件（annotator）按“tokenize（分词）, ssplit（断句）, pos（词性标注）, lemma（词元化）, ner（命名实体识别）, parse（语法分析）, dcoref（同义词分辨）”</strong></p>\n"},{"title":"Java学习笔记","date":"2017-08-30T16:00:00.000Z","description":"java读书笔记","top":1,"_content":"***\n##### 参考图书：21天学通Java（第六版），人民邮电出版社，[美]Rogers Cadenhead\n***\n#### 第一章\n- 类变量（class variable）：定义类的属性，不同的实例公用这个属性\n- 类方法（class method）：适用于类本身\n<p>\n- 三个重要概念：继承、借口、包\n- 继承其他类的叫子类，被继承的叫超类\n- 在子类中创建一个防止调用超类中定义的方法。为此，方法的名称、返回值和参数必须与超类方法相同，这称谓**覆盖**\n- Java单继承：仅有一个超类\n\n#### 第二章\n- 变量命名法：小驼峰\n- 数据类型（基本）：\n  - 存储整数：byte、short、int、long\n  - 浮点数：float、double\n  - char、boolean\n- 定义常量： 关键字：**final**。如：final float PI = 3.14；\n- 输出：System.out.println(\"str1\" + \"str2\");  // 在输出字符串后面回车，区别于print\n- 字符串对象是Java中的真正对象\n- 逻辑运算符：& 与 &&。如果用& ，不论两边式子真假，两边都会计算；如果是&&，lazy模式，若左边为true，右边则不会计算。\n\n#### 第三章\n- 参数数目和类型是区分构造函数的唯一途径\n- 类变量：关键字：**static**，修改它会改变所有的实例。\n- System.out.format(\"Test:$，d%n\", data）；  // %，d是十进制数，%n是回车符\n- System位于java.lang包中\n- System.out 是一个类型，他存储了PrintSteam的一个实例\n- 引用是一个地址，指明了对象的变量和方法的存储位置\n- 将对象赋值给变量或者作为参数传递给方法的时候，使用的仅仅是对象的引用。\n\n```\nInterger dataCount = new Integer(10);\nint newCount = dataCount.intValue();\n// 这里，dataCount是个Integer对象\n```\n- 若判断对象所属于的类：\n\n```\n1. String Name = key.getClass().getName();\n2. 用instanceof\nboolean check2 = pt instanceof String;\n```\n#### 第四章\n- 块语句，即{}，但是直接用并不常见\n- 在块中声明的局部变量创建了作用域\n- if，需要加括号\n- 三目运算：test?trueresult : falseresult;\n- break & continue 标号，标记要跳到哪里，P65\n\n#### 第五章\n- 指定超类：关键字：**extends**\n```\nclass Son extends Father {\npass\n}\n```\n- 类变量：关键字：**static**；例子：static int num；\n- this：类似于python中的self\n- P80 构造函数相关\n- 调用超类的构造函数：super.methodname(arguments)\n\n#### 第六章\n- static：创建类方法和类变量\n- final：设置常量。常与static一起使用，例如：static final String TITLE = \"sss\";\n- final方法不能被子类覆盖。\n- final类不能被继承。在final类中，所有的方法都是final的，无需再次限定。","source":"_posts/Java学习笔记.md","raw":"---\ntitle: Java学习笔记\ndate: 2017-8-31\ntags:\n\t- 笔记\n\t- Java\ndescription: java读书笔记\ntop: 1\n---\n***\n##### 参考图书：21天学通Java（第六版），人民邮电出版社，[美]Rogers Cadenhead\n***\n#### 第一章\n- 类变量（class variable）：定义类的属性，不同的实例公用这个属性\n- 类方法（class method）：适用于类本身\n<p>\n- 三个重要概念：继承、借口、包\n- 继承其他类的叫子类，被继承的叫超类\n- 在子类中创建一个防止调用超类中定义的方法。为此，方法的名称、返回值和参数必须与超类方法相同，这称谓**覆盖**\n- Java单继承：仅有一个超类\n\n#### 第二章\n- 变量命名法：小驼峰\n- 数据类型（基本）：\n  - 存储整数：byte、short、int、long\n  - 浮点数：float、double\n  - char、boolean\n- 定义常量： 关键字：**final**。如：final float PI = 3.14；\n- 输出：System.out.println(\"str1\" + \"str2\");  // 在输出字符串后面回车，区别于print\n- 字符串对象是Java中的真正对象\n- 逻辑运算符：& 与 &&。如果用& ，不论两边式子真假，两边都会计算；如果是&&，lazy模式，若左边为true，右边则不会计算。\n\n#### 第三章\n- 参数数目和类型是区分构造函数的唯一途径\n- 类变量：关键字：**static**，修改它会改变所有的实例。\n- System.out.format(\"Test:$，d%n\", data）；  // %，d是十进制数，%n是回车符\n- System位于java.lang包中\n- System.out 是一个类型，他存储了PrintSteam的一个实例\n- 引用是一个地址，指明了对象的变量和方法的存储位置\n- 将对象赋值给变量或者作为参数传递给方法的时候，使用的仅仅是对象的引用。\n\n```\nInterger dataCount = new Integer(10);\nint newCount = dataCount.intValue();\n// 这里，dataCount是个Integer对象\n```\n- 若判断对象所属于的类：\n\n```\n1. String Name = key.getClass().getName();\n2. 用instanceof\nboolean check2 = pt instanceof String;\n```\n#### 第四章\n- 块语句，即{}，但是直接用并不常见\n- 在块中声明的局部变量创建了作用域\n- if，需要加括号\n- 三目运算：test?trueresult : falseresult;\n- break & continue 标号，标记要跳到哪里，P65\n\n#### 第五章\n- 指定超类：关键字：**extends**\n```\nclass Son extends Father {\npass\n}\n```\n- 类变量：关键字：**static**；例子：static int num；\n- this：类似于python中的self\n- P80 构造函数相关\n- 调用超类的构造函数：super.methodname(arguments)\n\n#### 第六章\n- static：创建类方法和类变量\n- final：设置常量。常与static一起使用，例如：static final String TITLE = \"sss\";\n- final方法不能被子类覆盖。\n- final类不能被继承。在final类中，所有的方法都是final的，无需再次限定。","slug":"Java学习笔记","published":1,"updated":"2019-01-22T04:59:24.924Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2m70006scft5ac1dv8q","content":"<hr>\n<h5 id=\"参考图书：21天学通Java（第六版），人民邮电出版社，-美-Rogers-Cadenhead\"><a href=\"#参考图书：21天学通Java（第六版），人民邮电出版社，-美-Rogers-Cadenhead\" class=\"headerlink\" title=\"参考图书：21天学通Java（第六版），人民邮电出版社，[美]Rogers Cadenhead\"></a>参考图书：21天学通Java（第六版），人民邮电出版社，[美]Rogers Cadenhead</h5><hr>\n<h4 id=\"第一章\"><a href=\"#第一章\" class=\"headerlink\" title=\"第一章\"></a>第一章</h4><ul>\n<li>类变量（class variable）：定义类的属性，不同的实例公用这个属性</li>\n<li>类方法（class method）：适用于类本身<p></p></li>\n<li>三个重要概念：继承、借口、包</li>\n<li>继承其他类的叫子类，被继承的叫超类</li>\n<li>在子类中创建一个防止调用超类中定义的方法。为此，方法的名称、返回值和参数必须与超类方法相同，这称谓<strong>覆盖</strong></li>\n<li>Java单继承：仅有一个超类</li>\n</ul>\n<h4 id=\"第二章\"><a href=\"#第二章\" class=\"headerlink\" title=\"第二章\"></a>第二章</h4><ul>\n<li>变量命名法：小驼峰</li>\n<li>数据类型（基本）：<ul>\n<li>存储整数：byte、short、int、long</li>\n<li>浮点数：float、double</li>\n<li>char、boolean</li>\n</ul>\n</li>\n<li>定义常量： 关键字：<strong>final</strong>。如：final float PI = 3.14；</li>\n<li>输出：System.out.println(“str1” + “str2”);  // 在输出字符串后面回车，区别于print</li>\n<li>字符串对象是Java中的真正对象</li>\n<li>逻辑运算符：&amp; 与 &amp;&amp;。如果用&amp; ，不论两边式子真假，两边都会计算；如果是&amp;&amp;，lazy模式，若左边为true，右边则不会计算。</li>\n</ul>\n<h4 id=\"第三章\"><a href=\"#第三章\" class=\"headerlink\" title=\"第三章\"></a>第三章</h4><ul>\n<li>参数数目和类型是区分构造函数的唯一途径</li>\n<li>类变量：关键字：<strong>static</strong>，修改它会改变所有的实例。</li>\n<li>System.out.format(“Test:$，d%n”, data）；  // %，d是十进制数，%n是回车符</li>\n<li>System位于java.lang包中</li>\n<li>System.out 是一个类型，他存储了PrintSteam的一个实例</li>\n<li>引用是一个地址，指明了对象的变量和方法的存储位置</li>\n<li>将对象赋值给变量或者作为参数传递给方法的时候，使用的仅仅是对象的引用。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Interger dataCount = new Integer(10);</span><br><span class=\"line\">int newCount = dataCount.intValue();</span><br><span class=\"line\">// 这里，dataCount是个Integer对象</span><br></pre></td></tr></table></figure>\n<ul>\n<li>若判断对象所属于的类：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. String Name = key.getClass().getName();</span><br><span class=\"line\">2. 用instanceof</span><br><span class=\"line\">boolean check2 = pt instanceof String;</span><br></pre></td></tr></table></figure>\n<h4 id=\"第四章\"><a href=\"#第四章\" class=\"headerlink\" title=\"第四章\"></a>第四章</h4><ul>\n<li>块语句，即{}，但是直接用并不常见</li>\n<li>在块中声明的局部变量创建了作用域</li>\n<li>if，需要加括号</li>\n<li>三目运算：test?trueresult : falseresult;</li>\n<li>break &amp; continue 标号，标记要跳到哪里，P65</li>\n</ul>\n<h4 id=\"第五章\"><a href=\"#第五章\" class=\"headerlink\" title=\"第五章\"></a>第五章</h4><ul>\n<li><p>指定超类：关键字：<strong>extends</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Son extends Father &#123;</span><br><span class=\"line\">pass</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>类变量：关键字：<strong>static</strong>；例子：static int num；</p>\n</li>\n<li>this：类似于python中的self</li>\n<li>P80 构造函数相关</li>\n<li>调用超类的构造函数：super.methodname(arguments)</li>\n</ul>\n<h4 id=\"第六章\"><a href=\"#第六章\" class=\"headerlink\" title=\"第六章\"></a>第六章</h4><ul>\n<li>static：创建类方法和类变量</li>\n<li>final：设置常量。常与static一起使用，例如：static final String TITLE = “sss”;</li>\n<li>final方法不能被子类覆盖。</li>\n<li>final类不能被继承。在final类中，所有的方法都是final的，无需再次限定。</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<hr>\n<h5 id=\"参考图书：21天学通Java（第六版），人民邮电出版社，-美-Rogers-Cadenhead\"><a href=\"#参考图书：21天学通Java（第六版），人民邮电出版社，-美-Rogers-Cadenhead\" class=\"headerlink\" title=\"参考图书：21天学通Java（第六版），人民邮电出版社，[美]Rogers Cadenhead\"></a>参考图书：21天学通Java（第六版），人民邮电出版社，[美]Rogers Cadenhead</h5><hr>\n<h4 id=\"第一章\"><a href=\"#第一章\" class=\"headerlink\" title=\"第一章\"></a>第一章</h4><ul>\n<li>类变量（class variable）：定义类的属性，不同的实例公用这个属性</li>\n<li>类方法（class method）：适用于类本身<p></p></li>\n<li>三个重要概念：继承、借口、包</li>\n<li>继承其他类的叫子类，被继承的叫超类</li>\n<li>在子类中创建一个防止调用超类中定义的方法。为此，方法的名称、返回值和参数必须与超类方法相同，这称谓<strong>覆盖</strong></li>\n<li>Java单继承：仅有一个超类</li>\n</ul>\n<h4 id=\"第二章\"><a href=\"#第二章\" class=\"headerlink\" title=\"第二章\"></a>第二章</h4><ul>\n<li>变量命名法：小驼峰</li>\n<li>数据类型（基本）：<ul>\n<li>存储整数：byte、short、int、long</li>\n<li>浮点数：float、double</li>\n<li>char、boolean</li>\n</ul>\n</li>\n<li>定义常量： 关键字：<strong>final</strong>。如：final float PI = 3.14；</li>\n<li>输出：System.out.println(“str1” + “str2”);  // 在输出字符串后面回车，区别于print</li>\n<li>字符串对象是Java中的真正对象</li>\n<li>逻辑运算符：&amp; 与 &amp;&amp;。如果用&amp; ，不论两边式子真假，两边都会计算；如果是&amp;&amp;，lazy模式，若左边为true，右边则不会计算。</li>\n</ul>\n<h4 id=\"第三章\"><a href=\"#第三章\" class=\"headerlink\" title=\"第三章\"></a>第三章</h4><ul>\n<li>参数数目和类型是区分构造函数的唯一途径</li>\n<li>类变量：关键字：<strong>static</strong>，修改它会改变所有的实例。</li>\n<li>System.out.format(“Test:$，d%n”, data）；  // %，d是十进制数，%n是回车符</li>\n<li>System位于java.lang包中</li>\n<li>System.out 是一个类型，他存储了PrintSteam的一个实例</li>\n<li>引用是一个地址，指明了对象的变量和方法的存储位置</li>\n<li>将对象赋值给变量或者作为参数传递给方法的时候，使用的仅仅是对象的引用。</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Interger dataCount = new Integer(10);</span><br><span class=\"line\">int newCount = dataCount.intValue();</span><br><span class=\"line\">// 这里，dataCount是个Integer对象</span><br></pre></td></tr></table></figure>\n<ul>\n<li>若判断对象所属于的类：</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. String Name = key.getClass().getName();</span><br><span class=\"line\">2. 用instanceof</span><br><span class=\"line\">boolean check2 = pt instanceof String;</span><br></pre></td></tr></table></figure>\n<h4 id=\"第四章\"><a href=\"#第四章\" class=\"headerlink\" title=\"第四章\"></a>第四章</h4><ul>\n<li>块语句，即{}，但是直接用并不常见</li>\n<li>在块中声明的局部变量创建了作用域</li>\n<li>if，需要加括号</li>\n<li>三目运算：test?trueresult : falseresult;</li>\n<li>break &amp; continue 标号，标记要跳到哪里，P65</li>\n</ul>\n<h4 id=\"第五章\"><a href=\"#第五章\" class=\"headerlink\" title=\"第五章\"></a>第五章</h4><ul>\n<li><p>指定超类：关键字：<strong>extends</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Son extends Father &#123;</span><br><span class=\"line\">pass</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>类变量：关键字：<strong>static</strong>；例子：static int num；</p>\n</li>\n<li>this：类似于python中的self</li>\n<li>P80 构造函数相关</li>\n<li>调用超类的构造函数：super.methodname(arguments)</li>\n</ul>\n<h4 id=\"第六章\"><a href=\"#第六章\" class=\"headerlink\" title=\"第六章\"></a>第六章</h4><ul>\n<li>static：创建类方法和类变量</li>\n<li>final：设置常量。常与static一起使用，例如：static final String TITLE = “sss”;</li>\n<li>final方法不能被子类覆盖。</li>\n<li>final类不能被继承。在final类中，所有的方法都是final的，无需再次限定。</li>\n</ul>\n"},{"title":"End2end-coreference-resolution","date":"2018-12-28T17:01:36.000Z","description":"学习论文时的笔记-手写版","top":1,"_content":"\n#### 原文\n> https://arxiv.org/abs/1707.07045\n\n#### 开源代码\n> https://github.com/kentonl/e2e-coref\n\n#### 笔记\n\n![image](/end2end-coreference-resolution/2.png)\n\n![image](/end2end-coreference-resolution/3.png)\n\n![image](/end2end-coreference-resolution/4.png)\n\n![image](/end2end-coreference-resolution/5.png)\n\n![image](/end2end-coreference-resolution/6.png)\n\n![image](/end2end-coreference-resolution/7.png)\n\n![image](/end2end-coreference-resolution/8.png)\n\n![image](/end2end-coreference-resolution/9.png)\n\n![image](/end2end-coreference-resolution/10.png)","source":"_posts/end2end-coreference-resolution.md","raw":"---\ntitle: End2end-coreference-resolution\ndate: 2018-12-29 01:01:36\ntags:\n    - CR\n    - NLP\ndescription: 学习论文时的笔记-手写版\ntop: 1\n---\n\n#### 原文\n> https://arxiv.org/abs/1707.07045\n\n#### 开源代码\n> https://github.com/kentonl/e2e-coref\n\n#### 笔记\n\n![image](/end2end-coreference-resolution/2.png)\n\n![image](/end2end-coreference-resolution/3.png)\n\n![image](/end2end-coreference-resolution/4.png)\n\n![image](/end2end-coreference-resolution/5.png)\n\n![image](/end2end-coreference-resolution/6.png)\n\n![image](/end2end-coreference-resolution/7.png)\n\n![image](/end2end-coreference-resolution/8.png)\n\n![image](/end2end-coreference-resolution/9.png)\n\n![image](/end2end-coreference-resolution/10.png)","slug":"end2end-coreference-resolution","published":1,"updated":"2019-01-22T04:59:24.932Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2md0007scft71io7szx","content":"<h4 id=\"原文\"><a href=\"#原文\" class=\"headerlink\" title=\"原文\"></a>原文</h4><blockquote>\n<p><a href=\"https://arxiv.org/abs/1707.07045\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1707.07045</a></p>\n</blockquote>\n<h4 id=\"开源代码\"><a href=\"#开源代码\" class=\"headerlink\" title=\"开源代码\"></a>开源代码</h4><blockquote>\n<p><a href=\"https://github.com/kentonl/e2e-coref\" target=\"_blank\" rel=\"noopener\">https://github.com/kentonl/e2e-coref</a></p>\n</blockquote>\n<h4 id=\"笔记\"><a href=\"#笔记\" class=\"headerlink\" title=\"笔记\"></a>笔记</h4><p><img src=\"/2018/12/29/end2end-coreference-resolution/2.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/3.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/4.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/5.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/6.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/7.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/8.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/9.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/10.png\" alt=\"image\"></p>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"原文\"><a href=\"#原文\" class=\"headerlink\" title=\"原文\"></a>原文</h4><blockquote>\n<p><a href=\"https://arxiv.org/abs/1707.07045\" target=\"_blank\" rel=\"noopener\">https://arxiv.org/abs/1707.07045</a></p>\n</blockquote>\n<h4 id=\"开源代码\"><a href=\"#开源代码\" class=\"headerlink\" title=\"开源代码\"></a>开源代码</h4><blockquote>\n<p><a href=\"https://github.com/kentonl/e2e-coref\" target=\"_blank\" rel=\"noopener\">https://github.com/kentonl/e2e-coref</a></p>\n</blockquote>\n<h4 id=\"笔记\"><a href=\"#笔记\" class=\"headerlink\" title=\"笔记\"></a>笔记</h4><p><img src=\"/2018/12/29/end2end-coreference-resolution/2.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/3.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/4.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/5.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/6.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/7.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/8.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/9.png\" alt=\"image\"></p>\n<p><img src=\"/2018/12/29/end2end-coreference-resolution/10.png\" alt=\"image\"></p>\n"},{"title":"使用python生成markdown格式的日历","date":"2018-12-04T16:03:22.000Z","description":"使用python生成某年某月的月历，输出成markdown格式","top":1,"_content":"\n### 作用：\n使用python生成日报中的日历\n\n### 参考：\n- [python实现输出日历-csdn](https://blog.csdn.net/jlshix/article/details/46970563)\n\n### 代码：\n\n    \n\t# coding=utf-8\n\t\t \n\tdef is_leap_year(year):\n\t    # 判断是否为闰年\n\t    if year % 4 == 0 and year % 100 != 0 or year % 400 == 0:\n\t        return True\n\t    else:\n\t        return False\n\t \n\t \n\tdef get_num_of_days_in_month(year, month):\n\t    # 给定年月返回月份的天数\n\t    if month in (1, 3, 5, 7, 8, 10, 12):\n\t        return 31\n\t    elif month in (4, 6, 9, 11):\n\t        return 30\n\t    elif is_leap_year(year):\n\t        return 29\n\t    else:\n\t        return 28\n\t \n\t \n\tdef get_total_num_of_day(year, month):\n\t    # 自1800年1月1日以来过了多少天\n\t    days = 0\n\t    for y in range(1800, year):\n\t        if is_leap_year(y):\n\t            days += 366\n\t        else:\n\t            days += 365\n\t \n\t    for m in range(1, month):\n\t        days += get_num_of_days_in_month(year, m)\n\t \n\t    return days\n\t \n\t \n\tdef get_start_day(year, month):\n\t    # 返回当月1日是星期几，由1800.01.01是星期三推算\n\t    return 3 + get_total_num_of_day(year, month) % 7\n\t \n\t \n\t# 月份与名称对应的字典\n\tmonth_dict = {1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n\t              7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n\t \n\t \n\tdef get_month_name(month):\n\t    # 返回当月的名称\n\t    return month_dict[month]\n\t \n\t \n\tdef print_month_title(year, month):\n\t    # 打印日历的首部\n\n\t    cal.write('         ' + str(get_month_name(month)) +  '   ' + str(year) + '          \\n')\n\t    cal.write('Sun | Mon | Tue  | Wed | Thu | Fri | Sat \\n')\n\t    cal.write('---| ---| ---| ---| ---| ---| ---|\\n')\n\t \n\t \n\tdef print_month_body(year, month):\n\t    '''\n\t    打印日历正文\n\t    格式说明：空两个空格，每天的长度为5\n\t    需要注意的是print加逗号会多一个空格\n\t    '''\n\t    i = get_start_day(year, month)\n\t    if i != 7:\n\t        # cal.write(' ') # 打印行首的两个空格\n\t        cal.write('  |' * (i %7))   # 从星期几开始则空5*几个空格\n\t    for j in range(1, get_num_of_days_in_month(year, month)+1):\n\t        cal.write(' [' + str(j) + '](#' + str(month) + str(j) + ') |')# 宽度控制，4+1=5\n\t        i += 1\n\t        if i % 7 == 0:  # i用于计数和换行\n\t            cal.write('\\n')   # 每换行一次行首继续空格\n\n\t \n\t \n\t#   主函数部分\n\t# year = int(raw_input(\"Please input target year:\"))\n\t# month = int(raw_input(\"Please input target month:\"))\n\tyear = 2018\n\tmonth = 12\n\tcal = open(str(year) + '-' + str(month) + '-日历markdown版.txt','w')\n\tprint_month_title(year, month)\n\tprint_month_body(year, month)\n\tcal.close()\n\n\n---\n\n#### 主要是修改了输出部分的代码，python实现日历主要是感谢[原博主](https://blog.csdn.net/jlshix/article/details/46970563)，在本地生成的markdown文件内容如下所示：\n\n![image](/how-to-use-python-to-build-markdown-calc/1.png)\n\n\n效果：\n\n![image](/how-to-use-python-to-build-markdown-calc/2.JPG)\n\n\n### 其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\n\t<span id=\"122\">12-2</span>\n此处id即是日历中的#号后面的id\n\n\n### update:居中所使用的< center>貌似会使表格失效，因此去掉了该标签","source":"_posts/how-to-use-python-to-build-markdown-calc.md","raw":"---\ntitle: 使用python生成markdown格式的日历\ndate: 2018-12-05 00:03:22\ndescription: 使用python生成某年某月的月历，输出成markdown格式\ntags: \n\t- 随笔\n\t- python\ntop: 1\n---\n\n### 作用：\n使用python生成日报中的日历\n\n### 参考：\n- [python实现输出日历-csdn](https://blog.csdn.net/jlshix/article/details/46970563)\n\n### 代码：\n\n    \n\t# coding=utf-8\n\t\t \n\tdef is_leap_year(year):\n\t    # 判断是否为闰年\n\t    if year % 4 == 0 and year % 100 != 0 or year % 400 == 0:\n\t        return True\n\t    else:\n\t        return False\n\t \n\t \n\tdef get_num_of_days_in_month(year, month):\n\t    # 给定年月返回月份的天数\n\t    if month in (1, 3, 5, 7, 8, 10, 12):\n\t        return 31\n\t    elif month in (4, 6, 9, 11):\n\t        return 30\n\t    elif is_leap_year(year):\n\t        return 29\n\t    else:\n\t        return 28\n\t \n\t \n\tdef get_total_num_of_day(year, month):\n\t    # 自1800年1月1日以来过了多少天\n\t    days = 0\n\t    for y in range(1800, year):\n\t        if is_leap_year(y):\n\t            days += 366\n\t        else:\n\t            days += 365\n\t \n\t    for m in range(1, month):\n\t        days += get_num_of_days_in_month(year, m)\n\t \n\t    return days\n\t \n\t \n\tdef get_start_day(year, month):\n\t    # 返回当月1日是星期几，由1800.01.01是星期三推算\n\t    return 3 + get_total_num_of_day(year, month) % 7\n\t \n\t \n\t# 月份与名称对应的字典\n\tmonth_dict = {1: 'January', 2: 'February', 3: 'March', 4: 'April', 5: 'May', 6: 'June',\n\t              7: 'July', 8: 'August', 9: 'September', 10: 'October', 11: 'November', 12: 'December'}\n\t \n\t \n\tdef get_month_name(month):\n\t    # 返回当月的名称\n\t    return month_dict[month]\n\t \n\t \n\tdef print_month_title(year, month):\n\t    # 打印日历的首部\n\n\t    cal.write('         ' + str(get_month_name(month)) +  '   ' + str(year) + '          \\n')\n\t    cal.write('Sun | Mon | Tue  | Wed | Thu | Fri | Sat \\n')\n\t    cal.write('---| ---| ---| ---| ---| ---| ---|\\n')\n\t \n\t \n\tdef print_month_body(year, month):\n\t    '''\n\t    打印日历正文\n\t    格式说明：空两个空格，每天的长度为5\n\t    需要注意的是print加逗号会多一个空格\n\t    '''\n\t    i = get_start_day(year, month)\n\t    if i != 7:\n\t        # cal.write(' ') # 打印行首的两个空格\n\t        cal.write('  |' * (i %7))   # 从星期几开始则空5*几个空格\n\t    for j in range(1, get_num_of_days_in_month(year, month)+1):\n\t        cal.write(' [' + str(j) + '](#' + str(month) + str(j) + ') |')# 宽度控制，4+1=5\n\t        i += 1\n\t        if i % 7 == 0:  # i用于计数和换行\n\t            cal.write('\\n')   # 每换行一次行首继续空格\n\n\t \n\t \n\t#   主函数部分\n\t# year = int(raw_input(\"Please input target year:\"))\n\t# month = int(raw_input(\"Please input target month:\"))\n\tyear = 2018\n\tmonth = 12\n\tcal = open(str(year) + '-' + str(month) + '-日历markdown版.txt','w')\n\tprint_month_title(year, month)\n\tprint_month_body(year, month)\n\tcal.close()\n\n\n---\n\n#### 主要是修改了输出部分的代码，python实现日历主要是感谢[原博主](https://blog.csdn.net/jlshix/article/details/46970563)，在本地生成的markdown文件内容如下所示：\n\n![image](/how-to-use-python-to-build-markdown-calc/1.png)\n\n\n效果：\n\n![image](/how-to-use-python-to-build-markdown-calc/2.JPG)\n\n\n### 其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\n\t<span id=\"122\">12-2</span>\n此处id即是日历中的#号后面的id\n\n\n### update:居中所使用的< center>貌似会使表格失效，因此去掉了该标签","slug":"how-to-use-python-to-build-markdown-calc","published":1,"updated":"2019-01-22T04:59:24.952Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2mj000ascftxer5g1ay","content":"<h3 id=\"作用：\"><a href=\"#作用：\" class=\"headerlink\" title=\"作用：\"></a>作用：</h3><p>使用python生成日报中的日历</p>\n<h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><ul>\n<li><a href=\"https://blog.csdn.net/jlshix/article/details/46970563\" target=\"_blank\" rel=\"noopener\">python实现输出日历-csdn</a></li>\n</ul>\n<h3 id=\"代码：\"><a href=\"#代码：\" class=\"headerlink\" title=\"代码：\"></a>代码：</h3><pre><code># coding=utf-8\n\ndef is_leap_year(year):\n    # 判断是否为闰年\n    if year % 4 == 0 and year % 100 != 0 or year % 400 == 0:\n        return True\n    else:\n        return False\n\n\ndef get_num_of_days_in_month(year, month):\n    # 给定年月返回月份的天数\n    if month in (1, 3, 5, 7, 8, 10, 12):\n        return 31\n    elif month in (4, 6, 9, 11):\n        return 30\n    elif is_leap_year(year):\n        return 29\n    else:\n        return 28\n\n\ndef get_total_num_of_day(year, month):\n    # 自1800年1月1日以来过了多少天\n    days = 0\n    for y in range(1800, year):\n        if is_leap_year(y):\n            days += 366\n        else:\n            days += 365\n\n    for m in range(1, month):\n        days += get_num_of_days_in_month(year, m)\n\n    return days\n\n\ndef get_start_day(year, month):\n    # 返回当月1日是星期几，由1800.01.01是星期三推算\n    return 3 + get_total_num_of_day(year, month) % 7\n\n\n# 月份与名称对应的字典\nmonth_dict = {1: &apos;January&apos;, 2: &apos;February&apos;, 3: &apos;March&apos;, 4: &apos;April&apos;, 5: &apos;May&apos;, 6: &apos;June&apos;,\n              7: &apos;July&apos;, 8: &apos;August&apos;, 9: &apos;September&apos;, 10: &apos;October&apos;, 11: &apos;November&apos;, 12: &apos;December&apos;}\n\n\ndef get_month_name(month):\n    # 返回当月的名称\n    return month_dict[month]\n\n\ndef print_month_title(year, month):\n    # 打印日历的首部\n\n    cal.write(&apos;         &apos; + str(get_month_name(month)) +  &apos;   &apos; + str(year) + &apos;          \\n&apos;)\n    cal.write(&apos;Sun | Mon | Tue  | Wed | Thu | Fri | Sat \\n&apos;)\n    cal.write(&apos;---| ---| ---| ---| ---| ---| ---|\\n&apos;)\n\n\ndef print_month_body(year, month):\n    &apos;&apos;&apos;\n    打印日历正文\n    格式说明：空两个空格，每天的长度为5\n    需要注意的是print加逗号会多一个空格\n    &apos;&apos;&apos;\n    i = get_start_day(year, month)\n    if i != 7:\n        # cal.write(&apos; &apos;) # 打印行首的两个空格\n        cal.write(&apos;  |&apos; * (i %7))   # 从星期几开始则空5*几个空格\n    for j in range(1, get_num_of_days_in_month(year, month)+1):\n        cal.write(&apos; [&apos; + str(j) + &apos;](#&apos; + str(month) + str(j) + &apos;) |&apos;)# 宽度控制，4+1=5\n        i += 1\n        if i % 7 == 0:  # i用于计数和换行\n            cal.write(&apos;\\n&apos;)   # 每换行一次行首继续空格\n\n\n\n#   主函数部分\n# year = int(raw_input(&quot;Please input target year:&quot;))\n# month = int(raw_input(&quot;Please input target month:&quot;))\nyear = 2018\nmonth = 12\ncal = open(str(year) + &apos;-&apos; + str(month) + &apos;-日历markdown版.txt&apos;,&apos;w&apos;)\nprint_month_title(year, month)\nprint_month_body(year, month)\ncal.close()\n</code></pre><hr>\n<h4 id=\"主要是修改了输出部分的代码，python实现日历主要是感谢原博主，在本地生成的markdown文件内容如下所示：\"><a href=\"#主要是修改了输出部分的代码，python实现日历主要是感谢原博主，在本地生成的markdown文件内容如下所示：\" class=\"headerlink\" title=\"主要是修改了输出部分的代码，python实现日历主要是感谢原博主，在本地生成的markdown文件内容如下所示：\"></a>主要是修改了输出部分的代码，python实现日历主要是感谢<a href=\"https://blog.csdn.net/jlshix/article/details/46970563\" target=\"_blank\" rel=\"noopener\">原博主</a>，在本地生成的markdown文件内容如下所示：</h4><p><img src=\"/2018/12/05/how-to-use-python-to-build-markdown-calc/1.png\" alt=\"image\"></p>\n<p>效果：</p>\n<p><img src=\"/2018/12/05/how-to-use-python-to-build-markdown-calc/2.JPG\" alt=\"image\"></p>\n<h3 id=\"其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\"><a href=\"#其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\" class=\"headerlink\" title=\"其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\"></a>其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：</h3><pre><code>&lt;span id=&quot;122&quot;&gt;12-2&lt;/span&gt;\n</code></pre><p>此处id即是日历中的#号后面的id</p>\n<h3 id=\"update-居中所使用的-lt-center-gt-貌似会使表格失效，因此去掉了该标签\"><a href=\"#update-居中所使用的-lt-center-gt-貌似会使表格失效，因此去掉了该标签\" class=\"headerlink\" title=\"update:居中所使用的&lt; center&gt;貌似会使表格失效，因此去掉了该标签\"></a>update:居中所使用的&lt; center&gt;貌似会使表格失效，因此去掉了该标签</h3>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"作用：\"><a href=\"#作用：\" class=\"headerlink\" title=\"作用：\"></a>作用：</h3><p>使用python生成日报中的日历</p>\n<h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><ul>\n<li><a href=\"https://blog.csdn.net/jlshix/article/details/46970563\" target=\"_blank\" rel=\"noopener\">python实现输出日历-csdn</a></li>\n</ul>\n<h3 id=\"代码：\"><a href=\"#代码：\" class=\"headerlink\" title=\"代码：\"></a>代码：</h3><pre><code># coding=utf-8\n\ndef is_leap_year(year):\n    # 判断是否为闰年\n    if year % 4 == 0 and year % 100 != 0 or year % 400 == 0:\n        return True\n    else:\n        return False\n\n\ndef get_num_of_days_in_month(year, month):\n    # 给定年月返回月份的天数\n    if month in (1, 3, 5, 7, 8, 10, 12):\n        return 31\n    elif month in (4, 6, 9, 11):\n        return 30\n    elif is_leap_year(year):\n        return 29\n    else:\n        return 28\n\n\ndef get_total_num_of_day(year, month):\n    # 自1800年1月1日以来过了多少天\n    days = 0\n    for y in range(1800, year):\n        if is_leap_year(y):\n            days += 366\n        else:\n            days += 365\n\n    for m in range(1, month):\n        days += get_num_of_days_in_month(year, m)\n\n    return days\n\n\ndef get_start_day(year, month):\n    # 返回当月1日是星期几，由1800.01.01是星期三推算\n    return 3 + get_total_num_of_day(year, month) % 7\n\n\n# 月份与名称对应的字典\nmonth_dict = {1: &apos;January&apos;, 2: &apos;February&apos;, 3: &apos;March&apos;, 4: &apos;April&apos;, 5: &apos;May&apos;, 6: &apos;June&apos;,\n              7: &apos;July&apos;, 8: &apos;August&apos;, 9: &apos;September&apos;, 10: &apos;October&apos;, 11: &apos;November&apos;, 12: &apos;December&apos;}\n\n\ndef get_month_name(month):\n    # 返回当月的名称\n    return month_dict[month]\n\n\ndef print_month_title(year, month):\n    # 打印日历的首部\n\n    cal.write(&apos;         &apos; + str(get_month_name(month)) +  &apos;   &apos; + str(year) + &apos;          \\n&apos;)\n    cal.write(&apos;Sun | Mon | Tue  | Wed | Thu | Fri | Sat \\n&apos;)\n    cal.write(&apos;---| ---| ---| ---| ---| ---| ---|\\n&apos;)\n\n\ndef print_month_body(year, month):\n    &apos;&apos;&apos;\n    打印日历正文\n    格式说明：空两个空格，每天的长度为5\n    需要注意的是print加逗号会多一个空格\n    &apos;&apos;&apos;\n    i = get_start_day(year, month)\n    if i != 7:\n        # cal.write(&apos; &apos;) # 打印行首的两个空格\n        cal.write(&apos;  |&apos; * (i %7))   # 从星期几开始则空5*几个空格\n    for j in range(1, get_num_of_days_in_month(year, month)+1):\n        cal.write(&apos; [&apos; + str(j) + &apos;](#&apos; + str(month) + str(j) + &apos;) |&apos;)# 宽度控制，4+1=5\n        i += 1\n        if i % 7 == 0:  # i用于计数和换行\n            cal.write(&apos;\\n&apos;)   # 每换行一次行首继续空格\n\n\n\n#   主函数部分\n# year = int(raw_input(&quot;Please input target year:&quot;))\n# month = int(raw_input(&quot;Please input target month:&quot;))\nyear = 2018\nmonth = 12\ncal = open(str(year) + &apos;-&apos; + str(month) + &apos;-日历markdown版.txt&apos;,&apos;w&apos;)\nprint_month_title(year, month)\nprint_month_body(year, month)\ncal.close()\n</code></pre><hr>\n<h4 id=\"主要是修改了输出部分的代码，python实现日历主要是感谢原博主，在本地生成的markdown文件内容如下所示：\"><a href=\"#主要是修改了输出部分的代码，python实现日历主要是感谢原博主，在本地生成的markdown文件内容如下所示：\" class=\"headerlink\" title=\"主要是修改了输出部分的代码，python实现日历主要是感谢原博主，在本地生成的markdown文件内容如下所示：\"></a>主要是修改了输出部分的代码，python实现日历主要是感谢<a href=\"https://blog.csdn.net/jlshix/article/details/46970563\" target=\"_blank\" rel=\"noopener\">原博主</a>，在本地生成的markdown文件内容如下所示：</h4><p><img src=\"/2018/12/05/how-to-use-python-to-build-markdown-calc/1.png\" alt=\"image\"></p>\n<p>效果：</p>\n<p><img src=\"/2018/12/05/how-to-use-python-to-build-markdown-calc/2.JPG\" alt=\"image\"></p>\n<h3 id=\"其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\"><a href=\"#其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\" class=\"headerlink\" title=\"其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：\"></a>其中带有目录的链接，方便在文中快速定位，只需要按照如下方式定义页内链接即可：</h3><pre><code>&lt;span id=&quot;122&quot;&gt;12-2&lt;/span&gt;\n</code></pre><p>此处id即是日历中的#号后面的id</p>\n<h3 id=\"update-居中所使用的-lt-center-gt-貌似会使表格失效，因此去掉了该标签\"><a href=\"#update-居中所使用的-lt-center-gt-貌似会使表格失效，因此去掉了该标签\" class=\"headerlink\" title=\"update:居中所使用的&lt; center&gt;貌似会使表格失效，因此去掉了该标签\"></a>update:居中所使用的&lt; center&gt;貌似会使表格失效，因此去掉了该标签</h3>"},{"title":"TF入门——使用线性回归进行实验","date":"2018-12-10T13:39:33.000Z","description":"TF入门——使用线性回归进行训练","top":1,"_content":"\n### 思路\n\n首先，我们通过random构建一部分数据，这里随机了1000个数据构建一个一维数组作为输入，y_true随机取0或者1。然后使用sklearn的[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)函数进行划分，在本例子中使用0.1的比例进行划分。划分后的数据shape如下图所示：\n![image](/learning-tensorflow-linear-regression/1.png)\n然后我们定义了进行预测的公式（y = w * x + b)，使用[均方误差](https://baike.baidu.com/item/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE)作为损失函数，使用梯度下降对loss进行优化学习。然后定义epoch数，因为数据量比较小，所以这里也没有分batch进行学习。\n\n\n### 代码\n\n\t# encoding: utf-8\n\timport tensorflow as tf\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.model_selection import train_test_split\n\t\n\t# 一些定义\n\tnumber_of_data = 1000   # 数据总数\n\ttest_size = 0.1 # 测试集占比0.1\n\t\n\t# 准备数据，划分训练集和测试集\n\tX_ = np.random.rand(number_of_data)\n\ty_ = np.array([np.random.choice([0,1]) for _ in range(number_of_data)])\n\tX_train, X_test, y_train, y_test = train_test_split(X_, y_,test_size=test_size)\n\t\n\tX = tf.placeholder(tf.float32)\n\tY = tf.placeholder(tf.float32)\n\t\n\tW = tf.Variable(np.random.randn(), name=\"weight\")\n\tb = tf.Variable(np.random.rand(), name=\"bias\")\n\t\n\t# 1. 定义pred的函数\n\tpred = tf.add(tf.multiply(W, X), b)\n\t# 2. 定义loss\n\ttrain_size = X_train.shape[0]   # 即训练集数据的个数\n\tloss = tf.reduce_sum(tf.pow((pred - Y), 2)) / (2 * train_size)\n\t\n\t# 3. 定义优化器，这里使用梯度下降\n\tlearning_rate = 0.01    # 定义学习率\n\toptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\t\n\t# 初始化以上的内容\n\tinit = tf.global_variables_initializer()\n\t\n\t# 创建图\n\tepoch_num = 1000    # 定义epoch\n\twith tf.Session() as sess:\n\t    sess.run(init)\n\t    for epoch in range(epoch_num):\n\t        for (x, y) in zip(X_train, y_train):    # 这里没有使用mini batch学习\n\t            sess.run(optimizer, feed_dict={X: x, Y: y})\n\t\n\t        if epoch % 50 == 0:\n\t            dev_loss = sess.run(loss, feed_dict={X: X_train, Y: y_train})\n\t            print(\"Epoch: \", epoch, end='')\n\t            print(\" Loss: \", dev_loss, end='')\n\t            print(\" W: \", sess.run(W), \" b: \", sess.run(b))\n\n\n### 输出\n\n**运行时输出**\n![image](/learning-tensorflow-linear-regression/2.JPG)\n\n**loss变化**\n![image](/learning-tensorflow-linear-regression/3.png)\n由于这组数据没有什么实际的意义，因此没啥可解释的，可以看到loss很快下降并且很快就趋于稳定","source":"_posts/learning-tensorflow-linear-regression.md","raw":"---\ntitle: TF入门——使用线性回归进行实验\ndate: 2018-12-10 21:39:33\ntags:\n\t- Tensorflow\ndescription: TF入门——使用线性回归进行训练\ntop: 1\n---\n\n### 思路\n\n首先，我们通过random构建一部分数据，这里随机了1000个数据构建一个一维数组作为输入，y_true随机取0或者1。然后使用sklearn的[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)函数进行划分，在本例子中使用0.1的比例进行划分。划分后的数据shape如下图所示：\n![image](/learning-tensorflow-linear-regression/1.png)\n然后我们定义了进行预测的公式（y = w * x + b)，使用[均方误差](https://baike.baidu.com/item/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE)作为损失函数，使用梯度下降对loss进行优化学习。然后定义epoch数，因为数据量比较小，所以这里也没有分batch进行学习。\n\n\n### 代码\n\n\t# encoding: utf-8\n\timport tensorflow as tf\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\tfrom sklearn.model_selection import train_test_split\n\t\n\t# 一些定义\n\tnumber_of_data = 1000   # 数据总数\n\ttest_size = 0.1 # 测试集占比0.1\n\t\n\t# 准备数据，划分训练集和测试集\n\tX_ = np.random.rand(number_of_data)\n\ty_ = np.array([np.random.choice([0,1]) for _ in range(number_of_data)])\n\tX_train, X_test, y_train, y_test = train_test_split(X_, y_,test_size=test_size)\n\t\n\tX = tf.placeholder(tf.float32)\n\tY = tf.placeholder(tf.float32)\n\t\n\tW = tf.Variable(np.random.randn(), name=\"weight\")\n\tb = tf.Variable(np.random.rand(), name=\"bias\")\n\t\n\t# 1. 定义pred的函数\n\tpred = tf.add(tf.multiply(W, X), b)\n\t# 2. 定义loss\n\ttrain_size = X_train.shape[0]   # 即训练集数据的个数\n\tloss = tf.reduce_sum(tf.pow((pred - Y), 2)) / (2 * train_size)\n\t\n\t# 3. 定义优化器，这里使用梯度下降\n\tlearning_rate = 0.01    # 定义学习率\n\toptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\t\n\t# 初始化以上的内容\n\tinit = tf.global_variables_initializer()\n\t\n\t# 创建图\n\tepoch_num = 1000    # 定义epoch\n\twith tf.Session() as sess:\n\t    sess.run(init)\n\t    for epoch in range(epoch_num):\n\t        for (x, y) in zip(X_train, y_train):    # 这里没有使用mini batch学习\n\t            sess.run(optimizer, feed_dict={X: x, Y: y})\n\t\n\t        if epoch % 50 == 0:\n\t            dev_loss = sess.run(loss, feed_dict={X: X_train, Y: y_train})\n\t            print(\"Epoch: \", epoch, end='')\n\t            print(\" Loss: \", dev_loss, end='')\n\t            print(\" W: \", sess.run(W), \" b: \", sess.run(b))\n\n\n### 输出\n\n**运行时输出**\n![image](/learning-tensorflow-linear-regression/2.JPG)\n\n**loss变化**\n![image](/learning-tensorflow-linear-regression/3.png)\n由于这组数据没有什么实际的意义，因此没啥可解释的，可以看到loss很快下降并且很快就趋于稳定","slug":"learning-tensorflow-linear-regression","published":1,"updated":"2019-01-22T04:59:24.952Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2ml000bscftn84wewas","content":"<h3 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h3><p>首先，我们通过random构建一部分数据，这里随机了1000个数据构建一个一维数组作为输入，y_true随机取0或者1。然后使用sklearn的<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" target=\"_blank\" rel=\"noopener\">train_test_split</a>函数进行划分，在本例子中使用0.1的比例进行划分。划分后的数据shape如下图所示：<br><img src=\"/2018/12/10/learning-tensorflow-linear-regression/1.png\" alt=\"image\"><br>然后我们定义了进行预测的公式（y = w * x + b)，使用<a href=\"https://baike.baidu.com/item/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE\" target=\"_blank\" rel=\"noopener\">均方误差</a>作为损失函数，使用梯度下降对loss进行优化学习。然后定义epoch数，因为数据量比较小，所以这里也没有分batch进行学习。</p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code># encoding: utf-8\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# 一些定义\nnumber_of_data = 1000   # 数据总数\ntest_size = 0.1 # 测试集占比0.1\n\n# 准备数据，划分训练集和测试集\nX_ = np.random.rand(number_of_data)\ny_ = np.array([np.random.choice([0,1]) for _ in range(number_of_data)])\nX_train, X_test, y_train, y_test = train_test_split(X_, y_,test_size=test_size)\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(np.random.randn(), name=&quot;weight&quot;)\nb = tf.Variable(np.random.rand(), name=&quot;bias&quot;)\n\n# 1. 定义pred的函数\npred = tf.add(tf.multiply(W, X), b)\n# 2. 定义loss\ntrain_size = X_train.shape[0]   # 即训练集数据的个数\nloss = tf.reduce_sum(tf.pow((pred - Y), 2)) / (2 * train_size)\n\n# 3. 定义优化器，这里使用梯度下降\nlearning_rate = 0.01    # 定义学习率\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# 初始化以上的内容\ninit = tf.global_variables_initializer()\n\n# 创建图\nepoch_num = 1000    # 定义epoch\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(epoch_num):\n        for (x, y) in zip(X_train, y_train):    # 这里没有使用mini batch学习\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n\n        if epoch % 50 == 0:\n            dev_loss = sess.run(loss, feed_dict={X: X_train, Y: y_train})\n            print(&quot;Epoch: &quot;, epoch, end=&apos;&apos;)\n            print(&quot; Loss: &quot;, dev_loss, end=&apos;&apos;)\n            print(&quot; W: &quot;, sess.run(W), &quot; b: &quot;, sess.run(b))\n</code></pre><h3 id=\"输出\"><a href=\"#输出\" class=\"headerlink\" title=\"输出\"></a>输出</h3><p><strong>运行时输出</strong><br><img src=\"/2018/12/10/learning-tensorflow-linear-regression/2.JPG\" alt=\"image\"></p>\n<p><strong>loss变化</strong><br><img src=\"/2018/12/10/learning-tensorflow-linear-regression/3.png\" alt=\"image\"><br>由于这组数据没有什么实际的意义，因此没啥可解释的，可以看到loss很快下降并且很快就趋于稳定</p>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"思路\"><a href=\"#思路\" class=\"headerlink\" title=\"思路\"></a>思路</h3><p>首先，我们通过random构建一部分数据，这里随机了1000个数据构建一个一维数组作为输入，y_true随机取0或者1。然后使用sklearn的<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" target=\"_blank\" rel=\"noopener\">train_test_split</a>函数进行划分，在本例子中使用0.1的比例进行划分。划分后的数据shape如下图所示：<br><img src=\"/2018/12/10/learning-tensorflow-linear-regression/1.png\" alt=\"image\"><br>然后我们定义了进行预测的公式（y = w * x + b)，使用<a href=\"https://baike.baidu.com/item/%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE\" target=\"_blank\" rel=\"noopener\">均方误差</a>作为损失函数，使用梯度下降对loss进行优化学习。然后定义epoch数，因为数据量比较小，所以这里也没有分batch进行学习。</p>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code># encoding: utf-8\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# 一些定义\nnumber_of_data = 1000   # 数据总数\ntest_size = 0.1 # 测试集占比0.1\n\n# 准备数据，划分训练集和测试集\nX_ = np.random.rand(number_of_data)\ny_ = np.array([np.random.choice([0,1]) for _ in range(number_of_data)])\nX_train, X_test, y_train, y_test = train_test_split(X_, y_,test_size=test_size)\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n\nW = tf.Variable(np.random.randn(), name=&quot;weight&quot;)\nb = tf.Variable(np.random.rand(), name=&quot;bias&quot;)\n\n# 1. 定义pred的函数\npred = tf.add(tf.multiply(W, X), b)\n# 2. 定义loss\ntrain_size = X_train.shape[0]   # 即训练集数据的个数\nloss = tf.reduce_sum(tf.pow((pred - Y), 2)) / (2 * train_size)\n\n# 3. 定义优化器，这里使用梯度下降\nlearning_rate = 0.01    # 定义学习率\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# 初始化以上的内容\ninit = tf.global_variables_initializer()\n\n# 创建图\nepoch_num = 1000    # 定义epoch\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(epoch_num):\n        for (x, y) in zip(X_train, y_train):    # 这里没有使用mini batch学习\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n\n        if epoch % 50 == 0:\n            dev_loss = sess.run(loss, feed_dict={X: X_train, Y: y_train})\n            print(&quot;Epoch: &quot;, epoch, end=&apos;&apos;)\n            print(&quot; Loss: &quot;, dev_loss, end=&apos;&apos;)\n            print(&quot; W: &quot;, sess.run(W), &quot; b: &quot;, sess.run(b))\n</code></pre><h3 id=\"输出\"><a href=\"#输出\" class=\"headerlink\" title=\"输出\"></a>输出</h3><p><strong>运行时输出</strong><br><img src=\"/2018/12/10/learning-tensorflow-linear-regression/2.JPG\" alt=\"image\"></p>\n<p><strong>loss变化</strong><br><img src=\"/2018/12/10/learning-tensorflow-linear-regression/3.png\" alt=\"image\"><br>由于这组数据没有什么实际的意义，因此没啥可解释的，可以看到loss很快下降并且很快就趋于稳定</p>\n"},{"title":"python操作excel","date":"2018-10-13T16:00:00.000Z","description":"使用xlsxwriter写入excel","top":1,"_content":"\n\n## 用python写入excel\n\nNeeds:\n- python2.7\n- xlsxwriter\n\n\n```\n# 0. 一些变量定义\nxlsx_path = 'test.xlsx'\ndict_tobe_write = {\n    'a': 1,\n    'b': 2,\n    'c': 3\n}\n\n# 1. 创建excel文档\nworkbook = xlsxwriter.Workbook(xlsx_path)\n\n# 2. 创建工作表\nworksheet_1 = workbook.add_sheet('First sheet')\n\n# 3. 创建一些写入时候的格式，使用workbook.add_format()方法\ntitle_format = workbook.add_format()\ntitle_format.set_bold() # 设置粗体字\ntitle_format.set_font_size(10) # 设置字体大小为10\ntitle_format.set_font_name('Microsoft yahei') # 设置字体样式为雅黑\ntitle_format.set_align('center') # 设置水平居中对齐\ntitle_format.set_align('vcenter') # 设置垂直居中对齐\n\nworksheet.set_row(0, None, title_format) # 直接设置第一行为此属性\n\ncell_format = workbook.add_format()\ncell_format.set_font_size(10) # 设置字体大小为10\ncell_format.set_font_name('Microsoft yahei') # 设置字体样式为雅黑\ncell_format.set_align('center') # 设置水平居中对齐\ncell_format.set_align('vcenter') # 设置垂直居中对齐\n\n# 4. 写入excel\nline_index = 0\nfor key, value in dict_tobe_write.items():\n    worksheet_1.write(line_index, 0, key, cell_format)\n    worksheet_1.write(line_index, 1, value, cell_format)\n    line_index += 1\n    \n# 5. 关闭excel\nworkbook.close()\n```\n","source":"_posts/python操作excel.md","raw":"---\ntitle: python操作excel\ndate: 2018-10-14\ntags:\n    - python\ndescription: 使用xlsxwriter写入excel\ntop: 1\n---\n\n\n## 用python写入excel\n\nNeeds:\n- python2.7\n- xlsxwriter\n\n\n```\n# 0. 一些变量定义\nxlsx_path = 'test.xlsx'\ndict_tobe_write = {\n    'a': 1,\n    'b': 2,\n    'c': 3\n}\n\n# 1. 创建excel文档\nworkbook = xlsxwriter.Workbook(xlsx_path)\n\n# 2. 创建工作表\nworksheet_1 = workbook.add_sheet('First sheet')\n\n# 3. 创建一些写入时候的格式，使用workbook.add_format()方法\ntitle_format = workbook.add_format()\ntitle_format.set_bold() # 设置粗体字\ntitle_format.set_font_size(10) # 设置字体大小为10\ntitle_format.set_font_name('Microsoft yahei') # 设置字体样式为雅黑\ntitle_format.set_align('center') # 设置水平居中对齐\ntitle_format.set_align('vcenter') # 设置垂直居中对齐\n\nworksheet.set_row(0, None, title_format) # 直接设置第一行为此属性\n\ncell_format = workbook.add_format()\ncell_format.set_font_size(10) # 设置字体大小为10\ncell_format.set_font_name('Microsoft yahei') # 设置字体样式为雅黑\ncell_format.set_align('center') # 设置水平居中对齐\ncell_format.set_align('vcenter') # 设置垂直居中对齐\n\n# 4. 写入excel\nline_index = 0\nfor key, value in dict_tobe_write.items():\n    worksheet_1.write(line_index, 0, key, cell_format)\n    worksheet_1.write(line_index, 1, value, cell_format)\n    line_index += 1\n    \n# 5. 关闭excel\nworkbook.close()\n```\n","slug":"python操作excel","published":1,"updated":"2019-01-22T04:59:24.956Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2mn000dscftpr1nxn7i","content":"<h2 id=\"用python写入excel\"><a href=\"#用python写入excel\" class=\"headerlink\" title=\"用python写入excel\"></a>用python写入excel</h2><p>Needs:</p>\n<ul>\n<li>python2.7</li>\n<li>xlsxwriter</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 0. 一些变量定义</span><br><span class=\"line\">xlsx_path = &apos;test.xlsx&apos;</span><br><span class=\"line\">dict_tobe_write = &#123;</span><br><span class=\"line\">    &apos;a&apos;: 1,</span><br><span class=\"line\">    &apos;b&apos;: 2,</span><br><span class=\"line\">    &apos;c&apos;: 3</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># 1. 创建excel文档</span><br><span class=\"line\">workbook = xlsxwriter.Workbook(xlsx_path)</span><br><span class=\"line\"></span><br><span class=\"line\"># 2. 创建工作表</span><br><span class=\"line\">worksheet_1 = workbook.add_sheet(&apos;First sheet&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 3. 创建一些写入时候的格式，使用workbook.add_format()方法</span><br><span class=\"line\">title_format = workbook.add_format()</span><br><span class=\"line\">title_format.set_bold() # 设置粗体字</span><br><span class=\"line\">title_format.set_font_size(10) # 设置字体大小为10</span><br><span class=\"line\">title_format.set_font_name(&apos;Microsoft yahei&apos;) # 设置字体样式为雅黑</span><br><span class=\"line\">title_format.set_align(&apos;center&apos;) # 设置水平居中对齐</span><br><span class=\"line\">title_format.set_align(&apos;vcenter&apos;) # 设置垂直居中对齐</span><br><span class=\"line\"></span><br><span class=\"line\">worksheet.set_row(0, None, title_format) # 直接设置第一行为此属性</span><br><span class=\"line\"></span><br><span class=\"line\">cell_format = workbook.add_format()</span><br><span class=\"line\">cell_format.set_font_size(10) # 设置字体大小为10</span><br><span class=\"line\">cell_format.set_font_name(&apos;Microsoft yahei&apos;) # 设置字体样式为雅黑</span><br><span class=\"line\">cell_format.set_align(&apos;center&apos;) # 设置水平居中对齐</span><br><span class=\"line\">cell_format.set_align(&apos;vcenter&apos;) # 设置垂直居中对齐</span><br><span class=\"line\"></span><br><span class=\"line\"># 4. 写入excel</span><br><span class=\"line\">line_index = 0</span><br><span class=\"line\">for key, value in dict_tobe_write.items():</span><br><span class=\"line\">    worksheet_1.write(line_index, 0, key, cell_format)</span><br><span class=\"line\">    worksheet_1.write(line_index, 1, value, cell_format)</span><br><span class=\"line\">    line_index += 1</span><br><span class=\"line\">    </span><br><span class=\"line\"># 5. 关闭excel</span><br><span class=\"line\">workbook.close()</span><br></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"用python写入excel\"><a href=\"#用python写入excel\" class=\"headerlink\" title=\"用python写入excel\"></a>用python写入excel</h2><p>Needs:</p>\n<ul>\n<li>python2.7</li>\n<li>xlsxwriter</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 0. 一些变量定义</span><br><span class=\"line\">xlsx_path = &apos;test.xlsx&apos;</span><br><span class=\"line\">dict_tobe_write = &#123;</span><br><span class=\"line\">    &apos;a&apos;: 1,</span><br><span class=\"line\">    &apos;b&apos;: 2,</span><br><span class=\"line\">    &apos;c&apos;: 3</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"># 1. 创建excel文档</span><br><span class=\"line\">workbook = xlsxwriter.Workbook(xlsx_path)</span><br><span class=\"line\"></span><br><span class=\"line\"># 2. 创建工作表</span><br><span class=\"line\">worksheet_1 = workbook.add_sheet(&apos;First sheet&apos;)</span><br><span class=\"line\"></span><br><span class=\"line\"># 3. 创建一些写入时候的格式，使用workbook.add_format()方法</span><br><span class=\"line\">title_format = workbook.add_format()</span><br><span class=\"line\">title_format.set_bold() # 设置粗体字</span><br><span class=\"line\">title_format.set_font_size(10) # 设置字体大小为10</span><br><span class=\"line\">title_format.set_font_name(&apos;Microsoft yahei&apos;) # 设置字体样式为雅黑</span><br><span class=\"line\">title_format.set_align(&apos;center&apos;) # 设置水平居中对齐</span><br><span class=\"line\">title_format.set_align(&apos;vcenter&apos;) # 设置垂直居中对齐</span><br><span class=\"line\"></span><br><span class=\"line\">worksheet.set_row(0, None, title_format) # 直接设置第一行为此属性</span><br><span class=\"line\"></span><br><span class=\"line\">cell_format = workbook.add_format()</span><br><span class=\"line\">cell_format.set_font_size(10) # 设置字体大小为10</span><br><span class=\"line\">cell_format.set_font_name(&apos;Microsoft yahei&apos;) # 设置字体样式为雅黑</span><br><span class=\"line\">cell_format.set_align(&apos;center&apos;) # 设置水平居中对齐</span><br><span class=\"line\">cell_format.set_align(&apos;vcenter&apos;) # 设置垂直居中对齐</span><br><span class=\"line\"></span><br><span class=\"line\"># 4. 写入excel</span><br><span class=\"line\">line_index = 0</span><br><span class=\"line\">for key, value in dict_tobe_write.items():</span><br><span class=\"line\">    worksheet_1.write(line_index, 0, key, cell_format)</span><br><span class=\"line\">    worksheet_1.write(line_index, 1, value, cell_format)</span><br><span class=\"line\">    line_index += 1</span><br><span class=\"line\">    </span><br><span class=\"line\"># 5. 关闭excel</span><br><span class=\"line\">workbook.close()</span><br></pre></td></tr></table></figure>\n"},{"title":"spacy-leaning","date":"2019-01-22T04:23:30.000Z","_content":"","source":"_posts/spacy-leaning.md","raw":"---\ntitle: spacy-leaning\ndate: 2019-01-22 12:23:30\ntags:\n---\n","slug":"spacy-leaning","published":1,"updated":"2019-01-22T04:59:24.956Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2mo000escftjxqzvmpj","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"使用python遍历二叉树","date":"2018-12-05T16:00:00.000Z","top":1,"description":"递归遍历二叉树-python实验","_content":"\n#### 参考：[python实现二叉树的遍历-csdn](https://www.cnblogs.com/freeman818/p/7252041.html)\n\n### 数据定义\n\n\tclass Node(object):\n\t\tdef __init__(self, data, l_child=None, r_child=None):\n\t\t\tself.data = data\n\t\t\tself.left_child = l_child\n\t\t\tself.right_child = r_child\n\n### 递归遍历（前序、中序、后序）\n\n\tdef pre_(root):\n\t\tif root == None:\n\t\t\treturn\n\t\tprint(root.data)\n\t\tpre_(root.left_child)\n\t\tpre_(root.right_child)\n\t\n\tdef mid_(root):\n\t\tif root == None:\n\t\t\treturn\n\t\tmid_(root.left_child)\n\t\tprint(root.data)\n\t\tmid_(root.right_child)\n\t\n\tdef after_(root):\n\t\tif root == None:\n\t\t\treturn\n\t\tafter_(root.left_child)\n\t\tafter_(root.right_child)\n\t\tprint(root.data)\n\n### 单元测试\n\n\tif __name__ == '__main__':\n\t\ttest_tree = Node('A', \n\t\t\t\t\t\tNode('B', \n\t\t\t\t\t\t\tNode('D'),\n\t\t\t\t\t\t\tNone), \n\t\t\t\t\t\tNode('C', \n\t\t\t\t\t\t\tNode('E', \n\t\t\t\t\t\t\t\tNone, \n\t\t\t\t\t\t\t\tNode('G')), \n\t\t\t\t\t\t\tNode('F', \n\t\t\t\t\t\t\t\tNode('H'), \n\t\t\t\t\t\t\t\tNode('I'))))\n\t\n\t\tpre_(test_tree)\n\t\tprint('------------------')\n\t\tmid_(test_tree)\n\t\tprint('------------------')\n\t\tafter_(test_tree)\n\n**其中树的结构为：**\n![image](/tree/1.jpg)\n\n### 正确的遍历顺序为：\n- 前序：A B D C E G F H I \n- 中序：D B A E G C H F I \n- 后序：D B G E H I F C A","source":"_posts/tree.md","raw":"---\ntitle: 使用python遍历二叉树\ndate: 2018-12-06\ntop: 1\ntags:\n\t- python\n\t- Algorithm\ndescription: 递归遍历二叉树-python实验\n---\n\n#### 参考：[python实现二叉树的遍历-csdn](https://www.cnblogs.com/freeman818/p/7252041.html)\n\n### 数据定义\n\n\tclass Node(object):\n\t\tdef __init__(self, data, l_child=None, r_child=None):\n\t\t\tself.data = data\n\t\t\tself.left_child = l_child\n\t\t\tself.right_child = r_child\n\n### 递归遍历（前序、中序、后序）\n\n\tdef pre_(root):\n\t\tif root == None:\n\t\t\treturn\n\t\tprint(root.data)\n\t\tpre_(root.left_child)\n\t\tpre_(root.right_child)\n\t\n\tdef mid_(root):\n\t\tif root == None:\n\t\t\treturn\n\t\tmid_(root.left_child)\n\t\tprint(root.data)\n\t\tmid_(root.right_child)\n\t\n\tdef after_(root):\n\t\tif root == None:\n\t\t\treturn\n\t\tafter_(root.left_child)\n\t\tafter_(root.right_child)\n\t\tprint(root.data)\n\n### 单元测试\n\n\tif __name__ == '__main__':\n\t\ttest_tree = Node('A', \n\t\t\t\t\t\tNode('B', \n\t\t\t\t\t\t\tNode('D'),\n\t\t\t\t\t\t\tNone), \n\t\t\t\t\t\tNode('C', \n\t\t\t\t\t\t\tNode('E', \n\t\t\t\t\t\t\t\tNone, \n\t\t\t\t\t\t\t\tNode('G')), \n\t\t\t\t\t\t\tNode('F', \n\t\t\t\t\t\t\t\tNode('H'), \n\t\t\t\t\t\t\t\tNode('I'))))\n\t\n\t\tpre_(test_tree)\n\t\tprint('------------------')\n\t\tmid_(test_tree)\n\t\tprint('------------------')\n\t\tafter_(test_tree)\n\n**其中树的结构为：**\n![image](/tree/1.jpg)\n\n### 正确的遍历顺序为：\n- 前序：A B D C E G F H I \n- 中序：D B A E G C H F I \n- 后序：D B G E H I F C A","slug":"tree","published":1,"updated":"2019-01-22T04:59:24.956Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2mq000gscft86om5msh","content":"<h4 id=\"参考：python实现二叉树的遍历-csdn\"><a href=\"#参考：python实现二叉树的遍历-csdn\" class=\"headerlink\" title=\"参考：python实现二叉树的遍历-csdn\"></a>参考：<a href=\"https://www.cnblogs.com/freeman818/p/7252041.html\" target=\"_blank\" rel=\"noopener\">python实现二叉树的遍历-csdn</a></h4><h3 id=\"数据定义\"><a href=\"#数据定义\" class=\"headerlink\" title=\"数据定义\"></a>数据定义</h3><pre><code>class Node(object):\n    def __init__(self, data, l_child=None, r_child=None):\n        self.data = data\n        self.left_child = l_child\n        self.right_child = r_child\n</code></pre><h3 id=\"递归遍历（前序、中序、后序）\"><a href=\"#递归遍历（前序、中序、后序）\" class=\"headerlink\" title=\"递归遍历（前序、中序、后序）\"></a>递归遍历（前序、中序、后序）</h3><pre><code>def pre_(root):\n    if root == None:\n        return\n    print(root.data)\n    pre_(root.left_child)\n    pre_(root.right_child)\n\ndef mid_(root):\n    if root == None:\n        return\n    mid_(root.left_child)\n    print(root.data)\n    mid_(root.right_child)\n\ndef after_(root):\n    if root == None:\n        return\n    after_(root.left_child)\n    after_(root.right_child)\n    print(root.data)\n</code></pre><h3 id=\"单元测试\"><a href=\"#单元测试\" class=\"headerlink\" title=\"单元测试\"></a>单元测试</h3><pre><code>if __name__ == &apos;__main__&apos;:\n    test_tree = Node(&apos;A&apos;, \n                    Node(&apos;B&apos;, \n                        Node(&apos;D&apos;),\n                        None), \n                    Node(&apos;C&apos;, \n                        Node(&apos;E&apos;, \n                            None, \n                            Node(&apos;G&apos;)), \n                        Node(&apos;F&apos;, \n                            Node(&apos;H&apos;), \n                            Node(&apos;I&apos;))))\n\n    pre_(test_tree)\n    print(&apos;------------------&apos;)\n    mid_(test_tree)\n    print(&apos;------------------&apos;)\n    after_(test_tree)\n</code></pre><p><strong>其中树的结构为：</strong><br><img src=\"/2018/12/06/tree/1.jpg\" alt=\"image\"></p>\n<h3 id=\"正确的遍历顺序为：\"><a href=\"#正确的遍历顺序为：\" class=\"headerlink\" title=\"正确的遍历顺序为：\"></a>正确的遍历顺序为：</h3><ul>\n<li>前序：A B D C E G F H I </li>\n<li>中序：D B A E G C H F I </li>\n<li>后序：D B G E H I F C A</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"参考：python实现二叉树的遍历-csdn\"><a href=\"#参考：python实现二叉树的遍历-csdn\" class=\"headerlink\" title=\"参考：python实现二叉树的遍历-csdn\"></a>参考：<a href=\"https://www.cnblogs.com/freeman818/p/7252041.html\" target=\"_blank\" rel=\"noopener\">python实现二叉树的遍历-csdn</a></h4><h3 id=\"数据定义\"><a href=\"#数据定义\" class=\"headerlink\" title=\"数据定义\"></a>数据定义</h3><pre><code>class Node(object):\n    def __init__(self, data, l_child=None, r_child=None):\n        self.data = data\n        self.left_child = l_child\n        self.right_child = r_child\n</code></pre><h3 id=\"递归遍历（前序、中序、后序）\"><a href=\"#递归遍历（前序、中序、后序）\" class=\"headerlink\" title=\"递归遍历（前序、中序、后序）\"></a>递归遍历（前序、中序、后序）</h3><pre><code>def pre_(root):\n    if root == None:\n        return\n    print(root.data)\n    pre_(root.left_child)\n    pre_(root.right_child)\n\ndef mid_(root):\n    if root == None:\n        return\n    mid_(root.left_child)\n    print(root.data)\n    mid_(root.right_child)\n\ndef after_(root):\n    if root == None:\n        return\n    after_(root.left_child)\n    after_(root.right_child)\n    print(root.data)\n</code></pre><h3 id=\"单元测试\"><a href=\"#单元测试\" class=\"headerlink\" title=\"单元测试\"></a>单元测试</h3><pre><code>if __name__ == &apos;__main__&apos;:\n    test_tree = Node(&apos;A&apos;, \n                    Node(&apos;B&apos;, \n                        Node(&apos;D&apos;),\n                        None), \n                    Node(&apos;C&apos;, \n                        Node(&apos;E&apos;, \n                            None, \n                            Node(&apos;G&apos;)), \n                        Node(&apos;F&apos;, \n                            Node(&apos;H&apos;), \n                            Node(&apos;I&apos;))))\n\n    pre_(test_tree)\n    print(&apos;------------------&apos;)\n    mid_(test_tree)\n    print(&apos;------------------&apos;)\n    after_(test_tree)\n</code></pre><p><strong>其中树的结构为：</strong><br><img src=\"/2018/12/06/tree/1.jpg\" alt=\"image\"></p>\n<h3 id=\"正确的遍历顺序为：\"><a href=\"#正确的遍历顺序为：\" class=\"headerlink\" title=\"正确的遍历顺序为：\"></a>正确的遍历顺序为：</h3><ul>\n<li>前序：A B D C E G F H I </li>\n<li>中序：D B A E G C H F I </li>\n<li>后序：D B G E H I F C A</li>\n</ul>\n"},{"title":"(置顶)年度计划","date":"2019-01-02T08:08:14.000Z","top":100,"description":"年初制定，规划一年的计划。","_content":"\n- 18年日报见：[2018日记&日报](https://junzx.github.io/2018/11/29/2018-work-schedule/)\n- 19年日报见：[2019日记&日报](https://junzx.github.io/2019/01/02/2019-work-schedule/)\n- 使用python生成markdown格式的日历：[使用python生成日历](https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/)\n\n---\n\n### 年度待完成的目标\n\n- 应掌握的技能\n  \n  - [ ] 英语\n  - [ ] 手写SQL\n  - [ ] python高级编程——熟练掌握OO、闭包、装饰器、函数式编程、进程与线程\n  - [ ] python爬虫框架——scrapy、对ajax网页爬取\n  - [ ] python中web框架——Django、Flask\n  - [ ] 机器学习知识——理论、实战\n  - [ ] Tensorflow/深度学习\n  - [ ] Letax语法\n  - [ ] Excel中各种操作——VBS等\n  - [ ] PPT制作\n  - [ ] 金融的知识\n  - [ ] Docker入门\n\n\n- 应完成的项目\n  - [ ] 利用wxpy等开源工具实现聊天机器人\n  - [ ] 租借服务器完善微信公众号\n  - [ ] 实现对自己的定制的消息获取工具\n\n- 高阶任务\n  - [ ] 摄像头体态检测项目\n  - [ ] Android端护理程序开发\n  \n\n\n### 参考\n- http://blog.knownsec.com/Knownsec_RD_Checklist/index.html\n\n","source":"_posts/year-plan.md","raw":"---\ntitle: (置顶)年度计划\ndate: 2019-01-02 16:08:14\ntags:\n    - 随笔\ntop: 100\ndescription: 年初制定，规划一年的计划。\n---\n\n- 18年日报见：[2018日记&日报](https://junzx.github.io/2018/11/29/2018-work-schedule/)\n- 19年日报见：[2019日记&日报](https://junzx.github.io/2019/01/02/2019-work-schedule/)\n- 使用python生成markdown格式的日历：[使用python生成日历](https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/)\n\n---\n\n### 年度待完成的目标\n\n- 应掌握的技能\n  \n  - [ ] 英语\n  - [ ] 手写SQL\n  - [ ] python高级编程——熟练掌握OO、闭包、装饰器、函数式编程、进程与线程\n  - [ ] python爬虫框架——scrapy、对ajax网页爬取\n  - [ ] python中web框架——Django、Flask\n  - [ ] 机器学习知识——理论、实战\n  - [ ] Tensorflow/深度学习\n  - [ ] Letax语法\n  - [ ] Excel中各种操作——VBS等\n  - [ ] PPT制作\n  - [ ] 金融的知识\n  - [ ] Docker入门\n\n\n- 应完成的项目\n  - [ ] 利用wxpy等开源工具实现聊天机器人\n  - [ ] 租借服务器完善微信公众号\n  - [ ] 实现对自己的定制的消息获取工具\n\n- 高阶任务\n  - [ ] 摄像头体态检测项目\n  - [ ] Android端护理程序开发\n  \n\n\n### 参考\n- http://blog.knownsec.com/Knownsec_RD_Checklist/index.html\n\n","slug":"year-plan","published":1,"updated":"2019-01-22T04:59:24.956Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2mr000iscft401prcul","content":"<ul>\n<li>18年日报见：<a href=\"https://junzx.github.io/2018/11/29/2018-work-schedule/\" target=\"_blank\" rel=\"noopener\">2018日记&amp;日报</a></li>\n<li>19年日报见：<a href=\"https://junzx.github.io/2019/01/02/2019-work-schedule/\" target=\"_blank\" rel=\"noopener\">2019日记&amp;日报</a></li>\n<li>使用python生成markdown格式的日历：<a href=\"https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/\" target=\"_blank\" rel=\"noopener\">使用python生成日历</a></li>\n</ul>\n<hr>\n<h3 id=\"年度待完成的目标\"><a href=\"#年度待完成的目标\" class=\"headerlink\" title=\"年度待完成的目标\"></a>年度待完成的目标</h3><ul>\n<li><p>应掌握的技能</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 英语</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 手写SQL</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> python高级编程——熟练掌握OO、闭包、装饰器、函数式编程、进程与线程</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> python爬虫框架——scrapy、对ajax网页爬取</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> python中web框架——Django、Flask</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 机器学习知识——理论、实战</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Tensorflow/深度学习</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Letax语法</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Excel中各种操作——VBS等</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> PPT制作</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 金融的知识</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Docker入门</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>应完成的项目</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 利用wxpy等开源工具实现聊天机器人</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 租借服务器完善微信公众号</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 实现对自己的定制的消息获取工具</li>\n</ul>\n</li>\n<li><p>高阶任务</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 摄像头体态检测项目</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Android端护理程序开发</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><ul>\n<li><a href=\"http://blog.knownsec.com/Knownsec_RD_Checklist/index.html\" target=\"_blank\" rel=\"noopener\">http://blog.knownsec.com/Knownsec_RD_Checklist/index.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<ul>\n<li>18年日报见：<a href=\"https://junzx.github.io/2018/11/29/2018-work-schedule/\" target=\"_blank\" rel=\"noopener\">2018日记&amp;日报</a></li>\n<li>19年日报见：<a href=\"https://junzx.github.io/2019/01/02/2019-work-schedule/\" target=\"_blank\" rel=\"noopener\">2019日记&amp;日报</a></li>\n<li>使用python生成markdown格式的日历：<a href=\"https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/\" target=\"_blank\" rel=\"noopener\">使用python生成日历</a></li>\n</ul>\n<hr>\n<h3 id=\"年度待完成的目标\"><a href=\"#年度待完成的目标\" class=\"headerlink\" title=\"年度待完成的目标\"></a>年度待完成的目标</h3><ul>\n<li><p>应掌握的技能</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 英语</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 手写SQL</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> python高级编程——熟练掌握OO、闭包、装饰器、函数式编程、进程与线程</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> python爬虫框架——scrapy、对ajax网页爬取</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> python中web框架——Django、Flask</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 机器学习知识——理论、实战</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Tensorflow/深度学习</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Letax语法</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Excel中各种操作——VBS等</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> PPT制作</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 金融的知识</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Docker入门</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li><p>应完成的项目</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 利用wxpy等开源工具实现聊天机器人</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 租借服务器完善微信公众号</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 实现对自己的定制的消息获取工具</li>\n</ul>\n</li>\n<li><p>高阶任务</p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 摄像头体态检测项目</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> Android端护理程序开发</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><ul>\n<li><a href=\"http://blog.knownsec.com/Knownsec_RD_Checklist/index.html\" target=\"_blank\" rel=\"noopener\">http://blog.knownsec.com/Knownsec_RD_Checklist/index.html</a></li>\n</ul>\n"},{"title":"2018日记&日报","date":"2018-11-28T16:00:00.000Z","description":"用来记录每天的工作-2018","top":50,"_content":"### 使用python生成markdown格式的日历参考：[使用python生成日历](https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/)\n\n\n### November   2018          \n|Sun | Mon | Tue  | Wed | Thu | Fri | Sat \n|:-| :- | :- | :- | :- | :- | :- |\n|  |  |  |  | 1 | 2 | 3 |\n| 4| 5 | 6 | 7 | 8 | 9 | 10 |\n| 11 | 12 | 13 | 14 | 15 | 16 | 17 |\n| 19 | 20 | 21 | 22 | 23 | 24 | 25 |\n| 25 | 26 | 27 | 28 | [29](#1129) | [30](#1130) |\n\n\n\n---\n**<span id=\"1129\">11-29</span>**\n\n- 动手刷leetcode的习题\n- 总结需要学习的计能\n\t- python高级编程\n\t- 爬虫框架\n\t- Mongodb操作\n\t- 基础数据结构\n\t- 基础算法\n\t- ML\n\n---\n**<span id=\"1130\">11-30</span>**\n\n- 处理好快递的相关事宜\n- 学习树相关内容，笔记如下：\n\t- **二叉树**：每个节点最多含有两个子树的树称为二叉树；\n\t- **完全二叉树**：对于一颗二叉树，假设其深度为d（d>1）。除了第d层外，其它各层的节点数目均已达最大值，且第d层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树；\n\t- **满二叉树**：所有叶节点都在最底层的完全二叉树；\n\t- **平衡二叉树（AVL树）**：当且仅当任何节点的两棵子树的高度差不大于1的二叉树；\n\t- **红黑树**: 是一种自平衡二叉查找树，它是在1972年由鲁道夫·贝尔发明的，他称之为”对称二叉B树”\n\t- **排序二叉树(二叉查找树)**：也称二叉搜索树、有序二叉树；\n\t- **霍夫曼树**：带权路径最短的二叉树称为哈夫曼树或最优二叉树；\n\t- **B树**：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个子树。\n- 参考资料：\n\t- [浅谈数据结构-二叉树](https://www.cnblogs.com/polly333/p/4740355.html)\n\n---\n\n\n### **December   2018**          \nSun | Mon | Tue  | Wed | Thu | Fri | Sat \n---| ---| ---| ---| ---| ---| ---|\n  |  |  |  |  |  | 1 |\n [2](#122) | 3 | [4](#124) | [5](#125) | [6](#126) | 7 | 8 |\n [9](#129) | [10](#1210) | [11](#1211) | [12](#1212) | [13](#1213) | 14| 15 |\n 16 | [17](#1217) | [18](#1218) | [19](#1219) | [20](#1220) | [21](#1221) | 22 |\n 23 | 24 | 25 | [26](#1226) | [27](#1227) | [28](#1228) | [29](#1229) |\n [30](#1230) | [31](#1231) |\n\n\n---\n**<span id=\"122\">12-2</span>**\n\n**下周计划**\n\n- 准备面试相关内容\n\t- 项目经历\n\t- 实习经历\n\t- 常见问题\n- CS224n的课，完成前五节（至少）\n- 完成基础数据结构的复习\n- Leetcode简单的基础数据结构的题目\n- **！**项目归纳整理\n- 形成毕设的提纲\n- （不重要）整理博客的内容\n\n\n备忘\n\n- [二叉树的一些性质](https://www.cnblogs.com/willwu/p/6007555.html)\n- [二叉树的遍历方式](https://blog.csdn.net/u014465639/article/details/71076092)\n- [python实现二叉树](https://www.cnblogs.com/PrettyTom/p/6677993.html)\n- [也是一些遍历方式](https://www.jianshu.com/p/456af5480cee)\n\n\n\n总结：\n\n1. 遍历方式还需要再深入学习，掌握的不够牢固。\n2. 过了深度学习入门第五章内容，先继续进行接下来的章节内容。\n\n---\n**<span id=\"124\">12-4</span>**\n\n- 看coursera上数据结构关于二叉树部分的课程\n\t- 递归的遍历方法区别在于何时访问Root节点\n\t- 非递归的遍历方法稍微困难一点\n\t\n- 明日计划\n\t1. 熟练掌握各种遍历方法的python实现\n\t2. 《入门》下一章内容\n\t3. 224第二节\n\n---\n**<span id=\"125\">12-5</span>**\n\n- 《入门》第六章内容\n\t- 参数更新：\n\t\t- SGD\n\t\t- Momentum\n\t\t- AdaGrad\n\t\t- Adam\n\t- 权重初始化\n\t\t- 如果是线性激活函数，使用 1/\\sqrt{n};\n\t\t- 如果是ReLU，则使用\\sqrt{2/n}\n\t- Batch Normalization\n\t- 正则化\n- 刷了224第二节课，认为看视频的效率有点低，计划这系列课程只看7和15节，其余知识通过读书获取\n- **明日计划**\n\t- 两个小时构思毕设，做相应的准备，制定计划\n\t- 《入门》下一章内容\n\t- 关于树部分，复杂度相关知识的归纳整理\n\t- Leetcode关于树的习题\n\n---\n**<span id=\"126\">12-6</span>**\n\n- 复习cnn（[textCNN学习笔记](https://junzx.github.io/2018/12/06/cnn/)）\n- 二叉树代价分析\n\t- 深度搜索\n\t\t- 时间复杂度：O(n)，每个节点访问一次\n\t\t- 空间复杂度：最好O(logn)，最坏O(n)，与栈的深度即树的高度有关\n\t- 层次搜索\n\t\t- 时间复杂度：O(n)\n\t\t- 空间复杂度：最好O(1)，最坏O(n)，与树的最大宽度有关\n\n---\n**<span id=\"129\">12-9</span>**\n\n- 祝亲人生日快乐\n- 下周计划\n\t- 完成未完成的内容\n\t- 《TF实战》\n\n\n---\n**<span id=\"1210\">12-10</span>**\n\n- 入门tf： [TF入门——使用线性回归进行实验](https://junzx.github.io/2018/12/10/learning-tensorflow-linear-regression/)\n- 准备面试相关的内容\n- 确定近期预算\n\n---\n**<span id=\"1211\">12-11</span>**\n\n- 去便利蜂面试，面试官人很nice，但是很遗憾结果还是不理想，发现了一些问题如下\n\t1. 基础算法不够牢固，一些排序复杂度的弄不是很清楚\n\t2. 项目中算法类的不够，工程类的较多\n\t3. 现场在纸上手写代码的效率不行，思路很受干扰，说明思路不够清晰，代码还是不够熟练\n- 制定突破计划\n\n---\n**<span id=\"1212\">12-12</span>**\n\n- 着手撸排序和查找的基础算法\n- [新任务](https://github.com/huggingface/neuralcoref)\n\n---\n**<span id=\"1213\">12-13</span>**\n\n- spacy相关\n\t- https://juejin.im/post/5971a4b9f265da6c42353332\n\t- http://www.52nlp.cn/tag/spacy\n\t- 遇到一些问题，后来解决了，必须使用2.0.12版本的spacy\n\t- 示例代码：\n\t\t\tIn [1]: import spacy\n\t\t\t\n\t\t\tIn [2]: spacy.__version__\n\t\t\tOut[2]: '2.0.12'\n\t\t\t\n\t\t\tIn [3]: nlp = spacy.load('en_coref_sm')\n\t\t\t\n\t\t\tIn [4]: doc = nlp(u'My sister has a dog. She loves him')\n\t\t\t\n\t\t\tIn [5]: doc._.has_coref\n\t\t\tOut[5]: True\n\t\t\t\n\t\t\tIn [6]: doc._.coref_clusters\n\t\t\tOut[6]: [My sister: [My sister, She], a dog: [a dog, him]]\n\n- 搞定三个插入排序算法（[插入排序](https://junzx.github.io/2018/12/13/algorithm-learning-insert-sort/)）\n\n---\n**<span id=\"1217\">12-17</span>**\n\n- 上周总结\n\t- 面试bianlifeng，发现了自己的不足\n\t- 完成了三种插入排序的算法\n\t- 动手学习tf\n\t- 接到了新任务：[新任务](https://github.com/huggingface/neuralcoref)\n\n- 本周安排\n\t- 剩余的排序查找算法\n\t- **毕设提纲（必须在周三之前出来）**\n\t- 刷题\n\t- 保持每天扇贝的单词\n\n- 时间安排\n\t- 7：30 闹钟\n\t- 7：45 起床\n\t- 8：00 吃早饭\n\t- 9：30 之前 扇贝当日内容\n\t- 9：30~11：00 上午工作\n\t- 13：00 下午工作开始\n\t- 17：30 晚饭\n\t- 19：00 晚上工作\n\t- 23：00 上床睡觉\n\n---\n**<span id=\"1218\">12-18</span>**\n\n**今日成果**\n\n- 完成扇贝单词\n- 列出了毕设提纲\n- 完成选择排序：[算法入门——选择排序](https://junzx.github.io/2018/12/18/algorithm-learning-select-sort/)\n\n**明日计划**\n\n- 扇贝单词\n- 在屋内适当活动\n- 与相机和背包的客服沟通\n- 完成选择排序的笔记\n- 完成交换排序\n\n---\n**<span id=\"1219\">12-19</span>**\n\n**今日成果**\n\n- 扇贝单词\n- 跟客服扯皮 \n- 完善选择排序\n\n**明日计划**\n\n- 扇贝单词\n- 剩余的所有排序（快排，归并）\n- 毕设内容\n- 准备面试\n\n---\n**<span id=\"1220\">12-20</span>**\n\n**积累**\n\n1. The choice of your career path is **critical** to your future. 职业道路的选择对你的未来至关重要。\n2. The government is responsible for the imporvement of the transportation and public **utilities**. （政府负责改善交通与公共措施） \n3. The average income is high, though many people earn just a **fraction** of that average.（平均收入很高，但是很多人赚的只有平均工资的一小部分）\n4. world **renowned**（举世闻名）\n5. A **substantial** number of people supported the reform policy.（相当多的人支持这一改革政策）\n6. Increased competition makes it **essential** for the business to innovate.（日益激烈的竞争使得企业必须创新）\n\n**今日成果**\n- 扇贝单词\n- 完成交换排序相关[算法入门——交换排序](https://junzx.github.io/2018/12/20/algorithm-learning-swap-sort/)\n- 完成归并排序相关[算法入门——归并排序](https://junzx.github.io/2018/12/20/algorithm-learning-merge-sort/)\n\n**明日计划**\n- 书上关于查找的部分\n- 毕设内容\n- 整理简历&看面经&问经验\n\n---\n**<span id=\"1221\">12-21</span>**\n**积累**\n1. an area of **outstanding** natural beauty.(自然风景极美的地区/优秀的、突出的)\n\n**今日成果**\n- 扇贝单词\n- 排序总结\n- 过以前的论文，找到对毕设可能有用的\n\n![image](/2018-work-schedule/1.JPG)\n\n| 算法 | 稳定性 | 时间复杂度 | 空间复杂度 | \n| ---| ---| ---| ---| ---|\n| 直接插入排序 | 稳定 | O(n^2) | O(1) |\n| 折半插入排序 | 稳定 | O(n^2) | O(1) |\n| 希尔排序 | 不稳定 | O(n^2) | O(1) |\n| 冒泡排序 | 稳定 | O(n^2) | O(1) | \n| 快速排序 | 不稳定 | O(n^2) | 最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n) |\n| 简单选择排序 | 不稳定 | O(n^2) | O(1) |\n| 堆排序 | 不稳定 | O(nlog2n) | O(1) |\n| 归并排序 | 稳定 | O(nlog2n) | O(n) |\n| 基数排序 | 稳定 | O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r) | O(r) r个队列 |\n\n---\n**<span id=\"1226\">12-26</span>**\n\n**本周计划**\n- 基础算法\n- 整理GitHub代码以及收藏的内容\n- TF\n- Numpy——参考：[NumPy 中文文档](https://www.numpy.org.cn/)\n- 拓展阅读——[自动对话](http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf)\n- BERT相关内容——[BERT相关论文、文章和代码资源汇总-知乎](https://zhuanlan.zhihu.com/p/50717786)\n- [端到端的机器学习项目](https://github.com/DeqianBai/Your-first-machine-learning-Project---End-to-End-in-Python)\n- 单词\n\n---\n**<span id=\"1227\">12-27</span>**\n\n**今日安排**\n- [x] Numpy入门\n- [ ]单词\n- [x]整理GitHub stars\n\n**积累**\n- [创建一个数组](https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%95%B0%E7%BB%84)\n- [多维数组切片](https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E5%88%87%E7%89%87)\n\n\n\tdef quicksort(arr):\n\t    if len(arr) <= 1:\n\t        return arr\n\t    pivot = arr[len(arr) // 2]\n\t    left = [x for x in arr if x < pivot]\n\t    middle = [x for x in arr if x == pivot]\n\t    right = [x for x in arr if x > pivot]\n\t    return quicksort(left) + middle + quicksort(right)\n\t\n\t\tprint quicksort([3,6,8,10,1,2,1])\n\n- [NumPy数据分析练习](https://www.numpy.org.cn/article/advanced/numpy_exercises_for_data_analysis.html)\n- 写了一个小爬虫，用来爬去一些彩票数据，主要是学习beautifulsoup（不是认真的\n\n\t\tfrom bs4 import BeautifulSoup\n\t\tfrom time import sleep\n\t\timport urllib\n\t\timport gzip\n\t\timport random\n\t\t\n\t\tdef get_url_page(url_):\n\t\t    rsp = urllib.request.urlopen(url_)\n\t\t    tmp = gzip.decompress(rsp.read()).decode('gbk')\n\t\t    return tmp\n\t\t\n\t\tdef main():\n\t\t    main_url = 'http://kaijiang.500.com/shtml/dlt/%s.shtml'\n\t\t    dic_res = {}\n\t\t    for idx in range(7001, 7005):\n\t\t        idx = str(idx)\n\t\t        if len(idx) == 4:\n\t\t            idx = '0' + idx\n\t\t        url_ = main_url % idx\n\t\t        html_file = get_url_page(url_)\n\t\t        soup = BeautifulSoup(html_file.encode('utf-8'), 'html.parser', from_encoding='utf-8')\n\t\t        red_ball = [i.string for i in soup.find_all(name='li', attrs={'class':'ball_red'})]\n\t\t        blue_ball = [i.string for i in soup.find_all(name='li', attrs={'class':'ball_blue'})]\n\t\t        tmp_dic = {\n\t\t            'red_ball': red_ball,\n\t\t            'blue_ball': blue_ball\n\t\t        }\n\t\t        dic_res.setdefault(idx, tmp_dic)\n\t\t        sleep(random.uniform(0.3, 3.0))\n\t\t    return dic_res\n\t\t\n\t\tif __name__ == '__main__':\n\t\t    res = main()\n\t\t    import pickle\n\t\t    with open('all_ball.data','wb') as hdl:\n\t\t        pickle.dump(res, hdl)\n\n\n**明日计划**\n搞定CNN、RNN/LSTM\n\n---\n**<span id=\"1228\">12-28</span>**\n\n- https://zhuanlan.zhihu.com/p/28196873\n- https://zhuanlan.zhihu.com/p/27087310\n- https://github.com/NELSONZHAO/zhihu/tree/master/anna_lstm\n- https://github.com/hzy46/Char-RNN-TensorFlow\n- https://gist.github.com/karpathy/d4dee566867f8291f086\n\n---\n**<span id=\"1229\">12-29</span>**\n\n**做事需专注**\n\n- 同步了一篇论文笔记：https://junzx.github.io/2018/12/29/end2end-coreference-resolution/","source":"_posts/2018-work-schedule.md","raw":"---\ntitle: 2018日记&日报\ndate: 2018-11-29\ntags:\n    - 随笔\ndescription: 用来记录每天的工作-2018\ntop: 50\n---\n### 使用python生成markdown格式的日历参考：[使用python生成日历](https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/)\n\n\n### November   2018          \n|Sun | Mon | Tue  | Wed | Thu | Fri | Sat \n|:-| :- | :- | :- | :- | :- | :- |\n|  |  |  |  | 1 | 2 | 3 |\n| 4| 5 | 6 | 7 | 8 | 9 | 10 |\n| 11 | 12 | 13 | 14 | 15 | 16 | 17 |\n| 19 | 20 | 21 | 22 | 23 | 24 | 25 |\n| 25 | 26 | 27 | 28 | [29](#1129) | [30](#1130) |\n\n\n\n---\n**<span id=\"1129\">11-29</span>**\n\n- 动手刷leetcode的习题\n- 总结需要学习的计能\n\t- python高级编程\n\t- 爬虫框架\n\t- Mongodb操作\n\t- 基础数据结构\n\t- 基础算法\n\t- ML\n\n---\n**<span id=\"1130\">11-30</span>**\n\n- 处理好快递的相关事宜\n- 学习树相关内容，笔记如下：\n\t- **二叉树**：每个节点最多含有两个子树的树称为二叉树；\n\t- **完全二叉树**：对于一颗二叉树，假设其深度为d（d>1）。除了第d层外，其它各层的节点数目均已达最大值，且第d层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树；\n\t- **满二叉树**：所有叶节点都在最底层的完全二叉树；\n\t- **平衡二叉树（AVL树）**：当且仅当任何节点的两棵子树的高度差不大于1的二叉树；\n\t- **红黑树**: 是一种自平衡二叉查找树，它是在1972年由鲁道夫·贝尔发明的，他称之为”对称二叉B树”\n\t- **排序二叉树(二叉查找树)**：也称二叉搜索树、有序二叉树；\n\t- **霍夫曼树**：带权路径最短的二叉树称为哈夫曼树或最优二叉树；\n\t- **B树**：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个子树。\n- 参考资料：\n\t- [浅谈数据结构-二叉树](https://www.cnblogs.com/polly333/p/4740355.html)\n\n---\n\n\n### **December   2018**          \nSun | Mon | Tue  | Wed | Thu | Fri | Sat \n---| ---| ---| ---| ---| ---| ---|\n  |  |  |  |  |  | 1 |\n [2](#122) | 3 | [4](#124) | [5](#125) | [6](#126) | 7 | 8 |\n [9](#129) | [10](#1210) | [11](#1211) | [12](#1212) | [13](#1213) | 14| 15 |\n 16 | [17](#1217) | [18](#1218) | [19](#1219) | [20](#1220) | [21](#1221) | 22 |\n 23 | 24 | 25 | [26](#1226) | [27](#1227) | [28](#1228) | [29](#1229) |\n [30](#1230) | [31](#1231) |\n\n\n---\n**<span id=\"122\">12-2</span>**\n\n**下周计划**\n\n- 准备面试相关内容\n\t- 项目经历\n\t- 实习经历\n\t- 常见问题\n- CS224n的课，完成前五节（至少）\n- 完成基础数据结构的复习\n- Leetcode简单的基础数据结构的题目\n- **！**项目归纳整理\n- 形成毕设的提纲\n- （不重要）整理博客的内容\n\n\n备忘\n\n- [二叉树的一些性质](https://www.cnblogs.com/willwu/p/6007555.html)\n- [二叉树的遍历方式](https://blog.csdn.net/u014465639/article/details/71076092)\n- [python实现二叉树](https://www.cnblogs.com/PrettyTom/p/6677993.html)\n- [也是一些遍历方式](https://www.jianshu.com/p/456af5480cee)\n\n\n\n总结：\n\n1. 遍历方式还需要再深入学习，掌握的不够牢固。\n2. 过了深度学习入门第五章内容，先继续进行接下来的章节内容。\n\n---\n**<span id=\"124\">12-4</span>**\n\n- 看coursera上数据结构关于二叉树部分的课程\n\t- 递归的遍历方法区别在于何时访问Root节点\n\t- 非递归的遍历方法稍微困难一点\n\t\n- 明日计划\n\t1. 熟练掌握各种遍历方法的python实现\n\t2. 《入门》下一章内容\n\t3. 224第二节\n\n---\n**<span id=\"125\">12-5</span>**\n\n- 《入门》第六章内容\n\t- 参数更新：\n\t\t- SGD\n\t\t- Momentum\n\t\t- AdaGrad\n\t\t- Adam\n\t- 权重初始化\n\t\t- 如果是线性激活函数，使用 1/\\sqrt{n};\n\t\t- 如果是ReLU，则使用\\sqrt{2/n}\n\t- Batch Normalization\n\t- 正则化\n- 刷了224第二节课，认为看视频的效率有点低，计划这系列课程只看7和15节，其余知识通过读书获取\n- **明日计划**\n\t- 两个小时构思毕设，做相应的准备，制定计划\n\t- 《入门》下一章内容\n\t- 关于树部分，复杂度相关知识的归纳整理\n\t- Leetcode关于树的习题\n\n---\n**<span id=\"126\">12-6</span>**\n\n- 复习cnn（[textCNN学习笔记](https://junzx.github.io/2018/12/06/cnn/)）\n- 二叉树代价分析\n\t- 深度搜索\n\t\t- 时间复杂度：O(n)，每个节点访问一次\n\t\t- 空间复杂度：最好O(logn)，最坏O(n)，与栈的深度即树的高度有关\n\t- 层次搜索\n\t\t- 时间复杂度：O(n)\n\t\t- 空间复杂度：最好O(1)，最坏O(n)，与树的最大宽度有关\n\n---\n**<span id=\"129\">12-9</span>**\n\n- 祝亲人生日快乐\n- 下周计划\n\t- 完成未完成的内容\n\t- 《TF实战》\n\n\n---\n**<span id=\"1210\">12-10</span>**\n\n- 入门tf： [TF入门——使用线性回归进行实验](https://junzx.github.io/2018/12/10/learning-tensorflow-linear-regression/)\n- 准备面试相关的内容\n- 确定近期预算\n\n---\n**<span id=\"1211\">12-11</span>**\n\n- 去便利蜂面试，面试官人很nice，但是很遗憾结果还是不理想，发现了一些问题如下\n\t1. 基础算法不够牢固，一些排序复杂度的弄不是很清楚\n\t2. 项目中算法类的不够，工程类的较多\n\t3. 现场在纸上手写代码的效率不行，思路很受干扰，说明思路不够清晰，代码还是不够熟练\n- 制定突破计划\n\n---\n**<span id=\"1212\">12-12</span>**\n\n- 着手撸排序和查找的基础算法\n- [新任务](https://github.com/huggingface/neuralcoref)\n\n---\n**<span id=\"1213\">12-13</span>**\n\n- spacy相关\n\t- https://juejin.im/post/5971a4b9f265da6c42353332\n\t- http://www.52nlp.cn/tag/spacy\n\t- 遇到一些问题，后来解决了，必须使用2.0.12版本的spacy\n\t- 示例代码：\n\t\t\tIn [1]: import spacy\n\t\t\t\n\t\t\tIn [2]: spacy.__version__\n\t\t\tOut[2]: '2.0.12'\n\t\t\t\n\t\t\tIn [3]: nlp = spacy.load('en_coref_sm')\n\t\t\t\n\t\t\tIn [4]: doc = nlp(u'My sister has a dog. She loves him')\n\t\t\t\n\t\t\tIn [5]: doc._.has_coref\n\t\t\tOut[5]: True\n\t\t\t\n\t\t\tIn [6]: doc._.coref_clusters\n\t\t\tOut[6]: [My sister: [My sister, She], a dog: [a dog, him]]\n\n- 搞定三个插入排序算法（[插入排序](https://junzx.github.io/2018/12/13/algorithm-learning-insert-sort/)）\n\n---\n**<span id=\"1217\">12-17</span>**\n\n- 上周总结\n\t- 面试bianlifeng，发现了自己的不足\n\t- 完成了三种插入排序的算法\n\t- 动手学习tf\n\t- 接到了新任务：[新任务](https://github.com/huggingface/neuralcoref)\n\n- 本周安排\n\t- 剩余的排序查找算法\n\t- **毕设提纲（必须在周三之前出来）**\n\t- 刷题\n\t- 保持每天扇贝的单词\n\n- 时间安排\n\t- 7：30 闹钟\n\t- 7：45 起床\n\t- 8：00 吃早饭\n\t- 9：30 之前 扇贝当日内容\n\t- 9：30~11：00 上午工作\n\t- 13：00 下午工作开始\n\t- 17：30 晚饭\n\t- 19：00 晚上工作\n\t- 23：00 上床睡觉\n\n---\n**<span id=\"1218\">12-18</span>**\n\n**今日成果**\n\n- 完成扇贝单词\n- 列出了毕设提纲\n- 完成选择排序：[算法入门——选择排序](https://junzx.github.io/2018/12/18/algorithm-learning-select-sort/)\n\n**明日计划**\n\n- 扇贝单词\n- 在屋内适当活动\n- 与相机和背包的客服沟通\n- 完成选择排序的笔记\n- 完成交换排序\n\n---\n**<span id=\"1219\">12-19</span>**\n\n**今日成果**\n\n- 扇贝单词\n- 跟客服扯皮 \n- 完善选择排序\n\n**明日计划**\n\n- 扇贝单词\n- 剩余的所有排序（快排，归并）\n- 毕设内容\n- 准备面试\n\n---\n**<span id=\"1220\">12-20</span>**\n\n**积累**\n\n1. The choice of your career path is **critical** to your future. 职业道路的选择对你的未来至关重要。\n2. The government is responsible for the imporvement of the transportation and public **utilities**. （政府负责改善交通与公共措施） \n3. The average income is high, though many people earn just a **fraction** of that average.（平均收入很高，但是很多人赚的只有平均工资的一小部分）\n4. world **renowned**（举世闻名）\n5. A **substantial** number of people supported the reform policy.（相当多的人支持这一改革政策）\n6. Increased competition makes it **essential** for the business to innovate.（日益激烈的竞争使得企业必须创新）\n\n**今日成果**\n- 扇贝单词\n- 完成交换排序相关[算法入门——交换排序](https://junzx.github.io/2018/12/20/algorithm-learning-swap-sort/)\n- 完成归并排序相关[算法入门——归并排序](https://junzx.github.io/2018/12/20/algorithm-learning-merge-sort/)\n\n**明日计划**\n- 书上关于查找的部分\n- 毕设内容\n- 整理简历&看面经&问经验\n\n---\n**<span id=\"1221\">12-21</span>**\n**积累**\n1. an area of **outstanding** natural beauty.(自然风景极美的地区/优秀的、突出的)\n\n**今日成果**\n- 扇贝单词\n- 排序总结\n- 过以前的论文，找到对毕设可能有用的\n\n![image](/2018-work-schedule/1.JPG)\n\n| 算法 | 稳定性 | 时间复杂度 | 空间复杂度 | \n| ---| ---| ---| ---| ---|\n| 直接插入排序 | 稳定 | O(n^2) | O(1) |\n| 折半插入排序 | 稳定 | O(n^2) | O(1) |\n| 希尔排序 | 不稳定 | O(n^2) | O(1) |\n| 冒泡排序 | 稳定 | O(n^2) | O(1) | \n| 快速排序 | 不稳定 | O(n^2) | 最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n) |\n| 简单选择排序 | 不稳定 | O(n^2) | O(1) |\n| 堆排序 | 不稳定 | O(nlog2n) | O(1) |\n| 归并排序 | 稳定 | O(nlog2n) | O(n) |\n| 基数排序 | 稳定 | O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r) | O(r) r个队列 |\n\n---\n**<span id=\"1226\">12-26</span>**\n\n**本周计划**\n- 基础算法\n- 整理GitHub代码以及收藏的内容\n- TF\n- Numpy——参考：[NumPy 中文文档](https://www.numpy.org.cn/)\n- 拓展阅读——[自动对话](http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf)\n- BERT相关内容——[BERT相关论文、文章和代码资源汇总-知乎](https://zhuanlan.zhihu.com/p/50717786)\n- [端到端的机器学习项目](https://github.com/DeqianBai/Your-first-machine-learning-Project---End-to-End-in-Python)\n- 单词\n\n---\n**<span id=\"1227\">12-27</span>**\n\n**今日安排**\n- [x] Numpy入门\n- [ ]单词\n- [x]整理GitHub stars\n\n**积累**\n- [创建一个数组](https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%95%B0%E7%BB%84)\n- [多维数组切片](https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E5%88%87%E7%89%87)\n\n\n\tdef quicksort(arr):\n\t    if len(arr) <= 1:\n\t        return arr\n\t    pivot = arr[len(arr) // 2]\n\t    left = [x for x in arr if x < pivot]\n\t    middle = [x for x in arr if x == pivot]\n\t    right = [x for x in arr if x > pivot]\n\t    return quicksort(left) + middle + quicksort(right)\n\t\n\t\tprint quicksort([3,6,8,10,1,2,1])\n\n- [NumPy数据分析练习](https://www.numpy.org.cn/article/advanced/numpy_exercises_for_data_analysis.html)\n- 写了一个小爬虫，用来爬去一些彩票数据，主要是学习beautifulsoup（不是认真的\n\n\t\tfrom bs4 import BeautifulSoup\n\t\tfrom time import sleep\n\t\timport urllib\n\t\timport gzip\n\t\timport random\n\t\t\n\t\tdef get_url_page(url_):\n\t\t    rsp = urllib.request.urlopen(url_)\n\t\t    tmp = gzip.decompress(rsp.read()).decode('gbk')\n\t\t    return tmp\n\t\t\n\t\tdef main():\n\t\t    main_url = 'http://kaijiang.500.com/shtml/dlt/%s.shtml'\n\t\t    dic_res = {}\n\t\t    for idx in range(7001, 7005):\n\t\t        idx = str(idx)\n\t\t        if len(idx) == 4:\n\t\t            idx = '0' + idx\n\t\t        url_ = main_url % idx\n\t\t        html_file = get_url_page(url_)\n\t\t        soup = BeautifulSoup(html_file.encode('utf-8'), 'html.parser', from_encoding='utf-8')\n\t\t        red_ball = [i.string for i in soup.find_all(name='li', attrs={'class':'ball_red'})]\n\t\t        blue_ball = [i.string for i in soup.find_all(name='li', attrs={'class':'ball_blue'})]\n\t\t        tmp_dic = {\n\t\t            'red_ball': red_ball,\n\t\t            'blue_ball': blue_ball\n\t\t        }\n\t\t        dic_res.setdefault(idx, tmp_dic)\n\t\t        sleep(random.uniform(0.3, 3.0))\n\t\t    return dic_res\n\t\t\n\t\tif __name__ == '__main__':\n\t\t    res = main()\n\t\t    import pickle\n\t\t    with open('all_ball.data','wb') as hdl:\n\t\t        pickle.dump(res, hdl)\n\n\n**明日计划**\n搞定CNN、RNN/LSTM\n\n---\n**<span id=\"1228\">12-28</span>**\n\n- https://zhuanlan.zhihu.com/p/28196873\n- https://zhuanlan.zhihu.com/p/27087310\n- https://github.com/NELSONZHAO/zhihu/tree/master/anna_lstm\n- https://github.com/hzy46/Char-RNN-TensorFlow\n- https://gist.github.com/karpathy/d4dee566867f8291f086\n\n---\n**<span id=\"1229\">12-29</span>**\n\n**做事需专注**\n\n- 同步了一篇论文笔记：https://junzx.github.io/2018/12/29/end2end-coreference-resolution/","slug":"2018-work-schedule","published":1,"updated":"2019-01-22T04:59:24.924Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2nj0017scft1j6zn1zj","content":"<h3 id=\"使用python生成markdown格式的日历参考：使用python生成日历\"><a href=\"#使用python生成markdown格式的日历参考：使用python生成日历\" class=\"headerlink\" title=\"使用python生成markdown格式的日历参考：使用python生成日历\"></a>使用python生成markdown格式的日历参考：<a href=\"https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/\" target=\"_blank\" rel=\"noopener\">使用python生成日历</a></h3><h3 id=\"November-2018\"><a href=\"#November-2018\" class=\"headerlink\" title=\"November   2018\"></a>November   2018</h3><table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Sun</th>\n<th style=\"text-align:left\">Mon</th>\n<th style=\"text-align:left\">Tue</th>\n<th style=\"text-align:left\">Wed</th>\n<th style=\"text-align:left\">Thu</th>\n<th style=\"text-align:left\">Fri</th>\n<th style=\"text-align:left\">Sat </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">3</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">6</td>\n<td style=\"text-align:left\">7</td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">11</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">13</td>\n<td style=\"text-align:left\">14</td>\n<td style=\"text-align:left\">15</td>\n<td style=\"text-align:left\">16</td>\n<td style=\"text-align:left\">17</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">19</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">21</td>\n<td style=\"text-align:left\">22</td>\n<td style=\"text-align:left\">23</td>\n<td style=\"text-align:left\">24</td>\n<td style=\"text-align:left\">25</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">25</td>\n<td style=\"text-align:left\">26</td>\n<td style=\"text-align:left\">27</td>\n<td style=\"text-align:left\">28</td>\n<td style=\"text-align:left\"><a href=\"#1129\">29</a></td>\n<td style=\"text-align:left\"><a href=\"#1130\">30</a></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p><strong><span id=\"1129\">11-29</span></strong></p>\n<ul>\n<li>动手刷leetcode的习题</li>\n<li>总结需要学习的计能<ul>\n<li>python高级编程</li>\n<li>爬虫框架</li>\n<li>Mongodb操作</li>\n<li>基础数据结构</li>\n<li>基础算法</li>\n<li>ML</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1130\">11-30</span></strong></p>\n<ul>\n<li>处理好快递的相关事宜</li>\n<li>学习树相关内容，笔记如下：<ul>\n<li><strong>二叉树</strong>：每个节点最多含有两个子树的树称为二叉树；</li>\n<li><strong>完全二叉树</strong>：对于一颗二叉树，假设其深度为d（d&gt;1）。除了第d层外，其它各层的节点数目均已达最大值，且第d层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树；</li>\n<li><strong>满二叉树</strong>：所有叶节点都在最底层的完全二叉树；</li>\n<li><strong>平衡二叉树（AVL树）</strong>：当且仅当任何节点的两棵子树的高度差不大于1的二叉树；</li>\n<li><strong>红黑树</strong>: 是一种自平衡二叉查找树，它是在1972年由鲁道夫·贝尔发明的，他称之为”对称二叉B树”</li>\n<li><strong>排序二叉树(二叉查找树)</strong>：也称二叉搜索树、有序二叉树；</li>\n<li><strong>霍夫曼树</strong>：带权路径最短的二叉树称为哈夫曼树或最优二叉树；</li>\n<li><strong>B树</strong>：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个子树。</li>\n</ul>\n</li>\n<li>参考资料：<ul>\n<li><a href=\"https://www.cnblogs.com/polly333/p/4740355.html\" target=\"_blank\" rel=\"noopener\">浅谈数据结构-二叉树</a></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"December-2018\"><a href=\"#December-2018\" class=\"headerlink\" title=\"December   2018\"></a><strong>December   2018</strong></h3><table>\n<thead>\n<tr>\n<th>Sun</th>\n<th>Mon</th>\n<th>Tue</th>\n<th>Wed</th>\n<th>Thu</th>\n<th>Fri</th>\n<th>Sat </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td>1</td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#122\">2</a></td>\n<td>3</td>\n<td><a href=\"#124\">4</a></td>\n<td><a href=\"#125\">5</a></td>\n<td><a href=\"#126\">6</a></td>\n<td>7</td>\n<td>8</td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#129\">9</a></td>\n<td><a href=\"#1210\">10</a></td>\n<td><a href=\"#1211\">11</a></td>\n<td><a href=\"#1212\">12</a></td>\n<td><a href=\"#1213\">13</a></td>\n<td>14</td>\n<td>15</td>\n<td></td>\n</tr>\n<tr>\n<td> 16</td>\n<td><a href=\"#1217\">17</a></td>\n<td><a href=\"#1218\">18</a></td>\n<td><a href=\"#1219\">19</a></td>\n<td><a href=\"#1220\">20</a></td>\n<td><a href=\"#1221\">21</a></td>\n<td>22</td>\n<td></td>\n</tr>\n<tr>\n<td> 23</td>\n<td>24</td>\n<td>25</td>\n<td><a href=\"#1226\">26</a></td>\n<td><a href=\"#1227\">27</a></td>\n<td><a href=\"#1228\">28</a></td>\n<td><a href=\"#1229\">29</a></td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#1230\">30</a></td>\n<td><a href=\"#1231\">31</a></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p><strong><span id=\"122\">12-2</span></strong></p>\n<p><strong>下周计划</strong></p>\n<ul>\n<li>准备面试相关内容<ul>\n<li>项目经历</li>\n<li>实习经历</li>\n<li>常见问题</li>\n</ul>\n</li>\n<li>CS224n的课，完成前五节（至少）</li>\n<li>完成基础数据结构的复习</li>\n<li>Leetcode简单的基础数据结构的题目</li>\n<li><strong>！</strong>项目归纳整理</li>\n<li>形成毕设的提纲</li>\n<li>（不重要）整理博客的内容</li>\n</ul>\n<p>备忘</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/willwu/p/6007555.html\" target=\"_blank\" rel=\"noopener\">二叉树的一些性质</a></li>\n<li><a href=\"https://blog.csdn.net/u014465639/article/details/71076092\" target=\"_blank\" rel=\"noopener\">二叉树的遍历方式</a></li>\n<li><a href=\"https://www.cnblogs.com/PrettyTom/p/6677993.html\" target=\"_blank\" rel=\"noopener\">python实现二叉树</a></li>\n<li><a href=\"https://www.jianshu.com/p/456af5480cee\" target=\"_blank\" rel=\"noopener\">也是一些遍历方式</a></li>\n</ul>\n<p>总结：</p>\n<ol>\n<li>遍历方式还需要再深入学习，掌握的不够牢固。</li>\n<li>过了深度学习入门第五章内容，先继续进行接下来的章节内容。</li>\n</ol>\n<hr>\n<p><strong><span id=\"124\">12-4</span></strong></p>\n<ul>\n<li><p>看coursera上数据结构关于二叉树部分的课程</p>\n<ul>\n<li>递归的遍历方法区别在于何时访问Root节点</li>\n<li>非递归的遍历方法稍微困难一点</li>\n</ul>\n</li>\n<li><p>明日计划</p>\n<ol>\n<li>熟练掌握各种遍历方法的python实现</li>\n<li>《入门》下一章内容</li>\n<li>224第二节</li>\n</ol>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"125\">12-5</span></strong></p>\n<ul>\n<li>《入门》第六章内容<ul>\n<li>参数更新：<ul>\n<li>SGD</li>\n<li>Momentum</li>\n<li>AdaGrad</li>\n<li>Adam</li>\n</ul>\n</li>\n<li>权重初始化<ul>\n<li>如果是线性激活函数，使用 1/\\sqrt{n};</li>\n<li>如果是ReLU，则使用\\sqrt{2/n}</li>\n</ul>\n</li>\n<li>Batch Normalization</li>\n<li>正则化</li>\n</ul>\n</li>\n<li>刷了224第二节课，认为看视频的效率有点低，计划这系列课程只看7和15节，其余知识通过读书获取</li>\n<li><strong>明日计划</strong><ul>\n<li>两个小时构思毕设，做相应的准备，制定计划</li>\n<li>《入门》下一章内容</li>\n<li>关于树部分，复杂度相关知识的归纳整理</li>\n<li>Leetcode关于树的习题</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"126\">12-6</span></strong></p>\n<ul>\n<li>复习cnn（<a href=\"https://junzx.github.io/2018/12/06/cnn/\" target=\"_blank\" rel=\"noopener\">textCNN学习笔记</a>）</li>\n<li>二叉树代价分析<ul>\n<li>深度搜索<ul>\n<li>时间复杂度：O(n)，每个节点访问一次</li>\n<li>空间复杂度：最好O(logn)，最坏O(n)，与栈的深度即树的高度有关</li>\n</ul>\n</li>\n<li>层次搜索<ul>\n<li>时间复杂度：O(n)</li>\n<li>空间复杂度：最好O(1)，最坏O(n)，与树的最大宽度有关</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"129\">12-9</span></strong></p>\n<ul>\n<li>祝亲人生日快乐</li>\n<li>下周计划<ul>\n<li>完成未完成的内容</li>\n<li>《TF实战》</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1210\">12-10</span></strong></p>\n<ul>\n<li>入门tf： <a href=\"https://junzx.github.io/2018/12/10/learning-tensorflow-linear-regression/\" target=\"_blank\" rel=\"noopener\">TF入门——使用线性回归进行实验</a></li>\n<li>准备面试相关的内容</li>\n<li>确定近期预算</li>\n</ul>\n<hr>\n<p><strong><span id=\"1211\">12-11</span></strong></p>\n<ul>\n<li>去便利蜂面试，面试官人很nice，但是很遗憾结果还是不理想，发现了一些问题如下<ol>\n<li>基础算法不够牢固，一些排序复杂度的弄不是很清楚</li>\n<li>项目中算法类的不够，工程类的较多</li>\n<li>现场在纸上手写代码的效率不行，思路很受干扰，说明思路不够清晰，代码还是不够熟练</li>\n</ol>\n</li>\n<li>制定突破计划</li>\n</ul>\n<hr>\n<p><strong><span id=\"1212\">12-12</span></strong></p>\n<ul>\n<li>着手撸排序和查找的基础算法</li>\n<li><a href=\"https://github.com/huggingface/neuralcoref\" target=\"_blank\" rel=\"noopener\">新任务</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"1213\">12-13</span></strong></p>\n<ul>\n<li><p>spacy相关</p>\n<ul>\n<li><a href=\"https://juejin.im/post/5971a4b9f265da6c42353332\" target=\"_blank\" rel=\"noopener\">https://juejin.im/post/5971a4b9f265da6c42353332</a></li>\n<li><a href=\"http://www.52nlp.cn/tag/spacy\" target=\"_blank\" rel=\"noopener\">http://www.52nlp.cn/tag/spacy</a></li>\n<li>遇到一些问题，后来解决了，必须使用2.0.12版本的spacy</li>\n<li><p>示例代码：</p>\n<pre><code>In [1]: import spacy\n\nIn [2]: spacy.__version__\nOut[2]: &apos;2.0.12&apos;\n\nIn [3]: nlp = spacy.load(&apos;en_coref_sm&apos;)\n\nIn [4]: doc = nlp(u&apos;My sister has a dog. She loves him&apos;)\n\nIn [5]: doc._.has_coref\nOut[5]: True\n\nIn [6]: doc._.coref_clusters\nOut[6]: [My sister: [My sister, She], a dog: [a dog, him]]\n</code></pre></li>\n</ul>\n</li>\n<li><p>搞定三个插入排序算法（<a href=\"https://junzx.github.io/2018/12/13/algorithm-learning-insert-sort/\" target=\"_blank\" rel=\"noopener\">插入排序</a>）</p>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1217\">12-17</span></strong></p>\n<ul>\n<li><p>上周总结</p>\n<ul>\n<li>面试bianlifeng，发现了自己的不足</li>\n<li>完成了三种插入排序的算法</li>\n<li>动手学习tf</li>\n<li>接到了新任务：<a href=\"https://github.com/huggingface/neuralcoref\" target=\"_blank\" rel=\"noopener\">新任务</a></li>\n</ul>\n</li>\n<li><p>本周安排</p>\n<ul>\n<li>剩余的排序查找算法</li>\n<li><strong>毕设提纲（必须在周三之前出来）</strong></li>\n<li>刷题</li>\n<li>保持每天扇贝的单词</li>\n</ul>\n</li>\n<li><p>时间安排</p>\n<ul>\n<li>7：30 闹钟</li>\n<li>7：45 起床</li>\n<li>8：00 吃早饭</li>\n<li>9：30 之前 扇贝当日内容</li>\n<li>9：30~11：00 上午工作</li>\n<li>13：00 下午工作开始</li>\n<li>17：30 晚饭</li>\n<li>19：00 晚上工作</li>\n<li>23：00 上床睡觉</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1218\">12-18</span></strong></p>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>完成扇贝单词</li>\n<li>列出了毕设提纲</li>\n<li>完成选择排序：<a href=\"https://junzx.github.io/2018/12/18/algorithm-learning-select-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——选择排序</a></li>\n</ul>\n<p><strong>明日计划</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>在屋内适当活动</li>\n<li>与相机和背包的客服沟通</li>\n<li>完成选择排序的笔记</li>\n<li>完成交换排序</li>\n</ul>\n<hr>\n<p><strong><span id=\"1219\">12-19</span></strong></p>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>跟客服扯皮 </li>\n<li>完善选择排序</li>\n</ul>\n<p><strong>明日计划</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>剩余的所有排序（快排，归并）</li>\n<li>毕设内容</li>\n<li>准备面试</li>\n</ul>\n<hr>\n<p><strong><span id=\"1220\">12-20</span></strong></p>\n<p><strong>积累</strong></p>\n<ol>\n<li>The choice of your career path is <strong>critical</strong> to your future. 职业道路的选择对你的未来至关重要。</li>\n<li>The government is responsible for the imporvement of the transportation and public <strong>utilities</strong>. （政府负责改善交通与公共措施） </li>\n<li>The average income is high, though many people earn just a <strong>fraction</strong> of that average.（平均收入很高，但是很多人赚的只有平均工资的一小部分）</li>\n<li>world <strong>renowned</strong>（举世闻名）</li>\n<li>A <strong>substantial</strong> number of people supported the reform policy.（相当多的人支持这一改革政策）</li>\n<li>Increased competition makes it <strong>essential</strong> for the business to innovate.（日益激烈的竞争使得企业必须创新）</li>\n</ol>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>完成交换排序相关<a href=\"https://junzx.github.io/2018/12/20/algorithm-learning-swap-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——交换排序</a></li>\n<li>完成归并排序相关<a href=\"https://junzx.github.io/2018/12/20/algorithm-learning-merge-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——归并排序</a></li>\n</ul>\n<p><strong>明日计划</strong></p>\n<ul>\n<li>书上关于查找的部分</li>\n<li>毕设内容</li>\n<li>整理简历&amp;看面经&amp;问经验</li>\n</ul>\n<hr>\n<p><strong><span id=\"1221\">12-21</span></strong><br><strong>积累</strong></p>\n<ol>\n<li>an area of <strong>outstanding</strong> natural beauty.(自然风景极美的地区/优秀的、突出的)</li>\n</ol>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>排序总结</li>\n<li>过以前的论文，找到对毕设可能有用的</li>\n</ul>\n<p><img src=\"/2018/11/29/2018-work-schedule/1.JPG\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>算法</th>\n<th>稳定性</th>\n<th>时间复杂度</th>\n<th>空间复杂度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>直接插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>折半插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>希尔排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>冒泡排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>快速排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n)</td>\n</tr>\n<tr>\n<td>简单选择排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>堆排序</td>\n<td>不稳定</td>\n<td>O(nlog2n)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>归并排序</td>\n<td>稳定</td>\n<td>O(nlog2n)</td>\n<td>O(n)</td>\n</tr>\n<tr>\n<td>基数排序</td>\n<td>稳定</td>\n<td>O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r)</td>\n<td>O(r) r个队列</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p><strong><span id=\"1226\">12-26</span></strong></p>\n<p><strong>本周计划</strong></p>\n<ul>\n<li>基础算法</li>\n<li>整理GitHub代码以及收藏的内容</li>\n<li>TF</li>\n<li>Numpy——参考：<a href=\"https://www.numpy.org.cn/\" target=\"_blank\" rel=\"noopener\">NumPy 中文文档</a></li>\n<li>拓展阅读——<a href=\"http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf\" target=\"_blank\" rel=\"noopener\">自动对话</a></li>\n<li>BERT相关内容——<a href=\"https://zhuanlan.zhihu.com/p/50717786\" target=\"_blank\" rel=\"noopener\">BERT相关论文、文章和代码资源汇总-知乎</a></li>\n<li><a href=\"https://github.com/DeqianBai/Your-first-machine-learning-Project---End-to-End-in-Python\" target=\"_blank\" rel=\"noopener\">端到端的机器学习项目</a></li>\n<li>单词</li>\n</ul>\n<hr>\n<p><strong><span id=\"1227\">12-27</span></strong></p>\n<p><strong>今日安排</strong></p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> Numpy入门</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 单词</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 整理GitHub stars</li>\n</ul>\n<p><strong>积累</strong></p>\n<ul>\n<li><a href=\"https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%95%B0%E7%BB%84\" target=\"_blank\" rel=\"noopener\">创建一个数组</a></li>\n<li><a href=\"https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E5%88%87%E7%89%87\" target=\"_blank\" rel=\"noopener\">多维数组切片</a></li>\n</ul>\n<pre><code>def quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\n    print quicksort([3,6,8,10,1,2,1])\n</code></pre><ul>\n<li><a href=\"https://www.numpy.org.cn/article/advanced/numpy_exercises_for_data_analysis.html\" target=\"_blank\" rel=\"noopener\">NumPy数据分析练习</a></li>\n<li><p>写了一个小爬虫，用来爬去一些彩票数据，主要是学习beautifulsoup（不是认真的</p>\n<pre><code>from bs4 import BeautifulSoup\nfrom time import sleep\nimport urllib\nimport gzip\nimport random\n\ndef get_url_page(url_):\n    rsp = urllib.request.urlopen(url_)\n    tmp = gzip.decompress(rsp.read()).decode(&apos;gbk&apos;)\n    return tmp\n\ndef main():\n    main_url = &apos;http://kaijiang.500.com/shtml/dlt/%s.shtml&apos;\n    dic_res = {}\n    for idx in range(7001, 7005):\n        idx = str(idx)\n        if len(idx) == 4:\n            idx = &apos;0&apos; + idx\n        url_ = main_url % idx\n        html_file = get_url_page(url_)\n        soup = BeautifulSoup(html_file.encode(&apos;utf-8&apos;), &apos;html.parser&apos;, from_encoding=&apos;utf-8&apos;)\n        red_ball = [i.string for i in soup.find_all(name=&apos;li&apos;, attrs={&apos;class&apos;:&apos;ball_red&apos;})]\n        blue_ball = [i.string for i in soup.find_all(name=&apos;li&apos;, attrs={&apos;class&apos;:&apos;ball_blue&apos;})]\n        tmp_dic = {\n            &apos;red_ball&apos;: red_ball,\n            &apos;blue_ball&apos;: blue_ball\n        }\n        dic_res.setdefault(idx, tmp_dic)\n        sleep(random.uniform(0.3, 3.0))\n    return dic_res\n\nif __name__ == &apos;__main__&apos;:\n    res = main()\n    import pickle\n    with open(&apos;all_ball.data&apos;,&apos;wb&apos;) as hdl:\n        pickle.dump(res, hdl)\n</code></pre></li>\n</ul>\n<p><strong>明日计划</strong><br>搞定CNN、RNN/LSTM</p>\n<hr>\n<p><strong><span id=\"1228\">12-28</span></strong></p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/28196873\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/28196873</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/27087310\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/27087310</a></li>\n<li><a href=\"https://github.com/NELSONZHAO/zhihu/tree/master/anna_lstm\" target=\"_blank\" rel=\"noopener\">https://github.com/NELSONZHAO/zhihu/tree/master/anna_lstm</a></li>\n<li><a href=\"https://github.com/hzy46/Char-RNN-TensorFlow\" target=\"_blank\" rel=\"noopener\">https://github.com/hzy46/Char-RNN-TensorFlow</a></li>\n<li><a href=\"https://gist.github.com/karpathy/d4dee566867f8291f086\" target=\"_blank\" rel=\"noopener\">https://gist.github.com/karpathy/d4dee566867f8291f086</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"1229\">12-29</span></strong></p>\n<p><strong>做事需专注</strong></p>\n<ul>\n<li>同步了一篇论文笔记：<a href=\"https://junzx.github.io/2018/12/29/end2end-coreference-resolution/\" target=\"_blank\" rel=\"noopener\">https://junzx.github.io/2018/12/29/end2end-coreference-resolution/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h3 id=\"使用python生成markdown格式的日历参考：使用python生成日历\"><a href=\"#使用python生成markdown格式的日历参考：使用python生成日历\" class=\"headerlink\" title=\"使用python生成markdown格式的日历参考：使用python生成日历\"></a>使用python生成markdown格式的日历参考：<a href=\"https://junzx.github.io/2018/12/05/how-to-use-python-to-build-markdown-calc/\" target=\"_blank\" rel=\"noopener\">使用python生成日历</a></h3><h3 id=\"November-2018\"><a href=\"#November-2018\" class=\"headerlink\" title=\"November   2018\"></a>November   2018</h3><table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Sun</th>\n<th style=\"text-align:left\">Mon</th>\n<th style=\"text-align:left\">Tue</th>\n<th style=\"text-align:left\">Wed</th>\n<th style=\"text-align:left\">Thu</th>\n<th style=\"text-align:left\">Fri</th>\n<th style=\"text-align:left\">Sat </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\"></td>\n<td style=\"text-align:left\">1</td>\n<td style=\"text-align:left\">2</td>\n<td style=\"text-align:left\">3</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">4</td>\n<td style=\"text-align:left\">5</td>\n<td style=\"text-align:left\">6</td>\n<td style=\"text-align:left\">7</td>\n<td style=\"text-align:left\">8</td>\n<td style=\"text-align:left\">9</td>\n<td style=\"text-align:left\">10</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">11</td>\n<td style=\"text-align:left\">12</td>\n<td style=\"text-align:left\">13</td>\n<td style=\"text-align:left\">14</td>\n<td style=\"text-align:left\">15</td>\n<td style=\"text-align:left\">16</td>\n<td style=\"text-align:left\">17</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">19</td>\n<td style=\"text-align:left\">20</td>\n<td style=\"text-align:left\">21</td>\n<td style=\"text-align:left\">22</td>\n<td style=\"text-align:left\">23</td>\n<td style=\"text-align:left\">24</td>\n<td style=\"text-align:left\">25</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">25</td>\n<td style=\"text-align:left\">26</td>\n<td style=\"text-align:left\">27</td>\n<td style=\"text-align:left\">28</td>\n<td style=\"text-align:left\"><a href=\"#1129\">29</a></td>\n<td style=\"text-align:left\"><a href=\"#1130\">30</a></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p><strong><span id=\"1129\">11-29</span></strong></p>\n<ul>\n<li>动手刷leetcode的习题</li>\n<li>总结需要学习的计能<ul>\n<li>python高级编程</li>\n<li>爬虫框架</li>\n<li>Mongodb操作</li>\n<li>基础数据结构</li>\n<li>基础算法</li>\n<li>ML</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1130\">11-30</span></strong></p>\n<ul>\n<li>处理好快递的相关事宜</li>\n<li>学习树相关内容，笔记如下：<ul>\n<li><strong>二叉树</strong>：每个节点最多含有两个子树的树称为二叉树；</li>\n<li><strong>完全二叉树</strong>：对于一颗二叉树，假设其深度为d（d&gt;1）。除了第d层外，其它各层的节点数目均已达最大值，且第d层所有节点从左向右连续地紧密排列，这样的二叉树被称为完全二叉树；</li>\n<li><strong>满二叉树</strong>：所有叶节点都在最底层的完全二叉树；</li>\n<li><strong>平衡二叉树（AVL树）</strong>：当且仅当任何节点的两棵子树的高度差不大于1的二叉树；</li>\n<li><strong>红黑树</strong>: 是一种自平衡二叉查找树，它是在1972年由鲁道夫·贝尔发明的，他称之为”对称二叉B树”</li>\n<li><strong>排序二叉树(二叉查找树)</strong>：也称二叉搜索树、有序二叉树；</li>\n<li><strong>霍夫曼树</strong>：带权路径最短的二叉树称为哈夫曼树或最优二叉树；</li>\n<li><strong>B树</strong>：一种对读写操作进行优化的自平衡的二叉查找树，能够保持数据有序，拥有多于两个子树。</li>\n</ul>\n</li>\n<li>参考资料：<ul>\n<li><a href=\"https://www.cnblogs.com/polly333/p/4740355.html\" target=\"_blank\" rel=\"noopener\">浅谈数据结构-二叉树</a></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h3 id=\"December-2018\"><a href=\"#December-2018\" class=\"headerlink\" title=\"December   2018\"></a><strong>December   2018</strong></h3><table>\n<thead>\n<tr>\n<th>Sun</th>\n<th>Mon</th>\n<th>Tue</th>\n<th>Wed</th>\n<th>Thu</th>\n<th>Fri</th>\n<th>Sat </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td></td>\n<td>1</td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#122\">2</a></td>\n<td>3</td>\n<td><a href=\"#124\">4</a></td>\n<td><a href=\"#125\">5</a></td>\n<td><a href=\"#126\">6</a></td>\n<td>7</td>\n<td>8</td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#129\">9</a></td>\n<td><a href=\"#1210\">10</a></td>\n<td><a href=\"#1211\">11</a></td>\n<td><a href=\"#1212\">12</a></td>\n<td><a href=\"#1213\">13</a></td>\n<td>14</td>\n<td>15</td>\n<td></td>\n</tr>\n<tr>\n<td> 16</td>\n<td><a href=\"#1217\">17</a></td>\n<td><a href=\"#1218\">18</a></td>\n<td><a href=\"#1219\">19</a></td>\n<td><a href=\"#1220\">20</a></td>\n<td><a href=\"#1221\">21</a></td>\n<td>22</td>\n<td></td>\n</tr>\n<tr>\n<td> 23</td>\n<td>24</td>\n<td>25</td>\n<td><a href=\"#1226\">26</a></td>\n<td><a href=\"#1227\">27</a></td>\n<td><a href=\"#1228\">28</a></td>\n<td><a href=\"#1229\">29</a></td>\n<td></td>\n</tr>\n<tr>\n<td> <a href=\"#1230\">30</a></td>\n<td><a href=\"#1231\">31</a></td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p><strong><span id=\"122\">12-2</span></strong></p>\n<p><strong>下周计划</strong></p>\n<ul>\n<li>准备面试相关内容<ul>\n<li>项目经历</li>\n<li>实习经历</li>\n<li>常见问题</li>\n</ul>\n</li>\n<li>CS224n的课，完成前五节（至少）</li>\n<li>完成基础数据结构的复习</li>\n<li>Leetcode简单的基础数据结构的题目</li>\n<li><strong>！</strong>项目归纳整理</li>\n<li>形成毕设的提纲</li>\n<li>（不重要）整理博客的内容</li>\n</ul>\n<p>备忘</p>\n<ul>\n<li><a href=\"https://www.cnblogs.com/willwu/p/6007555.html\" target=\"_blank\" rel=\"noopener\">二叉树的一些性质</a></li>\n<li><a href=\"https://blog.csdn.net/u014465639/article/details/71076092\" target=\"_blank\" rel=\"noopener\">二叉树的遍历方式</a></li>\n<li><a href=\"https://www.cnblogs.com/PrettyTom/p/6677993.html\" target=\"_blank\" rel=\"noopener\">python实现二叉树</a></li>\n<li><a href=\"https://www.jianshu.com/p/456af5480cee\" target=\"_blank\" rel=\"noopener\">也是一些遍历方式</a></li>\n</ul>\n<p>总结：</p>\n<ol>\n<li>遍历方式还需要再深入学习，掌握的不够牢固。</li>\n<li>过了深度学习入门第五章内容，先继续进行接下来的章节内容。</li>\n</ol>\n<hr>\n<p><strong><span id=\"124\">12-4</span></strong></p>\n<ul>\n<li><p>看coursera上数据结构关于二叉树部分的课程</p>\n<ul>\n<li>递归的遍历方法区别在于何时访问Root节点</li>\n<li>非递归的遍历方法稍微困难一点</li>\n</ul>\n</li>\n<li><p>明日计划</p>\n<ol>\n<li>熟练掌握各种遍历方法的python实现</li>\n<li>《入门》下一章内容</li>\n<li>224第二节</li>\n</ol>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"125\">12-5</span></strong></p>\n<ul>\n<li>《入门》第六章内容<ul>\n<li>参数更新：<ul>\n<li>SGD</li>\n<li>Momentum</li>\n<li>AdaGrad</li>\n<li>Adam</li>\n</ul>\n</li>\n<li>权重初始化<ul>\n<li>如果是线性激活函数，使用 1/\\sqrt{n};</li>\n<li>如果是ReLU，则使用\\sqrt{2/n}</li>\n</ul>\n</li>\n<li>Batch Normalization</li>\n<li>正则化</li>\n</ul>\n</li>\n<li>刷了224第二节课，认为看视频的效率有点低，计划这系列课程只看7和15节，其余知识通过读书获取</li>\n<li><strong>明日计划</strong><ul>\n<li>两个小时构思毕设，做相应的准备，制定计划</li>\n<li>《入门》下一章内容</li>\n<li>关于树部分，复杂度相关知识的归纳整理</li>\n<li>Leetcode关于树的习题</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"126\">12-6</span></strong></p>\n<ul>\n<li>复习cnn（<a href=\"https://junzx.github.io/2018/12/06/cnn/\" target=\"_blank\" rel=\"noopener\">textCNN学习笔记</a>）</li>\n<li>二叉树代价分析<ul>\n<li>深度搜索<ul>\n<li>时间复杂度：O(n)，每个节点访问一次</li>\n<li>空间复杂度：最好O(logn)，最坏O(n)，与栈的深度即树的高度有关</li>\n</ul>\n</li>\n<li>层次搜索<ul>\n<li>时间复杂度：O(n)</li>\n<li>空间复杂度：最好O(1)，最坏O(n)，与树的最大宽度有关</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"129\">12-9</span></strong></p>\n<ul>\n<li>祝亲人生日快乐</li>\n<li>下周计划<ul>\n<li>完成未完成的内容</li>\n<li>《TF实战》</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1210\">12-10</span></strong></p>\n<ul>\n<li>入门tf： <a href=\"https://junzx.github.io/2018/12/10/learning-tensorflow-linear-regression/\" target=\"_blank\" rel=\"noopener\">TF入门——使用线性回归进行实验</a></li>\n<li>准备面试相关的内容</li>\n<li>确定近期预算</li>\n</ul>\n<hr>\n<p><strong><span id=\"1211\">12-11</span></strong></p>\n<ul>\n<li>去便利蜂面试，面试官人很nice，但是很遗憾结果还是不理想，发现了一些问题如下<ol>\n<li>基础算法不够牢固，一些排序复杂度的弄不是很清楚</li>\n<li>项目中算法类的不够，工程类的较多</li>\n<li>现场在纸上手写代码的效率不行，思路很受干扰，说明思路不够清晰，代码还是不够熟练</li>\n</ol>\n</li>\n<li>制定突破计划</li>\n</ul>\n<hr>\n<p><strong><span id=\"1212\">12-12</span></strong></p>\n<ul>\n<li>着手撸排序和查找的基础算法</li>\n<li><a href=\"https://github.com/huggingface/neuralcoref\" target=\"_blank\" rel=\"noopener\">新任务</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"1213\">12-13</span></strong></p>\n<ul>\n<li><p>spacy相关</p>\n<ul>\n<li><a href=\"https://juejin.im/post/5971a4b9f265da6c42353332\" target=\"_blank\" rel=\"noopener\">https://juejin.im/post/5971a4b9f265da6c42353332</a></li>\n<li><a href=\"http://www.52nlp.cn/tag/spacy\" target=\"_blank\" rel=\"noopener\">http://www.52nlp.cn/tag/spacy</a></li>\n<li>遇到一些问题，后来解决了，必须使用2.0.12版本的spacy</li>\n<li><p>示例代码：</p>\n<pre><code>In [1]: import spacy\n\nIn [2]: spacy.__version__\nOut[2]: &apos;2.0.12&apos;\n\nIn [3]: nlp = spacy.load(&apos;en_coref_sm&apos;)\n\nIn [4]: doc = nlp(u&apos;My sister has a dog. She loves him&apos;)\n\nIn [5]: doc._.has_coref\nOut[5]: True\n\nIn [6]: doc._.coref_clusters\nOut[6]: [My sister: [My sister, She], a dog: [a dog, him]]\n</code></pre></li>\n</ul>\n</li>\n<li><p>搞定三个插入排序算法（<a href=\"https://junzx.github.io/2018/12/13/algorithm-learning-insert-sort/\" target=\"_blank\" rel=\"noopener\">插入排序</a>）</p>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1217\">12-17</span></strong></p>\n<ul>\n<li><p>上周总结</p>\n<ul>\n<li>面试bianlifeng，发现了自己的不足</li>\n<li>完成了三种插入排序的算法</li>\n<li>动手学习tf</li>\n<li>接到了新任务：<a href=\"https://github.com/huggingface/neuralcoref\" target=\"_blank\" rel=\"noopener\">新任务</a></li>\n</ul>\n</li>\n<li><p>本周安排</p>\n<ul>\n<li>剩余的排序查找算法</li>\n<li><strong>毕设提纲（必须在周三之前出来）</strong></li>\n<li>刷题</li>\n<li>保持每天扇贝的单词</li>\n</ul>\n</li>\n<li><p>时间安排</p>\n<ul>\n<li>7：30 闹钟</li>\n<li>7：45 起床</li>\n<li>8：00 吃早饭</li>\n<li>9：30 之前 扇贝当日内容</li>\n<li>9：30~11：00 上午工作</li>\n<li>13：00 下午工作开始</li>\n<li>17：30 晚饭</li>\n<li>19：00 晚上工作</li>\n<li>23：00 上床睡觉</li>\n</ul>\n</li>\n</ul>\n<hr>\n<p><strong><span id=\"1218\">12-18</span></strong></p>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>完成扇贝单词</li>\n<li>列出了毕设提纲</li>\n<li>完成选择排序：<a href=\"https://junzx.github.io/2018/12/18/algorithm-learning-select-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——选择排序</a></li>\n</ul>\n<p><strong>明日计划</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>在屋内适当活动</li>\n<li>与相机和背包的客服沟通</li>\n<li>完成选择排序的笔记</li>\n<li>完成交换排序</li>\n</ul>\n<hr>\n<p><strong><span id=\"1219\">12-19</span></strong></p>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>跟客服扯皮 </li>\n<li>完善选择排序</li>\n</ul>\n<p><strong>明日计划</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>剩余的所有排序（快排，归并）</li>\n<li>毕设内容</li>\n<li>准备面试</li>\n</ul>\n<hr>\n<p><strong><span id=\"1220\">12-20</span></strong></p>\n<p><strong>积累</strong></p>\n<ol>\n<li>The choice of your career path is <strong>critical</strong> to your future. 职业道路的选择对你的未来至关重要。</li>\n<li>The government is responsible for the imporvement of the transportation and public <strong>utilities</strong>. （政府负责改善交通与公共措施） </li>\n<li>The average income is high, though many people earn just a <strong>fraction</strong> of that average.（平均收入很高，但是很多人赚的只有平均工资的一小部分）</li>\n<li>world <strong>renowned</strong>（举世闻名）</li>\n<li>A <strong>substantial</strong> number of people supported the reform policy.（相当多的人支持这一改革政策）</li>\n<li>Increased competition makes it <strong>essential</strong> for the business to innovate.（日益激烈的竞争使得企业必须创新）</li>\n</ol>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>完成交换排序相关<a href=\"https://junzx.github.io/2018/12/20/algorithm-learning-swap-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——交换排序</a></li>\n<li>完成归并排序相关<a href=\"https://junzx.github.io/2018/12/20/algorithm-learning-merge-sort/\" target=\"_blank\" rel=\"noopener\">算法入门——归并排序</a></li>\n</ul>\n<p><strong>明日计划</strong></p>\n<ul>\n<li>书上关于查找的部分</li>\n<li>毕设内容</li>\n<li>整理简历&amp;看面经&amp;问经验</li>\n</ul>\n<hr>\n<p><strong><span id=\"1221\">12-21</span></strong><br><strong>积累</strong></p>\n<ol>\n<li>an area of <strong>outstanding</strong> natural beauty.(自然风景极美的地区/优秀的、突出的)</li>\n</ol>\n<p><strong>今日成果</strong></p>\n<ul>\n<li>扇贝单词</li>\n<li>排序总结</li>\n<li>过以前的论文，找到对毕设可能有用的</li>\n</ul>\n<p><img src=\"/2018/11/29/2018-work-schedule/1.JPG\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>算法</th>\n<th>稳定性</th>\n<th>时间复杂度</th>\n<th>空间复杂度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>直接插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>折半插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>希尔排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>冒泡排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>快速排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n)</td>\n</tr>\n<tr>\n<td>简单选择排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>堆排序</td>\n<td>不稳定</td>\n<td>O(nlog2n)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>归并排序</td>\n<td>稳定</td>\n<td>O(nlog2n)</td>\n<td>O(n)</td>\n</tr>\n<tr>\n<td>基数排序</td>\n<td>稳定</td>\n<td>O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r)</td>\n<td>O(r) r个队列</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<p><strong><span id=\"1226\">12-26</span></strong></p>\n<p><strong>本周计划</strong></p>\n<ul>\n<li>基础算法</li>\n<li>整理GitHub代码以及收藏的内容</li>\n<li>TF</li>\n<li>Numpy——参考：<a href=\"https://www.numpy.org.cn/\" target=\"_blank\" rel=\"noopener\">NumPy 中文文档</a></li>\n<li>拓展阅读——<a href=\"http://www.ruiyan.me/pubs/tutorial-emnlp18.pdf\" target=\"_blank\" rel=\"noopener\">自动对话</a></li>\n<li>BERT相关内容——<a href=\"https://zhuanlan.zhihu.com/p/50717786\" target=\"_blank\" rel=\"noopener\">BERT相关论文、文章和代码资源汇总-知乎</a></li>\n<li><a href=\"https://github.com/DeqianBai/Your-first-machine-learning-Project---End-to-End-in-Python\" target=\"_blank\" rel=\"noopener\">端到端的机器学习项目</a></li>\n<li>单词</li>\n</ul>\n<hr>\n<p><strong><span id=\"1227\">12-27</span></strong></p>\n<p><strong>今日安排</strong></p>\n<ul>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> Numpy入门</li>\n<li style=\"list-style: none\"><input type=\"checkbox\"> 单词</li>\n<li style=\"list-style: none\"><input type=\"checkbox\" checked> 整理GitHub stars</li>\n</ul>\n<p><strong>积累</strong></p>\n<ul>\n<li><a href=\"https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%95%B0%E7%BB%84\" target=\"_blank\" rel=\"noopener\">创建一个数组</a></li>\n<li><a href=\"https://www.numpy.org.cn/article/basics/an_introduction_to_scientific_python_numpy.html#%E5%A4%9A%E7%BB%B4%E6%95%B0%E7%BB%84%E5%88%87%E7%89%87\" target=\"_blank\" rel=\"noopener\">多维数组切片</a></li>\n</ul>\n<pre><code>def quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\n    print quicksort([3,6,8,10,1,2,1])\n</code></pre><ul>\n<li><a href=\"https://www.numpy.org.cn/article/advanced/numpy_exercises_for_data_analysis.html\" target=\"_blank\" rel=\"noopener\">NumPy数据分析练习</a></li>\n<li><p>写了一个小爬虫，用来爬去一些彩票数据，主要是学习beautifulsoup（不是认真的</p>\n<pre><code>from bs4 import BeautifulSoup\nfrom time import sleep\nimport urllib\nimport gzip\nimport random\n\ndef get_url_page(url_):\n    rsp = urllib.request.urlopen(url_)\n    tmp = gzip.decompress(rsp.read()).decode(&apos;gbk&apos;)\n    return tmp\n\ndef main():\n    main_url = &apos;http://kaijiang.500.com/shtml/dlt/%s.shtml&apos;\n    dic_res = {}\n    for idx in range(7001, 7005):\n        idx = str(idx)\n        if len(idx) == 4:\n            idx = &apos;0&apos; + idx\n        url_ = main_url % idx\n        html_file = get_url_page(url_)\n        soup = BeautifulSoup(html_file.encode(&apos;utf-8&apos;), &apos;html.parser&apos;, from_encoding=&apos;utf-8&apos;)\n        red_ball = [i.string for i in soup.find_all(name=&apos;li&apos;, attrs={&apos;class&apos;:&apos;ball_red&apos;})]\n        blue_ball = [i.string for i in soup.find_all(name=&apos;li&apos;, attrs={&apos;class&apos;:&apos;ball_blue&apos;})]\n        tmp_dic = {\n            &apos;red_ball&apos;: red_ball,\n            &apos;blue_ball&apos;: blue_ball\n        }\n        dic_res.setdefault(idx, tmp_dic)\n        sleep(random.uniform(0.3, 3.0))\n    return dic_res\n\nif __name__ == &apos;__main__&apos;:\n    res = main()\n    import pickle\n    with open(&apos;all_ball.data&apos;,&apos;wb&apos;) as hdl:\n        pickle.dump(res, hdl)\n</code></pre></li>\n</ul>\n<p><strong>明日计划</strong><br>搞定CNN、RNN/LSTM</p>\n<hr>\n<p><strong><span id=\"1228\">12-28</span></strong></p>\n<ul>\n<li><a href=\"https://zhuanlan.zhihu.com/p/28196873\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/28196873</a></li>\n<li><a href=\"https://zhuanlan.zhihu.com/p/27087310\" target=\"_blank\" rel=\"noopener\">https://zhuanlan.zhihu.com/p/27087310</a></li>\n<li><a href=\"https://github.com/NELSONZHAO/zhihu/tree/master/anna_lstm\" target=\"_blank\" rel=\"noopener\">https://github.com/NELSONZHAO/zhihu/tree/master/anna_lstm</a></li>\n<li><a href=\"https://github.com/hzy46/Char-RNN-TensorFlow\" target=\"_blank\" rel=\"noopener\">https://github.com/hzy46/Char-RNN-TensorFlow</a></li>\n<li><a href=\"https://gist.github.com/karpathy/d4dee566867f8291f086\" target=\"_blank\" rel=\"noopener\">https://gist.github.com/karpathy/d4dee566867f8291f086</a></li>\n</ul>\n<hr>\n<p><strong><span id=\"1229\">12-29</span></strong></p>\n<p><strong>做事需专注</strong></p>\n<ul>\n<li>同步了一篇论文笔记：<a href=\"https://junzx.github.io/2018/12/29/end2end-coreference-resolution/\" target=\"_blank\" rel=\"noopener\">https://junzx.github.io/2018/12/29/end2end-coreference-resolution/</a></li>\n</ul>\n"},{"title":"算法入门——python实现几个排序","date":"2018-12-12T15:07:49.000Z","top":1,"description":"用python实现直接插入、选择、交换、归并排序","_content":"\n![image](/algorithm-learning-sort/1.JPG)\n\n\n| 算法 | 稳定性 | 时间复杂度 | 空间复杂度 | \n| ---| ---| ---| ---| ---|\n| 直接插入排序 | 稳定 | O(n^2) | O(1) |\n| 折半插入排序 | 稳定 | O(n^2) | O(1) |\n| 希尔排序 | 不稳定 | O(n^2) | O(1) |\n| 冒泡排序 | 稳定 | O(n^2) | O(1) | \n| 快速排序 | 不稳定 | O(n^2) | 最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n) |\n| 简单选择排序 | 不稳定 | O(n^2) | O(1) |\n| 堆排序 | 不稳定 | O(nlog2n) | O(1) |\n| 归并排序 | 稳定 | O(nlog2n) | O(n) |\n| 基数排序 | 稳定 | O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r) | O(r) r个队列 |\n\n---\n\n## **插入排序**\n\n\n### 直接插入排序\n\n\tdef insert_sort(lst):\n        \"\"\"\n        插入排序：\n        对list中从左到右每个元素，找到左侧它最合适的位置。\n        也就是说默认第1个元素是正确位置，第二个元素跟第一个作比较，以此类推\n        \"\"\"\n        # 从第一个元素开始处理\n        for idx in range(len(lst)):\n\n            # 这一步是为了后续挪位置，所以插入排序的思想是每次处理一个数据，找到最合适的位置\n            tmp = lst[idx]\n\n            j = idx\n            # 这个边界条件很重要，每次考虑候选和它前面的数值作比较\n            while j > 0 and tmp < lst[j - 1]:\n                lst[j] = lst[j - 1]\n                j -= 1\n            lst[j] = tmp\n        return lst\n\n![image](/algorithm-learning-sort/insert-sort-1.jpg)\n**图中的部分是一次for的内容**\n\n---\n### 折半插入排序\n\n\tdef bin_sort(lst):\n        \"\"\"\n        折半插入排序\n        \"\"\"\n        for idx in range(len(lst)):\n            if idx == 0:\n                continue\n            i = 0\n            j = idx - 1\n            tmp = lst[idx]\n            while i <= j:\n                mid = int((i + j) / 2)\n                if tmp >= lst[mid]:\n                    i = mid + 1\n                elif tmp < lst[mid]:\n                    j = mid - 1\n\n            for tmp_idx in range(idx, i, -1):\n            # for tmp_idx in range(i, idx):\t# 这句就是错的\n                lst[tmp_idx] = lst[tmp_idx - 1]\n            lst[i] = tmp\n        return lst\n\n![image](/algorithm-learning-sort/insert-sort-2.jpg)\n**图中的部分是一次for的内容**\n\n---\n### 希尔排序\n\tdef shell_sort(lst):\n        \"\"\"\n        希尔排序\n        \"\"\"\n        gap = len(lst)\n        while gap >= 1:\n            gap = int(gap / 2)\n            for j in range(gap, len(lst)):\n                i = j\n                while i - gap >= 0: # 注意边界条件\n                    if lst[i] < lst[i - gap]:\n                        lst[i], lst[i - gap] = lst[i - gap], lst[i]\n                    else:\n                        break\n        return lst\n\n### 参考：\n\n- [Python希尔排序算法-csdn](https://blog.csdn.net/u014745194/article/details/72783357)\n- [白话经典算法系列之三 希尔排序的实现-csdn-c语言](https://blog.csdn.net/MoreWindows/article/details/6668714)\n\n\n\n## **选择排序**\n- 直接选择排序\n- 堆排序\n\n### 直接选择排序\n\n\t# 大概是世界上最简单的直接选择排序了\n    def select_sort(self, lst):\n        \"\"\"\n        直接选择排序\n        \"\"\"\n        for i in range(len(lst)):\n            for j in range(i + 1, len(lst)):\n                if lst[j] < lst[i]:\n                    lst[i], lst[j] = lst[j], lst[i]\n        return lst\n![image](/algorithm-learning-sort/select-sort-1.jpg)\n\n---\n\n### 堆排序\n\n\tdef __adjust_heap(self, lst, idx, length):\n        lchild = 2 * idx + 1\n        rchild = 2 * idx + 2\n        max_idx = idx\n        if idx < length:\n            if lchild < length and lst[lchild] > lst[max_idx]:\n                max_idx = lchild\n            if rchild < length and lst[rchild] > lst[max_idx]:\n                max_idx = rchild\n            if max_idx != idx:\n                lst[max_idx], lst[idx] = lst[idx], lst[max_idx]\n                self.__adjust_heap(lst, idx, length)\n\n    def __build_heap(self, lst, length):\n        for idx in range(0, int(length / 2))[::-1]:\n            self.__adjust_heap(lst, idx, length)\n\n    def heap_sort(self, lst):\n        length = len(lst)\n        self.__build_heap(lst, length)\n        for idx in range(0, length)[::-1]:\n            lst[0], lst[idx] = lst[idx], lst[0]\n            self.__adjust_heap(lst, 0, idx)# todo\n        return lst\n\n\n![image](/algorithm-learning-sort/select-sort-2.jpg)\n\n代码参考：\n\n- [八大排序算法的 Python 实现](http://python.jobbole.com/82270/)\n- [堆排序的Python实现(附详细过程图和讲解)-简书](https://www.jianshu.com/p/d174f1862601)\n- [最大堆（创建、删除、插入和堆排序）-简书](https://www.jianshu.com/p/21bef3fc3030)\n- [图解排序算法(三)之堆排序-cnblogs](https://www.cnblogs.com/chengxiao/p/6129630.html)\n- [一个油管上的视频讲解，强推](https://www.youtube.com/watch?v=MtQL_ll5KhQ)\n\n\n\n## **交换排序**\n\n- 冒泡排序\n- 快速排序\n\n### 冒泡排序\n    def bubble_sort(self, lst):\n        for i in range(0, len(lst)):\n            for j in range(i, len(lst)):\n                if lst[i] > lst[j]:\n                    lst[i], lst[j] = lst[j], lst[i]\n        return lst\n\n---\n\n### 快速排序\n\tdef quick_sort(self, lst, low, high):\n        if low < high:\n            mid = self.__Partitions(lst, low, high)\n            self.quick_sort(lst, low, mid - 1)\n            self.quick_sort(lst, mid + 1, high)\n        return lst\n\n    def __Partitions(self, lst, low, high):\n        i = low\n        j = high\n        key = lst[low]\n        while i < j:\n            while lst[i] <= key:\n                i += 1\n            while lst[j] > key:\n                j -= 1\n            if i < j:\n                lst[i], lst[j] = lst[j], lst[i]\n\n        lst[low] = lst[j]\n        lst[j] = key\n        return j\n\n\n![image](/algorithm-learning-sort/swap-sort-1.jpg)\n\n### 参考\n- [快速排序 Python实现-cnblogs](https://www.cnblogs.com/kunpengv5/p/7833361.html)\n- [快速排序的四种python实现-csdn](https://blog.csdn.net/razor87/article/details/71155518)\n- [快速排序-cnblogs](https://www.cnblogs.com/foreverking/articles/2234225.html)\n\n\n\n\n## **归并排序**\n\n归并排序分为自然归并和二路归并，其中二路归并是将数组划分为较均匀的，自然归并则不一定要等长。关于自然归并可以参考：[自然合并排序算法-csdn](https://blog.csdn.net/u014034497/article/details/45716423)和[自然归并排序算法时间复杂度分析-cnblogs](https://www.cnblogs.com/rixiang/p/6099755.html)。关于二路归并主要参考这一篇：[python二路归并排序实现法-csdn](https://blog.csdn.net/hqzxsc2006/article/details/47702127)\n\n引用归并过程的描述如下：\n> 比较a[i]和a[j]的大小，若a[i]≤a[j]，则将第一个有序表中的元素a[i]复制到r[k]中，并令i和k分别加上1；否则将第二个有序表中的元素a[j]复制到r[k]中，并令j和k分别加上1，如此循环下去，直到其中一个有序表取完，然后再将另一个有序表中剩余的元素复制到r中从下标k到下标t的单元。归并排序的算法我们通常用递归实现，先把待排序区间[s,t]以中点二分，接着把左边子区间排序，再把右边子区间排序，最后把左区间和右区间用一次归并操作合并成有序的区间[s,t]。\n\n### 代码\n\n\tdef merge_sort(self, lst):\n        if len(lst) <= 1:\n            return lst\n        mid = int(len(lst) / 2)\n        left = self.merge_sort(lst[:mid])\n        right = self.merge_sort(lst[mid:])\n        return self.__merge(left=left, right=right)\n\n    def __merge(self, left, right):\n        i, j = 0, 0\n        res = []\n        while i < len(left) and j < len(right):\n            if left[i] < right[j]:\n                res.append(left[i])\n                i += 1\n            elif left[i] > right[j]:\n                res.append(right[j])\n                j += 1\n        res.extend(left[i:])\n        res.extend(right[j:])\n        return res\n\n过程较为简单，主要是考虑每个元素作为初始的一个单位。然后两两一组，进行合并。合并的过程中，对于两个list，维护两个指针，分别挪动即可，而没必要先取最小的list长度进行遍历，使用while即可。\n","source":"_posts/algorithm-learning-sort.md","raw":"---\ntitle: 算法入门——python实现几个排序\ndate: 2018-12-12 23:07:49\ntag:\n\t- Algorithm \n\t- python\ntop: 1\ndescription: 用python实现直接插入、选择、交换、归并排序\n---\n\n![image](/algorithm-learning-sort/1.JPG)\n\n\n| 算法 | 稳定性 | 时间复杂度 | 空间复杂度 | \n| ---| ---| ---| ---| ---|\n| 直接插入排序 | 稳定 | O(n^2) | O(1) |\n| 折半插入排序 | 稳定 | O(n^2) | O(1) |\n| 希尔排序 | 不稳定 | O(n^2) | O(1) |\n| 冒泡排序 | 稳定 | O(n^2) | O(1) | \n| 快速排序 | 不稳定 | O(n^2) | 最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n) |\n| 简单选择排序 | 不稳定 | O(n^2) | O(1) |\n| 堆排序 | 不稳定 | O(nlog2n) | O(1) |\n| 归并排序 | 稳定 | O(nlog2n) | O(n) |\n| 基数排序 | 稳定 | O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r) | O(r) r个队列 |\n\n---\n\n## **插入排序**\n\n\n### 直接插入排序\n\n\tdef insert_sort(lst):\n        \"\"\"\n        插入排序：\n        对list中从左到右每个元素，找到左侧它最合适的位置。\n        也就是说默认第1个元素是正确位置，第二个元素跟第一个作比较，以此类推\n        \"\"\"\n        # 从第一个元素开始处理\n        for idx in range(len(lst)):\n\n            # 这一步是为了后续挪位置，所以插入排序的思想是每次处理一个数据，找到最合适的位置\n            tmp = lst[idx]\n\n            j = idx\n            # 这个边界条件很重要，每次考虑候选和它前面的数值作比较\n            while j > 0 and tmp < lst[j - 1]:\n                lst[j] = lst[j - 1]\n                j -= 1\n            lst[j] = tmp\n        return lst\n\n![image](/algorithm-learning-sort/insert-sort-1.jpg)\n**图中的部分是一次for的内容**\n\n---\n### 折半插入排序\n\n\tdef bin_sort(lst):\n        \"\"\"\n        折半插入排序\n        \"\"\"\n        for idx in range(len(lst)):\n            if idx == 0:\n                continue\n            i = 0\n            j = idx - 1\n            tmp = lst[idx]\n            while i <= j:\n                mid = int((i + j) / 2)\n                if tmp >= lst[mid]:\n                    i = mid + 1\n                elif tmp < lst[mid]:\n                    j = mid - 1\n\n            for tmp_idx in range(idx, i, -1):\n            # for tmp_idx in range(i, idx):\t# 这句就是错的\n                lst[tmp_idx] = lst[tmp_idx - 1]\n            lst[i] = tmp\n        return lst\n\n![image](/algorithm-learning-sort/insert-sort-2.jpg)\n**图中的部分是一次for的内容**\n\n---\n### 希尔排序\n\tdef shell_sort(lst):\n        \"\"\"\n        希尔排序\n        \"\"\"\n        gap = len(lst)\n        while gap >= 1:\n            gap = int(gap / 2)\n            for j in range(gap, len(lst)):\n                i = j\n                while i - gap >= 0: # 注意边界条件\n                    if lst[i] < lst[i - gap]:\n                        lst[i], lst[i - gap] = lst[i - gap], lst[i]\n                    else:\n                        break\n        return lst\n\n### 参考：\n\n- [Python希尔排序算法-csdn](https://blog.csdn.net/u014745194/article/details/72783357)\n- [白话经典算法系列之三 希尔排序的实现-csdn-c语言](https://blog.csdn.net/MoreWindows/article/details/6668714)\n\n\n\n## **选择排序**\n- 直接选择排序\n- 堆排序\n\n### 直接选择排序\n\n\t# 大概是世界上最简单的直接选择排序了\n    def select_sort(self, lst):\n        \"\"\"\n        直接选择排序\n        \"\"\"\n        for i in range(len(lst)):\n            for j in range(i + 1, len(lst)):\n                if lst[j] < lst[i]:\n                    lst[i], lst[j] = lst[j], lst[i]\n        return lst\n![image](/algorithm-learning-sort/select-sort-1.jpg)\n\n---\n\n### 堆排序\n\n\tdef __adjust_heap(self, lst, idx, length):\n        lchild = 2 * idx + 1\n        rchild = 2 * idx + 2\n        max_idx = idx\n        if idx < length:\n            if lchild < length and lst[lchild] > lst[max_idx]:\n                max_idx = lchild\n            if rchild < length and lst[rchild] > lst[max_idx]:\n                max_idx = rchild\n            if max_idx != idx:\n                lst[max_idx], lst[idx] = lst[idx], lst[max_idx]\n                self.__adjust_heap(lst, idx, length)\n\n    def __build_heap(self, lst, length):\n        for idx in range(0, int(length / 2))[::-1]:\n            self.__adjust_heap(lst, idx, length)\n\n    def heap_sort(self, lst):\n        length = len(lst)\n        self.__build_heap(lst, length)\n        for idx in range(0, length)[::-1]:\n            lst[0], lst[idx] = lst[idx], lst[0]\n            self.__adjust_heap(lst, 0, idx)# todo\n        return lst\n\n\n![image](/algorithm-learning-sort/select-sort-2.jpg)\n\n代码参考：\n\n- [八大排序算法的 Python 实现](http://python.jobbole.com/82270/)\n- [堆排序的Python实现(附详细过程图和讲解)-简书](https://www.jianshu.com/p/d174f1862601)\n- [最大堆（创建、删除、插入和堆排序）-简书](https://www.jianshu.com/p/21bef3fc3030)\n- [图解排序算法(三)之堆排序-cnblogs](https://www.cnblogs.com/chengxiao/p/6129630.html)\n- [一个油管上的视频讲解，强推](https://www.youtube.com/watch?v=MtQL_ll5KhQ)\n\n\n\n## **交换排序**\n\n- 冒泡排序\n- 快速排序\n\n### 冒泡排序\n    def bubble_sort(self, lst):\n        for i in range(0, len(lst)):\n            for j in range(i, len(lst)):\n                if lst[i] > lst[j]:\n                    lst[i], lst[j] = lst[j], lst[i]\n        return lst\n\n---\n\n### 快速排序\n\tdef quick_sort(self, lst, low, high):\n        if low < high:\n            mid = self.__Partitions(lst, low, high)\n            self.quick_sort(lst, low, mid - 1)\n            self.quick_sort(lst, mid + 1, high)\n        return lst\n\n    def __Partitions(self, lst, low, high):\n        i = low\n        j = high\n        key = lst[low]\n        while i < j:\n            while lst[i] <= key:\n                i += 1\n            while lst[j] > key:\n                j -= 1\n            if i < j:\n                lst[i], lst[j] = lst[j], lst[i]\n\n        lst[low] = lst[j]\n        lst[j] = key\n        return j\n\n\n![image](/algorithm-learning-sort/swap-sort-1.jpg)\n\n### 参考\n- [快速排序 Python实现-cnblogs](https://www.cnblogs.com/kunpengv5/p/7833361.html)\n- [快速排序的四种python实现-csdn](https://blog.csdn.net/razor87/article/details/71155518)\n- [快速排序-cnblogs](https://www.cnblogs.com/foreverking/articles/2234225.html)\n\n\n\n\n## **归并排序**\n\n归并排序分为自然归并和二路归并，其中二路归并是将数组划分为较均匀的，自然归并则不一定要等长。关于自然归并可以参考：[自然合并排序算法-csdn](https://blog.csdn.net/u014034497/article/details/45716423)和[自然归并排序算法时间复杂度分析-cnblogs](https://www.cnblogs.com/rixiang/p/6099755.html)。关于二路归并主要参考这一篇：[python二路归并排序实现法-csdn](https://blog.csdn.net/hqzxsc2006/article/details/47702127)\n\n引用归并过程的描述如下：\n> 比较a[i]和a[j]的大小，若a[i]≤a[j]，则将第一个有序表中的元素a[i]复制到r[k]中，并令i和k分别加上1；否则将第二个有序表中的元素a[j]复制到r[k]中，并令j和k分别加上1，如此循环下去，直到其中一个有序表取完，然后再将另一个有序表中剩余的元素复制到r中从下标k到下标t的单元。归并排序的算法我们通常用递归实现，先把待排序区间[s,t]以中点二分，接着把左边子区间排序，再把右边子区间排序，最后把左区间和右区间用一次归并操作合并成有序的区间[s,t]。\n\n### 代码\n\n\tdef merge_sort(self, lst):\n        if len(lst) <= 1:\n            return lst\n        mid = int(len(lst) / 2)\n        left = self.merge_sort(lst[:mid])\n        right = self.merge_sort(lst[mid:])\n        return self.__merge(left=left, right=right)\n\n    def __merge(self, left, right):\n        i, j = 0, 0\n        res = []\n        while i < len(left) and j < len(right):\n            if left[i] < right[j]:\n                res.append(left[i])\n                i += 1\n            elif left[i] > right[j]:\n                res.append(right[j])\n                j += 1\n        res.extend(left[i:])\n        res.extend(right[j:])\n        return res\n\n过程较为简单，主要是考虑每个元素作为初始的一个单位。然后两两一组，进行合并。合并的过程中，对于两个list，维护两个指针，分别挪动即可，而没必要先取最小的list长度进行遍历，使用while即可。\n","slug":"algorithm-learning-sort","published":1,"updated":"2019-01-22T04:59:24.924Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2nm0018scftufbzcgjs","content":"<p><img src=\"/2018/12/12/algorithm-learning-sort/1.JPG\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>算法</th>\n<th>稳定性</th>\n<th>时间复杂度</th>\n<th>空间复杂度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>直接插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>折半插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>希尔排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>冒泡排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>快速排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n)</td>\n</tr>\n<tr>\n<td>简单选择排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>堆排序</td>\n<td>不稳定</td>\n<td>O(nlog2n)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>归并排序</td>\n<td>稳定</td>\n<td>O(nlog2n)</td>\n<td>O(n)</td>\n</tr>\n<tr>\n<td>基数排序</td>\n<td>稳定</td>\n<td>O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r)</td>\n<td>O(r) r个队列</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"插入排序\"><a href=\"#插入排序\" class=\"headerlink\" title=\"插入排序\"></a><strong>插入排序</strong></h2><h3 id=\"直接插入排序\"><a href=\"#直接插入排序\" class=\"headerlink\" title=\"直接插入排序\"></a>直接插入排序</h3><pre><code>def insert_sort(lst):\n    &quot;&quot;&quot;\n    插入排序：\n    对list中从左到右每个元素，找到左侧它最合适的位置。\n    也就是说默认第1个元素是正确位置，第二个元素跟第一个作比较，以此类推\n    &quot;&quot;&quot;\n    # 从第一个元素开始处理\n    for idx in range(len(lst)):\n\n        # 这一步是为了后续挪位置，所以插入排序的思想是每次处理一个数据，找到最合适的位置\n        tmp = lst[idx]\n\n        j = idx\n        # 这个边界条件很重要，每次考虑候选和它前面的数值作比较\n        while j &gt; 0 and tmp &lt; lst[j - 1]:\n            lst[j] = lst[j - 1]\n            j -= 1\n        lst[j] = tmp\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/insert-sort-1.jpg\" alt=\"image\"><br><strong>图中的部分是一次for的内容</strong></p>\n<hr>\n<h3 id=\"折半插入排序\"><a href=\"#折半插入排序\" class=\"headerlink\" title=\"折半插入排序\"></a>折半插入排序</h3><pre><code>def bin_sort(lst):\n    &quot;&quot;&quot;\n    折半插入排序\n    &quot;&quot;&quot;\n    for idx in range(len(lst)):\n        if idx == 0:\n            continue\n        i = 0\n        j = idx - 1\n        tmp = lst[idx]\n        while i &lt;= j:\n            mid = int((i + j) / 2)\n            if tmp &gt;= lst[mid]:\n                i = mid + 1\n            elif tmp &lt; lst[mid]:\n                j = mid - 1\n\n        for tmp_idx in range(idx, i, -1):\n        # for tmp_idx in range(i, idx):    # 这句就是错的\n            lst[tmp_idx] = lst[tmp_idx - 1]\n        lst[i] = tmp\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/insert-sort-2.jpg\" alt=\"image\"><br><strong>图中的部分是一次for的内容</strong></p>\n<hr>\n<h3 id=\"希尔排序\"><a href=\"#希尔排序\" class=\"headerlink\" title=\"希尔排序\"></a>希尔排序</h3><pre><code>def shell_sort(lst):\n    &quot;&quot;&quot;\n    希尔排序\n    &quot;&quot;&quot;\n    gap = len(lst)\n    while gap &gt;= 1:\n        gap = int(gap / 2)\n        for j in range(gap, len(lst)):\n            i = j\n            while i - gap &gt;= 0: # 注意边界条件\n                if lst[i] &lt; lst[i - gap]:\n                    lst[i], lst[i - gap] = lst[i - gap], lst[i]\n                else:\n                    break\n    return lst\n</code></pre><h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><ul>\n<li><a href=\"https://blog.csdn.net/u014745194/article/details/72783357\" target=\"_blank\" rel=\"noopener\">Python希尔排序算法-csdn</a></li>\n<li><a href=\"https://blog.csdn.net/MoreWindows/article/details/6668714\" target=\"_blank\" rel=\"noopener\">白话经典算法系列之三 希尔排序的实现-csdn-c语言</a></li>\n</ul>\n<h2 id=\"选择排序\"><a href=\"#选择排序\" class=\"headerlink\" title=\"选择排序\"></a><strong>选择排序</strong></h2><ul>\n<li>直接选择排序</li>\n<li>堆排序</li>\n</ul>\n<h3 id=\"直接选择排序\"><a href=\"#直接选择排序\" class=\"headerlink\" title=\"直接选择排序\"></a>直接选择排序</h3><pre><code># 大概是世界上最简单的直接选择排序了\ndef select_sort(self, lst):\n    &quot;&quot;&quot;\n    直接选择排序\n    &quot;&quot;&quot;\n    for i in range(len(lst)):\n        for j in range(i + 1, len(lst)):\n            if lst[j] &lt; lst[i]:\n                lst[i], lst[j] = lst[j], lst[i]\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/select-sort-1.jpg\" alt=\"image\"></p>\n<hr>\n<h3 id=\"堆排序\"><a href=\"#堆排序\" class=\"headerlink\" title=\"堆排序\"></a>堆排序</h3><pre><code>def __adjust_heap(self, lst, idx, length):\n    lchild = 2 * idx + 1\n    rchild = 2 * idx + 2\n    max_idx = idx\n    if idx &lt; length:\n        if lchild &lt; length and lst[lchild] &gt; lst[max_idx]:\n            max_idx = lchild\n        if rchild &lt; length and lst[rchild] &gt; lst[max_idx]:\n            max_idx = rchild\n        if max_idx != idx:\n            lst[max_idx], lst[idx] = lst[idx], lst[max_idx]\n            self.__adjust_heap(lst, idx, length)\n\ndef __build_heap(self, lst, length):\n    for idx in range(0, int(length / 2))[::-1]:\n        self.__adjust_heap(lst, idx, length)\n\ndef heap_sort(self, lst):\n    length = len(lst)\n    self.__build_heap(lst, length)\n    for idx in range(0, length)[::-1]:\n        lst[0], lst[idx] = lst[idx], lst[0]\n        self.__adjust_heap(lst, 0, idx)# todo\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/select-sort-2.jpg\" alt=\"image\"></p>\n<p>代码参考：</p>\n<ul>\n<li><a href=\"http://python.jobbole.com/82270/\" target=\"_blank\" rel=\"noopener\">八大排序算法的 Python 实现</a></li>\n<li><a href=\"https://www.jianshu.com/p/d174f1862601\" target=\"_blank\" rel=\"noopener\">堆排序的Python实现(附详细过程图和讲解)-简书</a></li>\n<li><a href=\"https://www.jianshu.com/p/21bef3fc3030\" target=\"_blank\" rel=\"noopener\">最大堆（创建、删除、插入和堆排序）-简书</a></li>\n<li><a href=\"https://www.cnblogs.com/chengxiao/p/6129630.html\" target=\"_blank\" rel=\"noopener\">图解排序算法(三)之堆排序-cnblogs</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=MtQL_ll5KhQ\" target=\"_blank\" rel=\"noopener\">一个油管上的视频讲解，强推</a></li>\n</ul>\n<h2 id=\"交换排序\"><a href=\"#交换排序\" class=\"headerlink\" title=\"交换排序\"></a><strong>交换排序</strong></h2><ul>\n<li>冒泡排序</li>\n<li>快速排序</li>\n</ul>\n<h3 id=\"冒泡排序\"><a href=\"#冒泡排序\" class=\"headerlink\" title=\"冒泡排序\"></a>冒泡排序</h3><pre><code>def bubble_sort(self, lst):\n    for i in range(0, len(lst)):\n        for j in range(i, len(lst)):\n            if lst[i] &gt; lst[j]:\n                lst[i], lst[j] = lst[j], lst[i]\n    return lst\n</code></pre><hr>\n<h3 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h3><pre><code>def quick_sort(self, lst, low, high):\n    if low &lt; high:\n        mid = self.__Partitions(lst, low, high)\n        self.quick_sort(lst, low, mid - 1)\n        self.quick_sort(lst, mid + 1, high)\n    return lst\n\ndef __Partitions(self, lst, low, high):\n    i = low\n    j = high\n    key = lst[low]\n    while i &lt; j:\n        while lst[i] &lt;= key:\n            i += 1\n        while lst[j] &gt; key:\n            j -= 1\n        if i &lt; j:\n            lst[i], lst[j] = lst[j], lst[i]\n\n    lst[low] = lst[j]\n    lst[j] = key\n    return j\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/swap-sort-1.jpg\" alt=\"image\"></p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><ul>\n<li><a href=\"https://www.cnblogs.com/kunpengv5/p/7833361.html\" target=\"_blank\" rel=\"noopener\">快速排序 Python实现-cnblogs</a></li>\n<li><a href=\"https://blog.csdn.net/razor87/article/details/71155518\" target=\"_blank\" rel=\"noopener\">快速排序的四种python实现-csdn</a></li>\n<li><a href=\"https://www.cnblogs.com/foreverking/articles/2234225.html\" target=\"_blank\" rel=\"noopener\">快速排序-cnblogs</a></li>\n</ul>\n<h2 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a><strong>归并排序</strong></h2><p>归并排序分为自然归并和二路归并，其中二路归并是将数组划分为较均匀的，自然归并则不一定要等长。关于自然归并可以参考：<a href=\"https://blog.csdn.net/u014034497/article/details/45716423\" target=\"_blank\" rel=\"noopener\">自然合并排序算法-csdn</a>和<a href=\"https://www.cnblogs.com/rixiang/p/6099755.html\" target=\"_blank\" rel=\"noopener\">自然归并排序算法时间复杂度分析-cnblogs</a>。关于二路归并主要参考这一篇：<a href=\"https://blog.csdn.net/hqzxsc2006/article/details/47702127\" target=\"_blank\" rel=\"noopener\">python二路归并排序实现法-csdn</a></p>\n<p>引用归并过程的描述如下：</p>\n<blockquote>\n<p>比较a[i]和a[j]的大小，若a[i]≤a[j]，则将第一个有序表中的元素a[i]复制到r[k]中，并令i和k分别加上1；否则将第二个有序表中的元素a[j]复制到r[k]中，并令j和k分别加上1，如此循环下去，直到其中一个有序表取完，然后再将另一个有序表中剩余的元素复制到r中从下标k到下标t的单元。归并排序的算法我们通常用递归实现，先把待排序区间[s,t]以中点二分，接着把左边子区间排序，再把右边子区间排序，最后把左区间和右区间用一次归并操作合并成有序的区间[s,t]。</p>\n</blockquote>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>def merge_sort(self, lst):\n    if len(lst) &lt;= 1:\n        return lst\n    mid = int(len(lst) / 2)\n    left = self.merge_sort(lst[:mid])\n    right = self.merge_sort(lst[mid:])\n    return self.__merge(left=left, right=right)\n\ndef __merge(self, left, right):\n    i, j = 0, 0\n    res = []\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt; right[j]:\n            res.append(left[i])\n            i += 1\n        elif left[i] &gt; right[j]:\n            res.append(right[j])\n            j += 1\n    res.extend(left[i:])\n    res.extend(right[j:])\n    return res\n</code></pre><p>过程较为简单，主要是考虑每个元素作为初始的一个单位。然后两两一组，进行合并。合并的过程中，对于两个list，维护两个指针，分别挪动即可，而没必要先取最小的list长度进行遍历，使用while即可。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><img src=\"/2018/12/12/algorithm-learning-sort/1.JPG\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>算法</th>\n<th>稳定性</th>\n<th>时间复杂度</th>\n<th>空间复杂度</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>直接插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>折半插入排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>希尔排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>冒泡排序</td>\n<td>稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>快速排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>最好：O(log2(n+1))，最坏: O(n)，平均： O(log2n)</td>\n</tr>\n<tr>\n<td>简单选择排序</td>\n<td>不稳定</td>\n<td>O(n^2)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>堆排序</td>\n<td>不稳定</td>\n<td>O(nlog2n)</td>\n<td>O(1)</td>\n</tr>\n<tr>\n<td>归并排序</td>\n<td>稳定</td>\n<td>O(nlog2n)</td>\n<td>O(n)</td>\n</tr>\n<tr>\n<td>基数排序</td>\n<td>稳定</td>\n<td>O(d(n+r)) d趟分配和收集，一趟分配需要O(n)，一趟收集需要O(r)</td>\n<td>O(r) r个队列</td>\n</tr>\n</tbody>\n</table>\n<hr>\n<h2 id=\"插入排序\"><a href=\"#插入排序\" class=\"headerlink\" title=\"插入排序\"></a><strong>插入排序</strong></h2><h3 id=\"直接插入排序\"><a href=\"#直接插入排序\" class=\"headerlink\" title=\"直接插入排序\"></a>直接插入排序</h3><pre><code>def insert_sort(lst):\n    &quot;&quot;&quot;\n    插入排序：\n    对list中从左到右每个元素，找到左侧它最合适的位置。\n    也就是说默认第1个元素是正确位置，第二个元素跟第一个作比较，以此类推\n    &quot;&quot;&quot;\n    # 从第一个元素开始处理\n    for idx in range(len(lst)):\n\n        # 这一步是为了后续挪位置，所以插入排序的思想是每次处理一个数据，找到最合适的位置\n        tmp = lst[idx]\n\n        j = idx\n        # 这个边界条件很重要，每次考虑候选和它前面的数值作比较\n        while j &gt; 0 and tmp &lt; lst[j - 1]:\n            lst[j] = lst[j - 1]\n            j -= 1\n        lst[j] = tmp\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/insert-sort-1.jpg\" alt=\"image\"><br><strong>图中的部分是一次for的内容</strong></p>\n<hr>\n<h3 id=\"折半插入排序\"><a href=\"#折半插入排序\" class=\"headerlink\" title=\"折半插入排序\"></a>折半插入排序</h3><pre><code>def bin_sort(lst):\n    &quot;&quot;&quot;\n    折半插入排序\n    &quot;&quot;&quot;\n    for idx in range(len(lst)):\n        if idx == 0:\n            continue\n        i = 0\n        j = idx - 1\n        tmp = lst[idx]\n        while i &lt;= j:\n            mid = int((i + j) / 2)\n            if tmp &gt;= lst[mid]:\n                i = mid + 1\n            elif tmp &lt; lst[mid]:\n                j = mid - 1\n\n        for tmp_idx in range(idx, i, -1):\n        # for tmp_idx in range(i, idx):    # 这句就是错的\n            lst[tmp_idx] = lst[tmp_idx - 1]\n        lst[i] = tmp\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/insert-sort-2.jpg\" alt=\"image\"><br><strong>图中的部分是一次for的内容</strong></p>\n<hr>\n<h3 id=\"希尔排序\"><a href=\"#希尔排序\" class=\"headerlink\" title=\"希尔排序\"></a>希尔排序</h3><pre><code>def shell_sort(lst):\n    &quot;&quot;&quot;\n    希尔排序\n    &quot;&quot;&quot;\n    gap = len(lst)\n    while gap &gt;= 1:\n        gap = int(gap / 2)\n        for j in range(gap, len(lst)):\n            i = j\n            while i - gap &gt;= 0: # 注意边界条件\n                if lst[i] &lt; lst[i - gap]:\n                    lst[i], lst[i - gap] = lst[i - gap], lst[i]\n                else:\n                    break\n    return lst\n</code></pre><h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><ul>\n<li><a href=\"https://blog.csdn.net/u014745194/article/details/72783357\" target=\"_blank\" rel=\"noopener\">Python希尔排序算法-csdn</a></li>\n<li><a href=\"https://blog.csdn.net/MoreWindows/article/details/6668714\" target=\"_blank\" rel=\"noopener\">白话经典算法系列之三 希尔排序的实现-csdn-c语言</a></li>\n</ul>\n<h2 id=\"选择排序\"><a href=\"#选择排序\" class=\"headerlink\" title=\"选择排序\"></a><strong>选择排序</strong></h2><ul>\n<li>直接选择排序</li>\n<li>堆排序</li>\n</ul>\n<h3 id=\"直接选择排序\"><a href=\"#直接选择排序\" class=\"headerlink\" title=\"直接选择排序\"></a>直接选择排序</h3><pre><code># 大概是世界上最简单的直接选择排序了\ndef select_sort(self, lst):\n    &quot;&quot;&quot;\n    直接选择排序\n    &quot;&quot;&quot;\n    for i in range(len(lst)):\n        for j in range(i + 1, len(lst)):\n            if lst[j] &lt; lst[i]:\n                lst[i], lst[j] = lst[j], lst[i]\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/select-sort-1.jpg\" alt=\"image\"></p>\n<hr>\n<h3 id=\"堆排序\"><a href=\"#堆排序\" class=\"headerlink\" title=\"堆排序\"></a>堆排序</h3><pre><code>def __adjust_heap(self, lst, idx, length):\n    lchild = 2 * idx + 1\n    rchild = 2 * idx + 2\n    max_idx = idx\n    if idx &lt; length:\n        if lchild &lt; length and lst[lchild] &gt; lst[max_idx]:\n            max_idx = lchild\n        if rchild &lt; length and lst[rchild] &gt; lst[max_idx]:\n            max_idx = rchild\n        if max_idx != idx:\n            lst[max_idx], lst[idx] = lst[idx], lst[max_idx]\n            self.__adjust_heap(lst, idx, length)\n\ndef __build_heap(self, lst, length):\n    for idx in range(0, int(length / 2))[::-1]:\n        self.__adjust_heap(lst, idx, length)\n\ndef heap_sort(self, lst):\n    length = len(lst)\n    self.__build_heap(lst, length)\n    for idx in range(0, length)[::-1]:\n        lst[0], lst[idx] = lst[idx], lst[0]\n        self.__adjust_heap(lst, 0, idx)# todo\n    return lst\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/select-sort-2.jpg\" alt=\"image\"></p>\n<p>代码参考：</p>\n<ul>\n<li><a href=\"http://python.jobbole.com/82270/\" target=\"_blank\" rel=\"noopener\">八大排序算法的 Python 实现</a></li>\n<li><a href=\"https://www.jianshu.com/p/d174f1862601\" target=\"_blank\" rel=\"noopener\">堆排序的Python实现(附详细过程图和讲解)-简书</a></li>\n<li><a href=\"https://www.jianshu.com/p/21bef3fc3030\" target=\"_blank\" rel=\"noopener\">最大堆（创建、删除、插入和堆排序）-简书</a></li>\n<li><a href=\"https://www.cnblogs.com/chengxiao/p/6129630.html\" target=\"_blank\" rel=\"noopener\">图解排序算法(三)之堆排序-cnblogs</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=MtQL_ll5KhQ\" target=\"_blank\" rel=\"noopener\">一个油管上的视频讲解，强推</a></li>\n</ul>\n<h2 id=\"交换排序\"><a href=\"#交换排序\" class=\"headerlink\" title=\"交换排序\"></a><strong>交换排序</strong></h2><ul>\n<li>冒泡排序</li>\n<li>快速排序</li>\n</ul>\n<h3 id=\"冒泡排序\"><a href=\"#冒泡排序\" class=\"headerlink\" title=\"冒泡排序\"></a>冒泡排序</h3><pre><code>def bubble_sort(self, lst):\n    for i in range(0, len(lst)):\n        for j in range(i, len(lst)):\n            if lst[i] &gt; lst[j]:\n                lst[i], lst[j] = lst[j], lst[i]\n    return lst\n</code></pre><hr>\n<h3 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h3><pre><code>def quick_sort(self, lst, low, high):\n    if low &lt; high:\n        mid = self.__Partitions(lst, low, high)\n        self.quick_sort(lst, low, mid - 1)\n        self.quick_sort(lst, mid + 1, high)\n    return lst\n\ndef __Partitions(self, lst, low, high):\n    i = low\n    j = high\n    key = lst[low]\n    while i &lt; j:\n        while lst[i] &lt;= key:\n            i += 1\n        while lst[j] &gt; key:\n            j -= 1\n        if i &lt; j:\n            lst[i], lst[j] = lst[j], lst[i]\n\n    lst[low] = lst[j]\n    lst[j] = key\n    return j\n</code></pre><p><img src=\"/2018/12/12/algorithm-learning-sort/swap-sort-1.jpg\" alt=\"image\"></p>\n<h3 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h3><ul>\n<li><a href=\"https://www.cnblogs.com/kunpengv5/p/7833361.html\" target=\"_blank\" rel=\"noopener\">快速排序 Python实现-cnblogs</a></li>\n<li><a href=\"https://blog.csdn.net/razor87/article/details/71155518\" target=\"_blank\" rel=\"noopener\">快速排序的四种python实现-csdn</a></li>\n<li><a href=\"https://www.cnblogs.com/foreverking/articles/2234225.html\" target=\"_blank\" rel=\"noopener\">快速排序-cnblogs</a></li>\n</ul>\n<h2 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a><strong>归并排序</strong></h2><p>归并排序分为自然归并和二路归并，其中二路归并是将数组划分为较均匀的，自然归并则不一定要等长。关于自然归并可以参考：<a href=\"https://blog.csdn.net/u014034497/article/details/45716423\" target=\"_blank\" rel=\"noopener\">自然合并排序算法-csdn</a>和<a href=\"https://www.cnblogs.com/rixiang/p/6099755.html\" target=\"_blank\" rel=\"noopener\">自然归并排序算法时间复杂度分析-cnblogs</a>。关于二路归并主要参考这一篇：<a href=\"https://blog.csdn.net/hqzxsc2006/article/details/47702127\" target=\"_blank\" rel=\"noopener\">python二路归并排序实现法-csdn</a></p>\n<p>引用归并过程的描述如下：</p>\n<blockquote>\n<p>比较a[i]和a[j]的大小，若a[i]≤a[j]，则将第一个有序表中的元素a[i]复制到r[k]中，并令i和k分别加上1；否则将第二个有序表中的元素a[j]复制到r[k]中，并令j和k分别加上1，如此循环下去，直到其中一个有序表取完，然后再将另一个有序表中剩余的元素复制到r中从下标k到下标t的单元。归并排序的算法我们通常用递归实现，先把待排序区间[s,t]以中点二分，接着把左边子区间排序，再把右边子区间排序，最后把左区间和右区间用一次归并操作合并成有序的区间[s,t]。</p>\n</blockquote>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><pre><code>def merge_sort(self, lst):\n    if len(lst) &lt;= 1:\n        return lst\n    mid = int(len(lst) / 2)\n    left = self.merge_sort(lst[:mid])\n    right = self.merge_sort(lst[mid:])\n    return self.__merge(left=left, right=right)\n\ndef __merge(self, left, right):\n    i, j = 0, 0\n    res = []\n    while i &lt; len(left) and j &lt; len(right):\n        if left[i] &lt; right[j]:\n            res.append(left[i])\n            i += 1\n        elif left[i] &gt; right[j]:\n            res.append(right[j])\n            j += 1\n    res.extend(left[i:])\n    res.extend(right[j:])\n    return res\n</code></pre><p>过程较为简单，主要是考虑每个元素作为初始的一个单位。然后两两一组，进行合并。合并的过程中，对于两个list，维护两个指针，分别挪动即可，而没必要先取最小的list长度进行遍历，使用while即可。</p>\n"},{"title":"textCNN 学习笔记","date":"2018-12-05T16:00:00.000Z","top":1,"description":"学习textCNN的随笔","_content":"\n### 参考资料：\n\n- [cnn-text-classification-tf-github](https://github.com/dennybritz/cnn-text-classification-tf)\n- [Implementing a CNN for Text Classification in TensorFlow](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)\n\n### 卷积的过程\n\n根据网上很流行的图片来说：\n![image](/cnn/3-4.png)\n\n假设我们有这么一句话**“I like this movie very much!”**包含标点符号一共有7个词语，如果对于每个词语我们使用5维的向量进行表示，那么这句话可以表示为7x5的矩阵，所以此矩阵我们作为输入。然后我们可以使用不同尺寸的卷积核来进行，比如图里使用了尺寸分别为2，3，4来做，每种又包含两种（这里猜测是指通道数，比如图像有RGB所以会有三个）。故使用不同的卷积核会产生2x3=6个向量，然后我们可以选择最大池化或者平均池化的方法得到一个一维向量，再将6个拼凑起来。得到的这个向量（图中五颜六色的）我们可以理解为从不同的角度对文本提取出的特征，然后接全连接&softmax层得到最终的分类的结果。\n\n因此程序的思路就是首先定义cnn的网络结构，确定各个超参数，先构建词典从而对文本进行向量化，然后定义输入输出的接口，根据卷积核的尺寸和通道数定义卷积层，使用最大池化及ReLU激活。将数据划分训练集和验证集后，使用训练集构建batch iters进行训练。\n\n### 数据样式\n\n![image](/cnn/data.JPG)\n\n\n---\n\n### 代码\n\n### train.py\n\n\tdef preprocess():\n\t    \"\"\"\n\t    切分数据\n\t    \"\"\"\n\t    x_, y_ = data_helper.load_data_and_labels(tuple_datas)\n\t\n\t    # 构建词典，取所有的文本中最大的长度作为最大长度\n\t    max_document_length = max([int(len(x.split(\" \")))for x in x_])  # 19\n\t\n\t    # 根据最大长度初始化字典\n\t    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n\t\n\t    # 将文本填充到字典中，下面一行等于注释的那两行，其中fit_transform()方法会返回一个迭代器\n\t    # fit_transform() 后，vocab_processor的vocabulary_包含几个重要的属性：_freq(即每个字的出现频率), _mapping(每个字对应的id)\n\t    x = np.array(list(vocab_processor.fit_transform(x_)))\n\t    # t = vocab_processor.fit_transform(x_)\n\t    # x = np.array(list(t))\n\t\n\t    # 取随机种子并打乱数据\n\t    np.random.seed(100)\n\t    shuffle_indices = np.random.permutation(len(y_))\n\t    x_shuffled = np.array(x)[shuffle_indices]\n\t    y_shuffled = np.array(y_)[shuffle_indices]\n\t\n\t    # 根据验证集的比例确定验证集的index\n\t    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y_)))\n\t\n\t    # 划分训练集和验证集\n\t    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n\t    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n\t\n\t    # 手动删除无用的变量确保不会占用大量内存\n\t    del x, y_, x_shuffled, y_shuffled\n\t    return (x_train, y_train, x_dev, y_dev, vocab_processor)\n\nfit_transform后的vocab_processor如下图所示：\n![image](/cnn/vocab.JPG)\n\n\n定义训练过程\n\t\n\tdef train(x_train, y_train, x_dev, y_dev, vocab_processor):\n\t    # 创建图以及会话\n\t    with tf.Graph().as_default():\n\t        session_conf = tf.ConfigProto(\n\t            allow_soft_placement = FLAGS.allow_soft_placement, # 是否打印设备日志\n\t            log_device_placement = FLAGS.log_device_placement, # 若指定的设备不存在，是否允许自动分配设备\n\t        )\n\t        sess = tf.Session(config=session_conf)\n\t\n\t        with sess.as_default():\n\t            cnn_para = (\n\t                x_train.shape[1],\n\t                y_train.shape[1],   # num_classes, 2\n\t                len(vocab_processor.vocabulary_), # vocab_size\n\t                FLAGS.embedding_dim,    # emb size\n\t                list(map(int, FLAGS.filter_sizes.split(','))),\n\t                FLAGS.num_filters,  # 128\n\t                FLAGS.l2_reg_lambda,\n\t                vocab_processor\n\t            )\n\t            cnn = CNN(cnn_para)\n\t\n\t        global_step = tf.Variable(0, name = \"global_step\", trainable = False)   # 这里trainable是false\n\t\n\t        # 定义优化器\n\t        optimizer = tf.train.AdamOptimizer(1e-3)    # 括号内是学习率，使用ADAM优化器\n\t        grads_and_vars = optimizer.compute_gradients(cnn.loss)  # 计算梯度\n\t        train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step) # 应用梯度\n\t\n\t        # 输出的文件夹\n\t        timestamp = str(int(time.time()))\n\t        out_dir = os.path.abspath(os.path.join(os.path.curdir, 'runs', timestamp))\n\t        print('输出文件夹：', out_dir)\n\t\n\t        # checkpoint 目录\n\t        checkpoint_dir = os.path.abspath(os.path.join(out_dir,'checkpoints'))\n\t        checkpoint_prefix = os.path.join(checkpoint_dir,'model')\n\t        if not os.path.exists(checkpoint_dir):\n\t            os.makedirs(checkpoint_dir)\n\t        saver = tf.train.Saver(tf.global_variables(), max_to_keep = FLAGS.num_checkpoints)  # 保存器\n\t        \n\t        # 写入字典\n\t        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n\t\n\t        # 初始化向量\n\t        sess.run(tf.global_variables_initializer())\n\t        \n\t        def train_step(x_batch, y_batch):\n\t            feed_dict = {\n\t                   cnn.input_x: x_batch,    # 对应placeholder\n\t                   cnn.input_y: y_batch,\n\t                   cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n\t                   }\n\t\n\t            temp, step, loss, accuracy = sess.run(\n\t                    [train_op, global_step, cnn.loss, cnn.accuracy],\n\t                    feed_dict\n\t                    )\n\t            time_str = datetime.datetime.now().isoformat()\n\t            print(time_str, step, loss,accuracy)\n\t\n\t        # 创建batch\n\t        batches = data_helper.batch_iter(\n\t                    list(zip(x_train, y_train)),\n\t                    FLAGS.batch_size,\n\t                    FLAGS.num_epochs\n\t                )\n\t        for batch in batches:\n\t            x_batch, y_batch = zip(*batch)\n\t            train_step(x_batch, y_batch)\n\t            current_step = tf.train.global_step(sess, global_step)\n\t            \n\t            if current_step % FLAGS.checkpoint_every == 0:\n\t                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n\t                print('save the checkpoint to:   ',path)\n\t\n调用方法：\n\t\n\tdef main():\n\t    x_train, y_train, x_dev, y_dev, vocab_processor = preprocess()\n\t    train(x_train, y_train, x_dev, y_dev, vocab_processor)\n\t\n\tif __name__ == '__main__':\n\t    main()\n\n\n---\n### data_helper.py\n\n定义产生batch的方法\n\n\tdef batch_iter(data, batch_size, num_epochs, shuffle=True):\n\t    data = np.array(data)\n\t    print(len(data))\n\t    data_size = len(data)\n\t    num_batches_per_epoch = int((len(data) - 1)/batch_size) + 1 # 每个epoch有多少个batch\n\t    for epoch in range(num_epochs):\n\t        if shuffle:\n\t            shuffle_indices = np.random.permutation(np.arange(data_size))\n\t            shuffled_data = data[shuffle_indices]\n\t        else:\n\t            shuffled_data = data\n\t\n\t        for batch_num in range(num_batches_per_epoch):\n\t            start_index = batch_num * batch_size\n\t            end_index = min((batch_num + 1) * batch_size, data_size)    # 如果end index超过了最大，则选择最大的那个\n\t            yield shuffled_data[start_index: end_index]\n\n\n\n---\n### CNN_Network.py\n\n\tclass CNN(object):\n\t    def __init__(self, cnn_para):\n\t        \"\"\"\n\t        :param cnn_para:装在CNN网络的参数，是个tuple\n\t        \"\"\"\n\t        print('创建CNN网络中......')\n\t        print('CNN参数设置：')\n\t        print('seq_length:', cnn_para[0])\n\t        print('num_classes', cnn_para[1])\n\t        print('vocab_size:', cnn_para[2])\n\t\n\t        sequence_length = cnn_para[0]\n\t        num_classes = cnn_para[1]\n\t        vocab_size = cnn_para[2]\n\t        embedding_size = cnn_para[3]\n\t        filter_sizes = cnn_para[4]\n\t        num_filters = cnn_para[5]\n\t        l2_reg_lambda = cnn_para[6]\n\t        vocab_processor = cnn_para[7]\n\t\n\t        # 构建输入输出层、dropout\n\t        # 案例中self.input_x 的shape为(?, 19)\n\t        # self.input_y 的shape为(?, 2)\n\t        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name = 'input_x')\n\t        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name = 'input_y')\n\t        self.dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n\t\n\t        # 定义l2 loss\n\t        l2_loss = tf.constant(0.0)\n\t\n\t        # 定义char embedding层\n\t        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n\t            all_char_emb = [np.random.rand(300)]    # 初始化，必须先放进去一个，作为UNK，也就是说这个就是UNK的embedding\n\t            for k, v in vocab_processor.vocabulary_._mapping.items():\n\t                try:\n\t                    char_emb = list(char_emb_moedl[k]) # 这里可以改成别的embedding\n\t                except KeyError:\n\t                    char_emb = list(np.random.rand(300))\n\t                all_char_emb.append(char_emb)\n\t            # 每个字都被用一个300d的数组表示，由于一共有2180个字，因此all_char_emb的shape为(2180, 300)\n\t            all_char_emb = np.array(all_char_emb)\n\t\n\t            # 由于在此处使用embedding的每一维作为特征，因此构建W矩阵存放权重\n\t            self.W = tf.Variable(all_char_emb, name = \"W\", dtype = tf.float32)\n\t\n\t            # 执行下面两句后，self.embedding_chars的shape为(?, 19, 300)\n\t            # self.embedding_chars_expanded的shape为(?, 19, 300, 1)\n\t            self.embedding_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n\t            self.embedding_chars_expanded = tf.expand_dims(self.embedding_chars, -1)    # 添加一维\n\t\n\t            # 初始化变量\n\t            tess = tf.Session()\n\t            tess.run(tf.global_variables_initializer())\n\t\n\t        # 定义卷积&池化层\n\t        pooled_outputs = []\n\t        for i, filter_size in enumerate(filter_sizes):\n\t            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n\t\n\t                # 卷积\n\t                filter_shape = [filter_size, embedding_size, 1, num_filters]\n\t                # filter_shape = [filter_size, 300, 1,128]\n\t                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1, name = \"W\"))\n\t                # W 的shape：[filter_size, 300, 1, 128]\n\t                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n\t                # b 的shape：[128]\n\t\n\t                conv = tf.nn.conv2d(\n\t                    self.embedding_chars_expanded,\n\t                    W,\n\t                    strides = [1,1,1,1],\n\t                    padding = \"VALID\",\n\t                    name = \"conv\"\n\t                )\n\t\n\t                # 激活\n\t                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n\t\n\t                # 池化\n\t                pooled = tf.nn.max_pool(\n\t                    h,\n\t                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n\t                    strides=[1,1,1,1],\n\t                    padding=\"VALID\",\n\t                    name=\"pool\"\n\t                )\n\t                pooled_outputs.append(pooled)\n\t\n\t        # 结合所有的池化特征\n\t        num_filters_total = num_filters * len(filter_sizes)\n\t        # 2个卷积核，3个卷积，因此产生6维的向量 #update:说的不对，这里是128 * 3 = 384\n\t\n\t        self.h_pool = tf.concat(pooled_outputs, 3)  # TODO:what??\n\t        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) # -1表示这个维度不指定，由程序自己计算生成\n\t\n\t        # 添加dropout\n\t        with tf.name_scope(\"dropout\"):\n\t            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\t\n\t        # TODO：定义输出层？ | 最终（未正则化）的分数&预测\n\t        with tf.name_scope(\"output\"):\n\t            W = tf.get_variable(\n\t                \"W\",\n\t                shape=[num_filters_total, num_classes],\n\t                initializer=tf.contrib.layers.xavier_initializer()\n\t            )\n\t            b = tf.Variable(tf.constant(0.1, shape=[num_classes]),name = \"b\")\n\t            l2_loss += tf.nn.l2_loss(W) # 返回的是一个数值\n\t            l2_loss += tf.nn.l2_loss(b)\n\t            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\") # TODO:what's this\n\t            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n\t\n\t        # 计算交叉熵 # TODO:看懂这几个函数的意思\n\t        with tf.name_scope(\"loss\"):\n\t            losses = tf.nn.softmax_cross_entropy_with_logits(\n\t                logits=self.scores,\n\t                labels=self.input_y\n\t            )\n\t            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\t\n\t        # 精确度 # TODO: reduce mean\n\t        with tf.name_scope(\"Accuracy\"):\n\t            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n\t            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n\t\n\t        print('Create CNN network success!')\n\n\n","source":"_posts/cnn.md","raw":"---\ntitle: textCNN 学习笔记\ndate: 2018-12-06\ntags:\n\t- python\n\t- Tensorflow\n\t- NLP\ntop: 1\ndescription: 学习textCNN的随笔\n---\n\n### 参考资料：\n\n- [cnn-text-classification-tf-github](https://github.com/dennybritz/cnn-text-classification-tf)\n- [Implementing a CNN for Text Classification in TensorFlow](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)\n\n### 卷积的过程\n\n根据网上很流行的图片来说：\n![image](/cnn/3-4.png)\n\n假设我们有这么一句话**“I like this movie very much!”**包含标点符号一共有7个词语，如果对于每个词语我们使用5维的向量进行表示，那么这句话可以表示为7x5的矩阵，所以此矩阵我们作为输入。然后我们可以使用不同尺寸的卷积核来进行，比如图里使用了尺寸分别为2，3，4来做，每种又包含两种（这里猜测是指通道数，比如图像有RGB所以会有三个）。故使用不同的卷积核会产生2x3=6个向量，然后我们可以选择最大池化或者平均池化的方法得到一个一维向量，再将6个拼凑起来。得到的这个向量（图中五颜六色的）我们可以理解为从不同的角度对文本提取出的特征，然后接全连接&softmax层得到最终的分类的结果。\n\n因此程序的思路就是首先定义cnn的网络结构，确定各个超参数，先构建词典从而对文本进行向量化，然后定义输入输出的接口，根据卷积核的尺寸和通道数定义卷积层，使用最大池化及ReLU激活。将数据划分训练集和验证集后，使用训练集构建batch iters进行训练。\n\n### 数据样式\n\n![image](/cnn/data.JPG)\n\n\n---\n\n### 代码\n\n### train.py\n\n\tdef preprocess():\n\t    \"\"\"\n\t    切分数据\n\t    \"\"\"\n\t    x_, y_ = data_helper.load_data_and_labels(tuple_datas)\n\t\n\t    # 构建词典，取所有的文本中最大的长度作为最大长度\n\t    max_document_length = max([int(len(x.split(\" \")))for x in x_])  # 19\n\t\n\t    # 根据最大长度初始化字典\n\t    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n\t\n\t    # 将文本填充到字典中，下面一行等于注释的那两行，其中fit_transform()方法会返回一个迭代器\n\t    # fit_transform() 后，vocab_processor的vocabulary_包含几个重要的属性：_freq(即每个字的出现频率), _mapping(每个字对应的id)\n\t    x = np.array(list(vocab_processor.fit_transform(x_)))\n\t    # t = vocab_processor.fit_transform(x_)\n\t    # x = np.array(list(t))\n\t\n\t    # 取随机种子并打乱数据\n\t    np.random.seed(100)\n\t    shuffle_indices = np.random.permutation(len(y_))\n\t    x_shuffled = np.array(x)[shuffle_indices]\n\t    y_shuffled = np.array(y_)[shuffle_indices]\n\t\n\t    # 根据验证集的比例确定验证集的index\n\t    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y_)))\n\t\n\t    # 划分训练集和验证集\n\t    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n\t    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n\t\n\t    # 手动删除无用的变量确保不会占用大量内存\n\t    del x, y_, x_shuffled, y_shuffled\n\t    return (x_train, y_train, x_dev, y_dev, vocab_processor)\n\nfit_transform后的vocab_processor如下图所示：\n![image](/cnn/vocab.JPG)\n\n\n定义训练过程\n\t\n\tdef train(x_train, y_train, x_dev, y_dev, vocab_processor):\n\t    # 创建图以及会话\n\t    with tf.Graph().as_default():\n\t        session_conf = tf.ConfigProto(\n\t            allow_soft_placement = FLAGS.allow_soft_placement, # 是否打印设备日志\n\t            log_device_placement = FLAGS.log_device_placement, # 若指定的设备不存在，是否允许自动分配设备\n\t        )\n\t        sess = tf.Session(config=session_conf)\n\t\n\t        with sess.as_default():\n\t            cnn_para = (\n\t                x_train.shape[1],\n\t                y_train.shape[1],   # num_classes, 2\n\t                len(vocab_processor.vocabulary_), # vocab_size\n\t                FLAGS.embedding_dim,    # emb size\n\t                list(map(int, FLAGS.filter_sizes.split(','))),\n\t                FLAGS.num_filters,  # 128\n\t                FLAGS.l2_reg_lambda,\n\t                vocab_processor\n\t            )\n\t            cnn = CNN(cnn_para)\n\t\n\t        global_step = tf.Variable(0, name = \"global_step\", trainable = False)   # 这里trainable是false\n\t\n\t        # 定义优化器\n\t        optimizer = tf.train.AdamOptimizer(1e-3)    # 括号内是学习率，使用ADAM优化器\n\t        grads_and_vars = optimizer.compute_gradients(cnn.loss)  # 计算梯度\n\t        train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step) # 应用梯度\n\t\n\t        # 输出的文件夹\n\t        timestamp = str(int(time.time()))\n\t        out_dir = os.path.abspath(os.path.join(os.path.curdir, 'runs', timestamp))\n\t        print('输出文件夹：', out_dir)\n\t\n\t        # checkpoint 目录\n\t        checkpoint_dir = os.path.abspath(os.path.join(out_dir,'checkpoints'))\n\t        checkpoint_prefix = os.path.join(checkpoint_dir,'model')\n\t        if not os.path.exists(checkpoint_dir):\n\t            os.makedirs(checkpoint_dir)\n\t        saver = tf.train.Saver(tf.global_variables(), max_to_keep = FLAGS.num_checkpoints)  # 保存器\n\t        \n\t        # 写入字典\n\t        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n\t\n\t        # 初始化向量\n\t        sess.run(tf.global_variables_initializer())\n\t        \n\t        def train_step(x_batch, y_batch):\n\t            feed_dict = {\n\t                   cnn.input_x: x_batch,    # 对应placeholder\n\t                   cnn.input_y: y_batch,\n\t                   cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n\t                   }\n\t\n\t            temp, step, loss, accuracy = sess.run(\n\t                    [train_op, global_step, cnn.loss, cnn.accuracy],\n\t                    feed_dict\n\t                    )\n\t            time_str = datetime.datetime.now().isoformat()\n\t            print(time_str, step, loss,accuracy)\n\t\n\t        # 创建batch\n\t        batches = data_helper.batch_iter(\n\t                    list(zip(x_train, y_train)),\n\t                    FLAGS.batch_size,\n\t                    FLAGS.num_epochs\n\t                )\n\t        for batch in batches:\n\t            x_batch, y_batch = zip(*batch)\n\t            train_step(x_batch, y_batch)\n\t            current_step = tf.train.global_step(sess, global_step)\n\t            \n\t            if current_step % FLAGS.checkpoint_every == 0:\n\t                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n\t                print('save the checkpoint to:   ',path)\n\t\n调用方法：\n\t\n\tdef main():\n\t    x_train, y_train, x_dev, y_dev, vocab_processor = preprocess()\n\t    train(x_train, y_train, x_dev, y_dev, vocab_processor)\n\t\n\tif __name__ == '__main__':\n\t    main()\n\n\n---\n### data_helper.py\n\n定义产生batch的方法\n\n\tdef batch_iter(data, batch_size, num_epochs, shuffle=True):\n\t    data = np.array(data)\n\t    print(len(data))\n\t    data_size = len(data)\n\t    num_batches_per_epoch = int((len(data) - 1)/batch_size) + 1 # 每个epoch有多少个batch\n\t    for epoch in range(num_epochs):\n\t        if shuffle:\n\t            shuffle_indices = np.random.permutation(np.arange(data_size))\n\t            shuffled_data = data[shuffle_indices]\n\t        else:\n\t            shuffled_data = data\n\t\n\t        for batch_num in range(num_batches_per_epoch):\n\t            start_index = batch_num * batch_size\n\t            end_index = min((batch_num + 1) * batch_size, data_size)    # 如果end index超过了最大，则选择最大的那个\n\t            yield shuffled_data[start_index: end_index]\n\n\n\n---\n### CNN_Network.py\n\n\tclass CNN(object):\n\t    def __init__(self, cnn_para):\n\t        \"\"\"\n\t        :param cnn_para:装在CNN网络的参数，是个tuple\n\t        \"\"\"\n\t        print('创建CNN网络中......')\n\t        print('CNN参数设置：')\n\t        print('seq_length:', cnn_para[0])\n\t        print('num_classes', cnn_para[1])\n\t        print('vocab_size:', cnn_para[2])\n\t\n\t        sequence_length = cnn_para[0]\n\t        num_classes = cnn_para[1]\n\t        vocab_size = cnn_para[2]\n\t        embedding_size = cnn_para[3]\n\t        filter_sizes = cnn_para[4]\n\t        num_filters = cnn_para[5]\n\t        l2_reg_lambda = cnn_para[6]\n\t        vocab_processor = cnn_para[7]\n\t\n\t        # 构建输入输出层、dropout\n\t        # 案例中self.input_x 的shape为(?, 19)\n\t        # self.input_y 的shape为(?, 2)\n\t        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name = 'input_x')\n\t        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name = 'input_y')\n\t        self.dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n\t\n\t        # 定义l2 loss\n\t        l2_loss = tf.constant(0.0)\n\t\n\t        # 定义char embedding层\n\t        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n\t            all_char_emb = [np.random.rand(300)]    # 初始化，必须先放进去一个，作为UNK，也就是说这个就是UNK的embedding\n\t            for k, v in vocab_processor.vocabulary_._mapping.items():\n\t                try:\n\t                    char_emb = list(char_emb_moedl[k]) # 这里可以改成别的embedding\n\t                except KeyError:\n\t                    char_emb = list(np.random.rand(300))\n\t                all_char_emb.append(char_emb)\n\t            # 每个字都被用一个300d的数组表示，由于一共有2180个字，因此all_char_emb的shape为(2180, 300)\n\t            all_char_emb = np.array(all_char_emb)\n\t\n\t            # 由于在此处使用embedding的每一维作为特征，因此构建W矩阵存放权重\n\t            self.W = tf.Variable(all_char_emb, name = \"W\", dtype = tf.float32)\n\t\n\t            # 执行下面两句后，self.embedding_chars的shape为(?, 19, 300)\n\t            # self.embedding_chars_expanded的shape为(?, 19, 300, 1)\n\t            self.embedding_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n\t            self.embedding_chars_expanded = tf.expand_dims(self.embedding_chars, -1)    # 添加一维\n\t\n\t            # 初始化变量\n\t            tess = tf.Session()\n\t            tess.run(tf.global_variables_initializer())\n\t\n\t        # 定义卷积&池化层\n\t        pooled_outputs = []\n\t        for i, filter_size in enumerate(filter_sizes):\n\t            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n\t\n\t                # 卷积\n\t                filter_shape = [filter_size, embedding_size, 1, num_filters]\n\t                # filter_shape = [filter_size, 300, 1,128]\n\t                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1, name = \"W\"))\n\t                # W 的shape：[filter_size, 300, 1, 128]\n\t                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n\t                # b 的shape：[128]\n\t\n\t                conv = tf.nn.conv2d(\n\t                    self.embedding_chars_expanded,\n\t                    W,\n\t                    strides = [1,1,1,1],\n\t                    padding = \"VALID\",\n\t                    name = \"conv\"\n\t                )\n\t\n\t                # 激活\n\t                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = \"relu\")\n\t\n\t                # 池化\n\t                pooled = tf.nn.max_pool(\n\t                    h,\n\t                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n\t                    strides=[1,1,1,1],\n\t                    padding=\"VALID\",\n\t                    name=\"pool\"\n\t                )\n\t                pooled_outputs.append(pooled)\n\t\n\t        # 结合所有的池化特征\n\t        num_filters_total = num_filters * len(filter_sizes)\n\t        # 2个卷积核，3个卷积，因此产生6维的向量 #update:说的不对，这里是128 * 3 = 384\n\t\n\t        self.h_pool = tf.concat(pooled_outputs, 3)  # TODO:what??\n\t        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) # -1表示这个维度不指定，由程序自己计算生成\n\t\n\t        # 添加dropout\n\t        with tf.name_scope(\"dropout\"):\n\t            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\t\n\t        # TODO：定义输出层？ | 最终（未正则化）的分数&预测\n\t        with tf.name_scope(\"output\"):\n\t            W = tf.get_variable(\n\t                \"W\",\n\t                shape=[num_filters_total, num_classes],\n\t                initializer=tf.contrib.layers.xavier_initializer()\n\t            )\n\t            b = tf.Variable(tf.constant(0.1, shape=[num_classes]),name = \"b\")\n\t            l2_loss += tf.nn.l2_loss(W) # 返回的是一个数值\n\t            l2_loss += tf.nn.l2_loss(b)\n\t            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\") # TODO:what's this\n\t            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n\t\n\t        # 计算交叉熵 # TODO:看懂这几个函数的意思\n\t        with tf.name_scope(\"loss\"):\n\t            losses = tf.nn.softmax_cross_entropy_with_logits(\n\t                logits=self.scores,\n\t                labels=self.input_y\n\t            )\n\t            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\t\n\t        # 精确度 # TODO: reduce mean\n\t        with tf.name_scope(\"Accuracy\"):\n\t            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n\t            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n\t\n\t        print('Create CNN network success!')\n\n\n","slug":"cnn","published":1,"updated":"2019-01-22T04:59:24.928Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2nn001ascftyuzo6uof","content":"<h3 id=\"参考资料：\"><a href=\"#参考资料：\" class=\"headerlink\" title=\"参考资料：\"></a>参考资料：</h3><ul>\n<li><a href=\"https://github.com/dennybritz/cnn-text-classification-tf\" target=\"_blank\" rel=\"noopener\">cnn-text-classification-tf-github</a></li>\n<li><a href=\"http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\" target=\"_blank\" rel=\"noopener\">Implementing a CNN for Text Classification in TensorFlow</a></li>\n</ul>\n<h3 id=\"卷积的过程\"><a href=\"#卷积的过程\" class=\"headerlink\" title=\"卷积的过程\"></a>卷积的过程</h3><p>根据网上很流行的图片来说：<br><img src=\"/2018/12/06/cnn/3-4.png\" alt=\"image\"></p>\n<p>假设我们有这么一句话<strong>“I like this movie very much!”</strong>包含标点符号一共有7个词语，如果对于每个词语我们使用5维的向量进行表示，那么这句话可以表示为7x5的矩阵，所以此矩阵我们作为输入。然后我们可以使用不同尺寸的卷积核来进行，比如图里使用了尺寸分别为2，3，4来做，每种又包含两种（这里猜测是指通道数，比如图像有RGB所以会有三个）。故使用不同的卷积核会产生2x3=6个向量，然后我们可以选择最大池化或者平均池化的方法得到一个一维向量，再将6个拼凑起来。得到的这个向量（图中五颜六色的）我们可以理解为从不同的角度对文本提取出的特征，然后接全连接&amp;softmax层得到最终的分类的结果。</p>\n<p>因此程序的思路就是首先定义cnn的网络结构，确定各个超参数，先构建词典从而对文本进行向量化，然后定义输入输出的接口，根据卷积核的尺寸和通道数定义卷积层，使用最大池化及ReLU激活。将数据划分训练集和验证集后，使用训练集构建batch iters进行训练。</p>\n<h3 id=\"数据样式\"><a href=\"#数据样式\" class=\"headerlink\" title=\"数据样式\"></a>数据样式</h3><p><img src=\"/2018/12/06/cnn/data.JPG\" alt=\"image\"></p>\n<hr>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><h3 id=\"train-py\"><a href=\"#train-py\" class=\"headerlink\" title=\"train.py\"></a>train.py</h3><pre><code>def preprocess():\n    &quot;&quot;&quot;\n    切分数据\n    &quot;&quot;&quot;\n    x_, y_ = data_helper.load_data_and_labels(tuple_datas)\n\n    # 构建词典，取所有的文本中最大的长度作为最大长度\n    max_document_length = max([int(len(x.split(&quot; &quot;)))for x in x_])  # 19\n\n    # 根据最大长度初始化字典\n    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n\n    # 将文本填充到字典中，下面一行等于注释的那两行，其中fit_transform()方法会返回一个迭代器\n    # fit_transform() 后，vocab_processor的vocabulary_包含几个重要的属性：_freq(即每个字的出现频率), _mapping(每个字对应的id)\n    x = np.array(list(vocab_processor.fit_transform(x_)))\n    # t = vocab_processor.fit_transform(x_)\n    # x = np.array(list(t))\n\n    # 取随机种子并打乱数据\n    np.random.seed(100)\n    shuffle_indices = np.random.permutation(len(y_))\n    x_shuffled = np.array(x)[shuffle_indices]\n    y_shuffled = np.array(y_)[shuffle_indices]\n\n    # 根据验证集的比例确定验证集的index\n    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y_)))\n\n    # 划分训练集和验证集\n    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n\n    # 手动删除无用的变量确保不会占用大量内存\n    del x, y_, x_shuffled, y_shuffled\n    return (x_train, y_train, x_dev, y_dev, vocab_processor)\n</code></pre><p>fit_transform后的vocab_processor如下图所示：<br><img src=\"/2018/12/06/cnn/vocab.JPG\" alt=\"image\"></p>\n<p>定义训练过程</p>\n<pre><code>def train(x_train, y_train, x_dev, y_dev, vocab_processor):\n    # 创建图以及会话\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement = FLAGS.allow_soft_placement, # 是否打印设备日志\n            log_device_placement = FLAGS.log_device_placement, # 若指定的设备不存在，是否允许自动分配设备\n        )\n        sess = tf.Session(config=session_conf)\n\n        with sess.as_default():\n            cnn_para = (\n                x_train.shape[1],\n                y_train.shape[1],   # num_classes, 2\n                len(vocab_processor.vocabulary_), # vocab_size\n                FLAGS.embedding_dim,    # emb size\n                list(map(int, FLAGS.filter_sizes.split(&apos;,&apos;))),\n                FLAGS.num_filters,  # 128\n                FLAGS.l2_reg_lambda,\n                vocab_processor\n            )\n            cnn = CNN(cnn_para)\n\n        global_step = tf.Variable(0, name = &quot;global_step&quot;, trainable = False)   # 这里trainable是false\n\n        # 定义优化器\n        optimizer = tf.train.AdamOptimizer(1e-3)    # 括号内是学习率，使用ADAM优化器\n        grads_and_vars = optimizer.compute_gradients(cnn.loss)  # 计算梯度\n        train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step) # 应用梯度\n\n        # 输出的文件夹\n        timestamp = str(int(time.time()))\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, &apos;runs&apos;, timestamp))\n        print(&apos;输出文件夹：&apos;, out_dir)\n\n        # checkpoint 目录\n        checkpoint_dir = os.path.abspath(os.path.join(out_dir,&apos;checkpoints&apos;))\n        checkpoint_prefix = os.path.join(checkpoint_dir,&apos;model&apos;)\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep = FLAGS.num_checkpoints)  # 保存器\n\n        # 写入字典\n        vocab_processor.save(os.path.join(out_dir, &quot;vocab&quot;))\n\n        # 初始化向量\n        sess.run(tf.global_variables_initializer())\n\n        def train_step(x_batch, y_batch):\n            feed_dict = {\n                   cnn.input_x: x_batch,    # 对应placeholder\n                   cnn.input_y: y_batch,\n                   cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n                   }\n\n            temp, step, loss, accuracy = sess.run(\n                    [train_op, global_step, cnn.loss, cnn.accuracy],\n                    feed_dict\n                    )\n            time_str = datetime.datetime.now().isoformat()\n            print(time_str, step, loss,accuracy)\n\n        # 创建batch\n        batches = data_helper.batch_iter(\n                    list(zip(x_train, y_train)),\n                    FLAGS.batch_size,\n                    FLAGS.num_epochs\n                )\n        for batch in batches:\n            x_batch, y_batch = zip(*batch)\n            train_step(x_batch, y_batch)\n            current_step = tf.train.global_step(sess, global_step)\n\n            if current_step % FLAGS.checkpoint_every == 0:\n                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                print(&apos;save the checkpoint to:   &apos;,path)\n</code></pre><p>调用方法：</p>\n<pre><code>def main():\n    x_train, y_train, x_dev, y_dev, vocab_processor = preprocess()\n    train(x_train, y_train, x_dev, y_dev, vocab_processor)\n\nif __name__ == &apos;__main__&apos;:\n    main()\n</code></pre><hr>\n<h3 id=\"data-helper-py\"><a href=\"#data-helper-py\" class=\"headerlink\" title=\"data_helper.py\"></a>data_helper.py</h3><p>定义产生batch的方法</p>\n<pre><code>def batch_iter(data, batch_size, num_epochs, shuffle=True):\n    data = np.array(data)\n    print(len(data))\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data) - 1)/batch_size) + 1 # 每个epoch有多少个batch\n    for epoch in range(num_epochs):\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)    # 如果end index超过了最大，则选择最大的那个\n            yield shuffled_data[start_index: end_index]\n</code></pre><hr>\n<h3 id=\"CNN-Network-py\"><a href=\"#CNN-Network-py\" class=\"headerlink\" title=\"CNN_Network.py\"></a>CNN_Network.py</h3><pre><code>class CNN(object):\n    def __init__(self, cnn_para):\n        &quot;&quot;&quot;\n        :param cnn_para:装在CNN网络的参数，是个tuple\n        &quot;&quot;&quot;\n        print(&apos;创建CNN网络中......&apos;)\n        print(&apos;CNN参数设置：&apos;)\n        print(&apos;seq_length:&apos;, cnn_para[0])\n        print(&apos;num_classes&apos;, cnn_para[1])\n        print(&apos;vocab_size:&apos;, cnn_para[2])\n\n        sequence_length = cnn_para[0]\n        num_classes = cnn_para[1]\n        vocab_size = cnn_para[2]\n        embedding_size = cnn_para[3]\n        filter_sizes = cnn_para[4]\n        num_filters = cnn_para[5]\n        l2_reg_lambda = cnn_para[6]\n        vocab_processor = cnn_para[7]\n\n        # 构建输入输出层、dropout\n        # 案例中self.input_x 的shape为(?, 19)\n        # self.input_y 的shape为(?, 2)\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name = &apos;input_x&apos;)\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name = &apos;input_y&apos;)\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name = &quot;dropout_keep_prob&quot;)\n\n        # 定义l2 loss\n        l2_loss = tf.constant(0.0)\n\n        # 定义char embedding层\n        with tf.device(&apos;/cpu:0&apos;), tf.name_scope(&apos;embedding&apos;):\n            all_char_emb = [np.random.rand(300)]    # 初始化，必须先放进去一个，作为UNK，也就是说这个就是UNK的embedding\n            for k, v in vocab_processor.vocabulary_._mapping.items():\n                try:\n                    char_emb = list(char_emb_moedl[k]) # 这里可以改成别的embedding\n                except KeyError:\n                    char_emb = list(np.random.rand(300))\n                all_char_emb.append(char_emb)\n            # 每个字都被用一个300d的数组表示，由于一共有2180个字，因此all_char_emb的shape为(2180, 300)\n            all_char_emb = np.array(all_char_emb)\n\n            # 由于在此处使用embedding的每一维作为特征，因此构建W矩阵存放权重\n            self.W = tf.Variable(all_char_emb, name = &quot;W&quot;, dtype = tf.float32)\n\n            # 执行下面两句后，self.embedding_chars的shape为(?, 19, 300)\n            # self.embedding_chars_expanded的shape为(?, 19, 300, 1)\n            self.embedding_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n            self.embedding_chars_expanded = tf.expand_dims(self.embedding_chars, -1)    # 添加一维\n\n            # 初始化变量\n            tess = tf.Session()\n            tess.run(tf.global_variables_initializer())\n\n        # 定义卷积&amp;池化层\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(&quot;conv-maxpool-%s&quot; % filter_size):\n\n                # 卷积\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                # filter_shape = [filter_size, 300, 1,128]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1, name = &quot;W&quot;))\n                # W 的shape：[filter_size, 300, 1, 128]\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=&quot;b&quot;)\n                # b 的shape：[128]\n\n                conv = tf.nn.conv2d(\n                    self.embedding_chars_expanded,\n                    W,\n                    strides = [1,1,1,1],\n                    padding = &quot;VALID&quot;,\n                    name = &quot;conv&quot;\n                )\n\n                # 激活\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = &quot;relu&quot;)\n\n                # 池化\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1,1,1,1],\n                    padding=&quot;VALID&quot;,\n                    name=&quot;pool&quot;\n                )\n                pooled_outputs.append(pooled)\n\n        # 结合所有的池化特征\n        num_filters_total = num_filters * len(filter_sizes)\n        # 2个卷积核，3个卷积，因此产生6维的向量 #update:说的不对，这里是128 * 3 = 384\n\n        self.h_pool = tf.concat(pooled_outputs, 3)  # TODO:what??\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) # -1表示这个维度不指定，由程序自己计算生成\n\n        # 添加dropout\n        with tf.name_scope(&quot;dropout&quot;):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # TODO：定义输出层？ | 最终（未正则化）的分数&amp;预测\n        with tf.name_scope(&quot;output&quot;):\n            W = tf.get_variable(\n                &quot;W&quot;,\n                shape=[num_filters_total, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer()\n            )\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]),name = &quot;b&quot;)\n            l2_loss += tf.nn.l2_loss(W) # 返回的是一个数值\n            l2_loss += tf.nn.l2_loss(b)\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=&quot;scores&quot;) # TODO:what&apos;s this\n            self.predictions = tf.argmax(self.scores, 1, name=&quot;predictions&quot;)\n\n        # 计算交叉熵 # TODO:看懂这几个函数的意思\n        with tf.name_scope(&quot;loss&quot;):\n            losses = tf.nn.softmax_cross_entropy_with_logits(\n                logits=self.scores,\n                labels=self.input_y\n            )\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # 精确度 # TODO: reduce mean\n        with tf.name_scope(&quot;Accuracy&quot;):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, &quot;float&quot;), name=&quot;accuracy&quot;)\n\n        print(&apos;Create CNN network success!&apos;)\n</code></pre>","site":{"data":{}},"excerpt":"","more":"<h3 id=\"参考资料：\"><a href=\"#参考资料：\" class=\"headerlink\" title=\"参考资料：\"></a>参考资料：</h3><ul>\n<li><a href=\"https://github.com/dennybritz/cnn-text-classification-tf\" target=\"_blank\" rel=\"noopener\">cnn-text-classification-tf-github</a></li>\n<li><a href=\"http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\" target=\"_blank\" rel=\"noopener\">Implementing a CNN for Text Classification in TensorFlow</a></li>\n</ul>\n<h3 id=\"卷积的过程\"><a href=\"#卷积的过程\" class=\"headerlink\" title=\"卷积的过程\"></a>卷积的过程</h3><p>根据网上很流行的图片来说：<br><img src=\"/2018/12/06/cnn/3-4.png\" alt=\"image\"></p>\n<p>假设我们有这么一句话<strong>“I like this movie very much!”</strong>包含标点符号一共有7个词语，如果对于每个词语我们使用5维的向量进行表示，那么这句话可以表示为7x5的矩阵，所以此矩阵我们作为输入。然后我们可以使用不同尺寸的卷积核来进行，比如图里使用了尺寸分别为2，3，4来做，每种又包含两种（这里猜测是指通道数，比如图像有RGB所以会有三个）。故使用不同的卷积核会产生2x3=6个向量，然后我们可以选择最大池化或者平均池化的方法得到一个一维向量，再将6个拼凑起来。得到的这个向量（图中五颜六色的）我们可以理解为从不同的角度对文本提取出的特征，然后接全连接&amp;softmax层得到最终的分类的结果。</p>\n<p>因此程序的思路就是首先定义cnn的网络结构，确定各个超参数，先构建词典从而对文本进行向量化，然后定义输入输出的接口，根据卷积核的尺寸和通道数定义卷积层，使用最大池化及ReLU激活。将数据划分训练集和验证集后，使用训练集构建batch iters进行训练。</p>\n<h3 id=\"数据样式\"><a href=\"#数据样式\" class=\"headerlink\" title=\"数据样式\"></a>数据样式</h3><p><img src=\"/2018/12/06/cnn/data.JPG\" alt=\"image\"></p>\n<hr>\n<h3 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h3><h3 id=\"train-py\"><a href=\"#train-py\" class=\"headerlink\" title=\"train.py\"></a>train.py</h3><pre><code>def preprocess():\n    &quot;&quot;&quot;\n    切分数据\n    &quot;&quot;&quot;\n    x_, y_ = data_helper.load_data_and_labels(tuple_datas)\n\n    # 构建词典，取所有的文本中最大的长度作为最大长度\n    max_document_length = max([int(len(x.split(&quot; &quot;)))for x in x_])  # 19\n\n    # 根据最大长度初始化字典\n    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n\n    # 将文本填充到字典中，下面一行等于注释的那两行，其中fit_transform()方法会返回一个迭代器\n    # fit_transform() 后，vocab_processor的vocabulary_包含几个重要的属性：_freq(即每个字的出现频率), _mapping(每个字对应的id)\n    x = np.array(list(vocab_processor.fit_transform(x_)))\n    # t = vocab_processor.fit_transform(x_)\n    # x = np.array(list(t))\n\n    # 取随机种子并打乱数据\n    np.random.seed(100)\n    shuffle_indices = np.random.permutation(len(y_))\n    x_shuffled = np.array(x)[shuffle_indices]\n    y_shuffled = np.array(y_)[shuffle_indices]\n\n    # 根据验证集的比例确定验证集的index\n    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y_)))\n\n    # 划分训练集和验证集\n    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n\n    # 手动删除无用的变量确保不会占用大量内存\n    del x, y_, x_shuffled, y_shuffled\n    return (x_train, y_train, x_dev, y_dev, vocab_processor)\n</code></pre><p>fit_transform后的vocab_processor如下图所示：<br><img src=\"/2018/12/06/cnn/vocab.JPG\" alt=\"image\"></p>\n<p>定义训练过程</p>\n<pre><code>def train(x_train, y_train, x_dev, y_dev, vocab_processor):\n    # 创建图以及会话\n    with tf.Graph().as_default():\n        session_conf = tf.ConfigProto(\n            allow_soft_placement = FLAGS.allow_soft_placement, # 是否打印设备日志\n            log_device_placement = FLAGS.log_device_placement, # 若指定的设备不存在，是否允许自动分配设备\n        )\n        sess = tf.Session(config=session_conf)\n\n        with sess.as_default():\n            cnn_para = (\n                x_train.shape[1],\n                y_train.shape[1],   # num_classes, 2\n                len(vocab_processor.vocabulary_), # vocab_size\n                FLAGS.embedding_dim,    # emb size\n                list(map(int, FLAGS.filter_sizes.split(&apos;,&apos;))),\n                FLAGS.num_filters,  # 128\n                FLAGS.l2_reg_lambda,\n                vocab_processor\n            )\n            cnn = CNN(cnn_para)\n\n        global_step = tf.Variable(0, name = &quot;global_step&quot;, trainable = False)   # 这里trainable是false\n\n        # 定义优化器\n        optimizer = tf.train.AdamOptimizer(1e-3)    # 括号内是学习率，使用ADAM优化器\n        grads_and_vars = optimizer.compute_gradients(cnn.loss)  # 计算梯度\n        train_op = optimizer.apply_gradients(grads_and_vars, global_step = global_step) # 应用梯度\n\n        # 输出的文件夹\n        timestamp = str(int(time.time()))\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, &apos;runs&apos;, timestamp))\n        print(&apos;输出文件夹：&apos;, out_dir)\n\n        # checkpoint 目录\n        checkpoint_dir = os.path.abspath(os.path.join(out_dir,&apos;checkpoints&apos;))\n        checkpoint_prefix = os.path.join(checkpoint_dir,&apos;model&apos;)\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep = FLAGS.num_checkpoints)  # 保存器\n\n        # 写入字典\n        vocab_processor.save(os.path.join(out_dir, &quot;vocab&quot;))\n\n        # 初始化向量\n        sess.run(tf.global_variables_initializer())\n\n        def train_step(x_batch, y_batch):\n            feed_dict = {\n                   cnn.input_x: x_batch,    # 对应placeholder\n                   cnn.input_y: y_batch,\n                   cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n                   }\n\n            temp, step, loss, accuracy = sess.run(\n                    [train_op, global_step, cnn.loss, cnn.accuracy],\n                    feed_dict\n                    )\n            time_str = datetime.datetime.now().isoformat()\n            print(time_str, step, loss,accuracy)\n\n        # 创建batch\n        batches = data_helper.batch_iter(\n                    list(zip(x_train, y_train)),\n                    FLAGS.batch_size,\n                    FLAGS.num_epochs\n                )\n        for batch in batches:\n            x_batch, y_batch = zip(*batch)\n            train_step(x_batch, y_batch)\n            current_step = tf.train.global_step(sess, global_step)\n\n            if current_step % FLAGS.checkpoint_every == 0:\n                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                print(&apos;save the checkpoint to:   &apos;,path)\n</code></pre><p>调用方法：</p>\n<pre><code>def main():\n    x_train, y_train, x_dev, y_dev, vocab_processor = preprocess()\n    train(x_train, y_train, x_dev, y_dev, vocab_processor)\n\nif __name__ == &apos;__main__&apos;:\n    main()\n</code></pre><hr>\n<h3 id=\"data-helper-py\"><a href=\"#data-helper-py\" class=\"headerlink\" title=\"data_helper.py\"></a>data_helper.py</h3><p>定义产生batch的方法</p>\n<pre><code>def batch_iter(data, batch_size, num_epochs, shuffle=True):\n    data = np.array(data)\n    print(len(data))\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data) - 1)/batch_size) + 1 # 每个epoch有多少个batch\n    for epoch in range(num_epochs):\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)    # 如果end index超过了最大，则选择最大的那个\n            yield shuffled_data[start_index: end_index]\n</code></pre><hr>\n<h3 id=\"CNN-Network-py\"><a href=\"#CNN-Network-py\" class=\"headerlink\" title=\"CNN_Network.py\"></a>CNN_Network.py</h3><pre><code>class CNN(object):\n    def __init__(self, cnn_para):\n        &quot;&quot;&quot;\n        :param cnn_para:装在CNN网络的参数，是个tuple\n        &quot;&quot;&quot;\n        print(&apos;创建CNN网络中......&apos;)\n        print(&apos;CNN参数设置：&apos;)\n        print(&apos;seq_length:&apos;, cnn_para[0])\n        print(&apos;num_classes&apos;, cnn_para[1])\n        print(&apos;vocab_size:&apos;, cnn_para[2])\n\n        sequence_length = cnn_para[0]\n        num_classes = cnn_para[1]\n        vocab_size = cnn_para[2]\n        embedding_size = cnn_para[3]\n        filter_sizes = cnn_para[4]\n        num_filters = cnn_para[5]\n        l2_reg_lambda = cnn_para[6]\n        vocab_processor = cnn_para[7]\n\n        # 构建输入输出层、dropout\n        # 案例中self.input_x 的shape为(?, 19)\n        # self.input_y 的shape为(?, 2)\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name = &apos;input_x&apos;)\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name = &apos;input_y&apos;)\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name = &quot;dropout_keep_prob&quot;)\n\n        # 定义l2 loss\n        l2_loss = tf.constant(0.0)\n\n        # 定义char embedding层\n        with tf.device(&apos;/cpu:0&apos;), tf.name_scope(&apos;embedding&apos;):\n            all_char_emb = [np.random.rand(300)]    # 初始化，必须先放进去一个，作为UNK，也就是说这个就是UNK的embedding\n            for k, v in vocab_processor.vocabulary_._mapping.items():\n                try:\n                    char_emb = list(char_emb_moedl[k]) # 这里可以改成别的embedding\n                except KeyError:\n                    char_emb = list(np.random.rand(300))\n                all_char_emb.append(char_emb)\n            # 每个字都被用一个300d的数组表示，由于一共有2180个字，因此all_char_emb的shape为(2180, 300)\n            all_char_emb = np.array(all_char_emb)\n\n            # 由于在此处使用embedding的每一维作为特征，因此构建W矩阵存放权重\n            self.W = tf.Variable(all_char_emb, name = &quot;W&quot;, dtype = tf.float32)\n\n            # 执行下面两句后，self.embedding_chars的shape为(?, 19, 300)\n            # self.embedding_chars_expanded的shape为(?, 19, 300, 1)\n            self.embedding_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n            self.embedding_chars_expanded = tf.expand_dims(self.embedding_chars, -1)    # 添加一维\n\n            # 初始化变量\n            tess = tf.Session()\n            tess.run(tf.global_variables_initializer())\n\n        # 定义卷积&amp;池化层\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(&quot;conv-maxpool-%s&quot; % filter_size):\n\n                # 卷积\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                # filter_shape = [filter_size, 300, 1,128]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1, name = &quot;W&quot;))\n                # W 的shape：[filter_size, 300, 1, 128]\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=&quot;b&quot;)\n                # b 的shape：[128]\n\n                conv = tf.nn.conv2d(\n                    self.embedding_chars_expanded,\n                    W,\n                    strides = [1,1,1,1],\n                    padding = &quot;VALID&quot;,\n                    name = &quot;conv&quot;\n                )\n\n                # 激活\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name = &quot;relu&quot;)\n\n                # 池化\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1,1,1,1],\n                    padding=&quot;VALID&quot;,\n                    name=&quot;pool&quot;\n                )\n                pooled_outputs.append(pooled)\n\n        # 结合所有的池化特征\n        num_filters_total = num_filters * len(filter_sizes)\n        # 2个卷积核，3个卷积，因此产生6维的向量 #update:说的不对，这里是128 * 3 = 384\n\n        self.h_pool = tf.concat(pooled_outputs, 3)  # TODO:what??\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) # -1表示这个维度不指定，由程序自己计算生成\n\n        # 添加dropout\n        with tf.name_scope(&quot;dropout&quot;):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # TODO：定义输出层？ | 最终（未正则化）的分数&amp;预测\n        with tf.name_scope(&quot;output&quot;):\n            W = tf.get_variable(\n                &quot;W&quot;,\n                shape=[num_filters_total, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer()\n            )\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]),name = &quot;b&quot;)\n            l2_loss += tf.nn.l2_loss(W) # 返回的是一个数值\n            l2_loss += tf.nn.l2_loss(b)\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=&quot;scores&quot;) # TODO:what&apos;s this\n            self.predictions = tf.argmax(self.scores, 1, name=&quot;predictions&quot;)\n\n        # 计算交叉熵 # TODO:看懂这几个函数的意思\n        with tf.name_scope(&quot;loss&quot;):\n            losses = tf.nn.softmax_cross_entropy_with_logits(\n                logits=self.scores,\n                labels=self.input_y\n            )\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # 精确度 # TODO: reduce mean\n        with tf.name_scope(&quot;Accuracy&quot;):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, &quot;float&quot;), name=&quot;accuracy&quot;)\n\n        print(&apos;Create CNN network success!&apos;)\n</code></pre>"},{"title":"conll2011数据格式","date":"2017-08-23T16:00:00.000Z","description":"conll2011数据的格式相关内容","top":1,"_content":"\n##### From: https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html\n\n![image](http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/4E3C5A5EB28F402DA9F7FD3ADC16CD81/2000)\n\n#### http://conll.cemantix.org/2011/data.html\n\nRead _conll file format from CoNLL2011. See http://conll.bbn.com/index.php/data.html. \nCoNLL2011 files are in /scr/nlp/data/conll-2011/v0/data/ dev train Contains *_auto_conll files (auto generated) and _gold_conll (hand labelled), default reads _gold_conll \nThere is also /scr/nlp/data/conll-2011/v0/conll.trial which has *.conll files (parse has _ at end) \n\nColumn Type Description \n\n- 1 Document ID  | This is a variation on the document filename | 文档ID\n- 2 Part number |  Some files are divided into multiple parts numbered as 000, 001, 002, ... etc. | 分块ID\n- 3 Word number     | 词语ID\n- 4 Word itself     | 词语本身\n- 5 Part-of-Speech  | 词语的词性\n- 6 Parse bit | This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a *. The full parse can be created by substituting the asterix with the \"([pos] [word])\" string (or leaf) and concatenating the items in the rows of that column.     | 句法树\n- 7 Predicate lemma | The predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a \"-\" | semantic role information→语义角色信息、predicate lemma→谓词引理\n- 8 Predicate Frameset ID   |   This is the PropBank frameset ID of the predicate in Column 7. | PropBank frameset ID→命题树库\n- 9 Word sense  |   This is the word sense of the word in Column 3. \n- 10 Speaker/Author |   This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data.  | 说话的人\n- 11 Named Entities |  These columns identifies the spans representing various named entities. \n- 12:N Predicate Arguments  |  There is one column each of predicate argument structure information for the predicate mentioned in Column 7. N Coreference Coreference chain information encoded in a parenthesis structure.\n- 13：N\tCoreference\tCoreference chain information encoded in a parenthesis structure.\n \n***\nROOT：要处理文本的语句\n- IP：简单从句\n- NP：名词短语\n- VP：动词短语\n- PU：断句符，通常是句号、问号、感叹号等标点符号\n- LCP：方位词短语\n- PP：介词短语\n- CP：由‘的’构成的表示修饰性关系的短语\n- DNP：由‘的’构成的表示所属关系的短语\n- ADVP：副词短语\n- ADJP：形容词短语\n- DP：限定词短语\n- QP：量词短语\n- NN：常用名词\n- NR：固有名词\n- NT：时间名词\n- PN：代词\n- VV：动词\n- VC：是\n- CC：表示连词\n- VE：有\n- VA：表语形容词\n- AS：内容标记（如：了）\n- VRD：动补复合词\n- CD: 表示基数词\n- DT: determiner 表示限定词\n- EX: existential there 存在句\n- FW: foreign word 外来词\n- IN: preposition or conjunction, subordinating 介词或从属连词\n- JJ: adjective or numeral, ordinal 形容词或序数词\n- JJR: adjective, comparative 形容词比较级\n- JJS: adjective, superlative 形容词最高级\n- LS: list item marker 列表标识\n- MD: modal auxiliary 情态助动词\n- PDT: pre-determiner 前位限定词\n- POS: genitive marker 所有格标记\n- PRP: pronoun, personal 人称代词\n- RB: adverb 副词\n- RBR: adverb, comparative 副词比较级\n- RBS: adverb, superlative 副词最高级\n- RP: particle 小品词 \n- SYM: symbol 符号\n- TO:”to” as preposition or infinitive marker 作为介词或不定式标记 \n- WDT: WH-determiner WH限定词\n- WP: WH-pronoun WH代词\n- WP$: WH-pronoun, possessive WH所有格代词\n- WRB:Wh-adverb WH副词\n\n关系表示\n- abbrev: abbreviation modifier，缩写\n- acomp: adjectival complement，形容词的补充；\n- advcl : adverbial clause modifier，状语从句修饰词\n- advmod: adverbial modifier状语\n- agent: agent，代理，一般有by的时候会出现这个\n- amod: adjectival modifier形容词\n- appos: appositional modifier,同位词\n- attr: attributive，属性\n- aux: auxiliary，非主要动词和助词，如BE,HAVE SHOULD/COULD等到\n- auxpass: passive auxiliary 被动词\n- cc: coordination，并列关系，一般取第一个词\n- ccomp: clausal complement从句补充\n- complm: complementizer，引导从句的词好重聚中的主要动词\n- conj : conjunct，连接两个并列的词。\n- cop: copula。系动词（如be,seem,appear等），（命题主词与谓词间的）连系\n- csubj : clausal subject，从主关系\n- csubjpass: clausal passive subject 主从被动关系\n- dep: dependent依赖关系\n- det: determiner决定词，如冠词等\n- dobj : direct object直接宾语\n- expl: expletive，主要是抓取there\n- infmod: infinitival modifier，动词不定式\n- iobj : indirect object，非直接宾语，也就是所以的间接宾语；\n- mark: marker，主要出现在有“that” or “whether”“because”, “when”,\n- mwe: multi-word expression，多个词的表示\n- neg: negation modifier否定词\n- nn: noun compound modifier名词组合形式\n- npadvmod: noun phrase as adverbial modifier名词作状语\n- nsubj : nominal subject，名词主语\n- nsubjpass: passive nominal subject，被动的名词主语\n- num: numeric modifier，数值修饰\n- number: element of compound number，组合数字\n- parataxis: parataxis: parataxis，并列关系\n- partmod: participial modifier动词形式的修饰\n- pcomp: prepositional complement，介词补充\n- pobj : object of a preposition，介词的宾语\n- poss: possession modifier，所有形式，所有格，所属\n- possessive: possessive modifier，这个表示所有者和那个’S的关系\n- preconj : preconjunct，常常是出现在 “either”, “both”, “neither”的情况下\n- predet: predeterminer，前缀决定，常常是表示所有\n- prep: prepositional modifier\n- prepc: prepositional clausal modifier\n- prt: phrasal verb particle，动词短语\n- punct: punctuation，这个很少见，但是保留下来了，结果当中不会出现这个\n- purpcl : purpose clause modifier，目的从句\n- quantmod: quantifier phrase modifier，数量短语\n- rcmod: relative clause modifier相关关系\n- ref : referent，指示物，指代\n- rel : relative\n- root: root，最重要的词，从它开始，根节点\n- tmod: temporal modifier\n- xcomp: open clausal complement\n- xsubj : controlling subject 掌控者\n\n中心语为谓词\n-   subj — 主语\n-  nsubj — 名词性主语（nominal subject） （同步，建设）\n-    top — 主题（topic） （是，建筑）\n- npsubj — 被动型主语（nominal passive subject），专指由“被”引导的被动句中的主语，一般是谓词语义上的受事 （称作，镍）\n-  csubj — 从句主语（clausal subject），中文不存在\n-  xsubj — x主语，一般是一个主语下面含多个从句 （完善，有些）\n- 中心语为谓词或介词   \n-    obj — 宾语\n-   dobj — 直接宾语 （颁布，文件）\n-   iobj — 间接宾语（indirect object），基本不存在\n-  range — 间接宾语为数量词，又称为与格 （成交，元）\n-   pobj — 介词宾语 （根据，要求）\n-   lobj — 时间介词 （来，近年）\n\n中心语为谓词\n-   comp — 补语\n-  ccomp — 从句补语，一般由两个动词构成，中心语引导后一个动词所在的从句(IP) （出现，纳入）\n-  xcomp — x从句补语（xclausal complement），不存在   \n-  acomp — 形容词补语（adjectival complement）\n-  tcomp — 时间补语（temporal complement） （遇到，以前）\n- lccomp — 位置补语（localizer complement） （占，以上）\n- — 结果补语（resultative complement）\n\n中心语为名词\n-    mod — 修饰语（modifier）\n-   pass — 被动修饰（passive）\n-   tmod — 时间修饰（temporal modifier）\n-  rcmod — 关系从句修饰（relative clause modifier） （问题，遇到）\n-  numod — 数量修饰（numeric modifier） （规定，若干）\n- ornmod — 序数修饰（numeric modifier）\n-    clf — 类别修饰（classifier modifier） （文件，件）\n-   nmod — 复合名词修饰（noun compound modifier） （浦东，上海）\n-   amod — 形容词修饰（adjetive modifier） （情况，新）\n- advmod — 副词修饰（adverbial modifier） （做到，基本）\n-   vmod — 动词修饰（verb modifier，participle modifier）\n- prnmod — 插入词修饰（parenthetical modifier）\n-    neg — 不定修饰（negative modifier） (遇到，不)\n-    det — 限定词修饰（determiner modifier） （活动，这些）\n-  possm — 所属标记（possessive marker），NP\n-   poss — 所属修饰（possessive modifier），NP\n-   dvpm — DVP标记（dvp marker），DVP （简单，的）\n- dvpmod — DVP修饰（dvp modifier），DVP （采取，简单）\n-   assm — 关联标记（associative marker），DNP （开发，的）\n- assmod — 关联修饰（associative modifier），NP|QP （教训，特区）\n-   prep — 介词修饰（prepositional modifier） NP|VP|IP（采取，对）\n-  clmod — 从句修饰（clause modifier） （因为，开始）\n-  plmod — 介词性地点修饰（prepositional localizer modifier） （在，上）\n-    asp — 时态标词（aspect marker） （做到，了）\n- partmod– 分词修饰（participial modifier） 不存在\n-    etc — 等关系（etc） （办法，等）\n\n中心语为实词\n-   conj — 联合(conjunct)\n-    cop — 系动(copula) 双指助动词？？？？\n-    cc — 连接(coordination)，指中心词与连词 （开发，与）\n\n其它\n-   attr — 属性关系 （是，工程）\n- cordmod– 并列联合动词（coordinated verb compound） （颁布，实行）\n-   mmod — 情态动词（modal verb） （得到，能）\n-   ba — 把字关系\n- tclaus — 时间从句 （以后，积累）\n-        — semantic dependent\n-    cpm — 补语化成分（complementizer），一般指“的”引导的CP （振兴，的","source":"_posts/conll数据格式.md","raw":"---\ntitle: conll2011数据格式\ndate: 2017-8-24\ntags:\n    - CR\n    - NLP\ndescription: conll2011数据的格式相关内容\ntop: 1\n---\n\n##### From: https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html\n\n![image](http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/4E3C5A5EB28F402DA9F7FD3ADC16CD81/2000)\n\n#### http://conll.cemantix.org/2011/data.html\n\nRead _conll file format from CoNLL2011. See http://conll.bbn.com/index.php/data.html. \nCoNLL2011 files are in /scr/nlp/data/conll-2011/v0/data/ dev train Contains *_auto_conll files (auto generated) and _gold_conll (hand labelled), default reads _gold_conll \nThere is also /scr/nlp/data/conll-2011/v0/conll.trial which has *.conll files (parse has _ at end) \n\nColumn Type Description \n\n- 1 Document ID  | This is a variation on the document filename | 文档ID\n- 2 Part number |  Some files are divided into multiple parts numbered as 000, 001, 002, ... etc. | 分块ID\n- 3 Word number     | 词语ID\n- 4 Word itself     | 词语本身\n- 5 Part-of-Speech  | 词语的词性\n- 6 Parse bit | This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a *. The full parse can be created by substituting the asterix with the \"([pos] [word])\" string (or leaf) and concatenating the items in the rows of that column.     | 句法树\n- 7 Predicate lemma | The predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a \"-\" | semantic role information→语义角色信息、predicate lemma→谓词引理\n- 8 Predicate Frameset ID   |   This is the PropBank frameset ID of the predicate in Column 7. | PropBank frameset ID→命题树库\n- 9 Word sense  |   This is the word sense of the word in Column 3. \n- 10 Speaker/Author |   This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data.  | 说话的人\n- 11 Named Entities |  These columns identifies the spans representing various named entities. \n- 12:N Predicate Arguments  |  There is one column each of predicate argument structure information for the predicate mentioned in Column 7. N Coreference Coreference chain information encoded in a parenthesis structure.\n- 13：N\tCoreference\tCoreference chain information encoded in a parenthesis structure.\n \n***\nROOT：要处理文本的语句\n- IP：简单从句\n- NP：名词短语\n- VP：动词短语\n- PU：断句符，通常是句号、问号、感叹号等标点符号\n- LCP：方位词短语\n- PP：介词短语\n- CP：由‘的’构成的表示修饰性关系的短语\n- DNP：由‘的’构成的表示所属关系的短语\n- ADVP：副词短语\n- ADJP：形容词短语\n- DP：限定词短语\n- QP：量词短语\n- NN：常用名词\n- NR：固有名词\n- NT：时间名词\n- PN：代词\n- VV：动词\n- VC：是\n- CC：表示连词\n- VE：有\n- VA：表语形容词\n- AS：内容标记（如：了）\n- VRD：动补复合词\n- CD: 表示基数词\n- DT: determiner 表示限定词\n- EX: existential there 存在句\n- FW: foreign word 外来词\n- IN: preposition or conjunction, subordinating 介词或从属连词\n- JJ: adjective or numeral, ordinal 形容词或序数词\n- JJR: adjective, comparative 形容词比较级\n- JJS: adjective, superlative 形容词最高级\n- LS: list item marker 列表标识\n- MD: modal auxiliary 情态助动词\n- PDT: pre-determiner 前位限定词\n- POS: genitive marker 所有格标记\n- PRP: pronoun, personal 人称代词\n- RB: adverb 副词\n- RBR: adverb, comparative 副词比较级\n- RBS: adverb, superlative 副词最高级\n- RP: particle 小品词 \n- SYM: symbol 符号\n- TO:”to” as preposition or infinitive marker 作为介词或不定式标记 \n- WDT: WH-determiner WH限定词\n- WP: WH-pronoun WH代词\n- WP$: WH-pronoun, possessive WH所有格代词\n- WRB:Wh-adverb WH副词\n\n关系表示\n- abbrev: abbreviation modifier，缩写\n- acomp: adjectival complement，形容词的补充；\n- advcl : adverbial clause modifier，状语从句修饰词\n- advmod: adverbial modifier状语\n- agent: agent，代理，一般有by的时候会出现这个\n- amod: adjectival modifier形容词\n- appos: appositional modifier,同位词\n- attr: attributive，属性\n- aux: auxiliary，非主要动词和助词，如BE,HAVE SHOULD/COULD等到\n- auxpass: passive auxiliary 被动词\n- cc: coordination，并列关系，一般取第一个词\n- ccomp: clausal complement从句补充\n- complm: complementizer，引导从句的词好重聚中的主要动词\n- conj : conjunct，连接两个并列的词。\n- cop: copula。系动词（如be,seem,appear等），（命题主词与谓词间的）连系\n- csubj : clausal subject，从主关系\n- csubjpass: clausal passive subject 主从被动关系\n- dep: dependent依赖关系\n- det: determiner决定词，如冠词等\n- dobj : direct object直接宾语\n- expl: expletive，主要是抓取there\n- infmod: infinitival modifier，动词不定式\n- iobj : indirect object，非直接宾语，也就是所以的间接宾语；\n- mark: marker，主要出现在有“that” or “whether”“because”, “when”,\n- mwe: multi-word expression，多个词的表示\n- neg: negation modifier否定词\n- nn: noun compound modifier名词组合形式\n- npadvmod: noun phrase as adverbial modifier名词作状语\n- nsubj : nominal subject，名词主语\n- nsubjpass: passive nominal subject，被动的名词主语\n- num: numeric modifier，数值修饰\n- number: element of compound number，组合数字\n- parataxis: parataxis: parataxis，并列关系\n- partmod: participial modifier动词形式的修饰\n- pcomp: prepositional complement，介词补充\n- pobj : object of a preposition，介词的宾语\n- poss: possession modifier，所有形式，所有格，所属\n- possessive: possessive modifier，这个表示所有者和那个’S的关系\n- preconj : preconjunct，常常是出现在 “either”, “both”, “neither”的情况下\n- predet: predeterminer，前缀决定，常常是表示所有\n- prep: prepositional modifier\n- prepc: prepositional clausal modifier\n- prt: phrasal verb particle，动词短语\n- punct: punctuation，这个很少见，但是保留下来了，结果当中不会出现这个\n- purpcl : purpose clause modifier，目的从句\n- quantmod: quantifier phrase modifier，数量短语\n- rcmod: relative clause modifier相关关系\n- ref : referent，指示物，指代\n- rel : relative\n- root: root，最重要的词，从它开始，根节点\n- tmod: temporal modifier\n- xcomp: open clausal complement\n- xsubj : controlling subject 掌控者\n\n中心语为谓词\n-   subj — 主语\n-  nsubj — 名词性主语（nominal subject） （同步，建设）\n-    top — 主题（topic） （是，建筑）\n- npsubj — 被动型主语（nominal passive subject），专指由“被”引导的被动句中的主语，一般是谓词语义上的受事 （称作，镍）\n-  csubj — 从句主语（clausal subject），中文不存在\n-  xsubj — x主语，一般是一个主语下面含多个从句 （完善，有些）\n- 中心语为谓词或介词   \n-    obj — 宾语\n-   dobj — 直接宾语 （颁布，文件）\n-   iobj — 间接宾语（indirect object），基本不存在\n-  range — 间接宾语为数量词，又称为与格 （成交，元）\n-   pobj — 介词宾语 （根据，要求）\n-   lobj — 时间介词 （来，近年）\n\n中心语为谓词\n-   comp — 补语\n-  ccomp — 从句补语，一般由两个动词构成，中心语引导后一个动词所在的从句(IP) （出现，纳入）\n-  xcomp — x从句补语（xclausal complement），不存在   \n-  acomp — 形容词补语（adjectival complement）\n-  tcomp — 时间补语（temporal complement） （遇到，以前）\n- lccomp — 位置补语（localizer complement） （占，以上）\n- — 结果补语（resultative complement）\n\n中心语为名词\n-    mod — 修饰语（modifier）\n-   pass — 被动修饰（passive）\n-   tmod — 时间修饰（temporal modifier）\n-  rcmod — 关系从句修饰（relative clause modifier） （问题，遇到）\n-  numod — 数量修饰（numeric modifier） （规定，若干）\n- ornmod — 序数修饰（numeric modifier）\n-    clf — 类别修饰（classifier modifier） （文件，件）\n-   nmod — 复合名词修饰（noun compound modifier） （浦东，上海）\n-   amod — 形容词修饰（adjetive modifier） （情况，新）\n- advmod — 副词修饰（adverbial modifier） （做到，基本）\n-   vmod — 动词修饰（verb modifier，participle modifier）\n- prnmod — 插入词修饰（parenthetical modifier）\n-    neg — 不定修饰（negative modifier） (遇到，不)\n-    det — 限定词修饰（determiner modifier） （活动，这些）\n-  possm — 所属标记（possessive marker），NP\n-   poss — 所属修饰（possessive modifier），NP\n-   dvpm — DVP标记（dvp marker），DVP （简单，的）\n- dvpmod — DVP修饰（dvp modifier），DVP （采取，简单）\n-   assm — 关联标记（associative marker），DNP （开发，的）\n- assmod — 关联修饰（associative modifier），NP|QP （教训，特区）\n-   prep — 介词修饰（prepositional modifier） NP|VP|IP（采取，对）\n-  clmod — 从句修饰（clause modifier） （因为，开始）\n-  plmod — 介词性地点修饰（prepositional localizer modifier） （在，上）\n-    asp — 时态标词（aspect marker） （做到，了）\n- partmod– 分词修饰（participial modifier） 不存在\n-    etc — 等关系（etc） （办法，等）\n\n中心语为实词\n-   conj — 联合(conjunct)\n-    cop — 系动(copula) 双指助动词？？？？\n-    cc — 连接(coordination)，指中心词与连词 （开发，与）\n\n其它\n-   attr — 属性关系 （是，工程）\n- cordmod– 并列联合动词（coordinated verb compound） （颁布，实行）\n-   mmod — 情态动词（modal verb） （得到，能）\n-   ba — 把字关系\n- tclaus — 时间从句 （以后，积累）\n-        — semantic dependent\n-    cpm — 补语化成分（complementizer），一般指“的”引导的CP （振兴，的","slug":"conll数据格式","published":1,"updated":"2019-01-22T04:59:24.932Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2nw001cscftsk0rezd7","content":"<h5 id=\"From-https-nlp-stanford-edu-nlp-javadoc-javanlp-edu-stanford-nlp-dcoref-CoNLL2011DocumentReader-html\"><a href=\"#From-https-nlp-stanford-edu-nlp-javadoc-javanlp-edu-stanford-nlp-dcoref-CoNLL2011DocumentReader-html\" class=\"headerlink\" title=\"From: https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html\"></a>From: <a href=\"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html\" target=\"_blank\" rel=\"noopener\">https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html</a></h5><p><img src=\"http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/4E3C5A5EB28F402DA9F7FD3ADC16CD81/2000\" alt=\"image\"></p>\n<h4 id=\"http-conll-cemantix-org-2011-data-html\"><a href=\"#http-conll-cemantix-org-2011-data-html\" class=\"headerlink\" title=\"http://conll.cemantix.org/2011/data.html\"></a><a href=\"http://conll.cemantix.org/2011/data.html\" target=\"_blank\" rel=\"noopener\">http://conll.cemantix.org/2011/data.html</a></h4><p>Read _conll file format from CoNLL2011. See <a href=\"http://conll.bbn.com/index.php/data.html\" target=\"_blank\" rel=\"noopener\">http://conll.bbn.com/index.php/data.html</a>.<br>CoNLL2011 files are in /scr/nlp/data/conll-2011/v0/data/ dev train Contains <em>_auto_conll files (auto generated) and _gold_conll (hand labelled), default reads _gold_conll<br>There is also /scr/nlp/data/conll-2011/v0/conll.trial which has </em>.conll files (parse has _ at end) </p>\n<p>Column Type Description </p>\n<ul>\n<li>1 Document ID  | This is a variation on the document filename | 文档ID</li>\n<li>2 Part number |  Some files are divided into multiple parts numbered as 000, 001, 002, … etc. | 分块ID</li>\n<li>3 Word number     | 词语ID</li>\n<li>4 Word itself     | 词语本身</li>\n<li>5 Part-of-Speech  | 词语的词性</li>\n<li>6 Parse bit | This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a *. The full parse can be created by substituting the asterix with the “([pos] [word])” string (or leaf) and concatenating the items in the rows of that column.     | 句法树</li>\n<li>7 Predicate lemma | The predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a “-“ | semantic role information→语义角色信息、predicate lemma→谓词引理</li>\n<li>8 Predicate Frameset ID   |   This is the PropBank frameset ID of the predicate in Column 7. | PropBank frameset ID→命题树库</li>\n<li>9 Word sense  |   This is the word sense of the word in Column 3. </li>\n<li>10 Speaker/Author |   This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data.  | 说话的人</li>\n<li>11 Named Entities |  These columns identifies the spans representing various named entities. </li>\n<li>12:N Predicate Arguments  |  There is one column each of predicate argument structure information for the predicate mentioned in Column 7. N Coreference Coreference chain information encoded in a parenthesis structure.</li>\n<li>13：N    Coreference    Coreference chain information encoded in a parenthesis structure.</li>\n</ul>\n<hr>\n<p>ROOT：要处理文本的语句</p>\n<ul>\n<li>IP：简单从句</li>\n<li>NP：名词短语</li>\n<li>VP：动词短语</li>\n<li>PU：断句符，通常是句号、问号、感叹号等标点符号</li>\n<li>LCP：方位词短语</li>\n<li>PP：介词短语</li>\n<li>CP：由‘的’构成的表示修饰性关系的短语</li>\n<li>DNP：由‘的’构成的表示所属关系的短语</li>\n<li>ADVP：副词短语</li>\n<li>ADJP：形容词短语</li>\n<li>DP：限定词短语</li>\n<li>QP：量词短语</li>\n<li>NN：常用名词</li>\n<li>NR：固有名词</li>\n<li>NT：时间名词</li>\n<li>PN：代词</li>\n<li>VV：动词</li>\n<li>VC：是</li>\n<li>CC：表示连词</li>\n<li>VE：有</li>\n<li>VA：表语形容词</li>\n<li>AS：内容标记（如：了）</li>\n<li>VRD：动补复合词</li>\n<li>CD: 表示基数词</li>\n<li>DT: determiner 表示限定词</li>\n<li>EX: existential there 存在句</li>\n<li>FW: foreign word 外来词</li>\n<li>IN: preposition or conjunction, subordinating 介词或从属连词</li>\n<li>JJ: adjective or numeral, ordinal 形容词或序数词</li>\n<li>JJR: adjective, comparative 形容词比较级</li>\n<li>JJS: adjective, superlative 形容词最高级</li>\n<li>LS: list item marker 列表标识</li>\n<li>MD: modal auxiliary 情态助动词</li>\n<li>PDT: pre-determiner 前位限定词</li>\n<li>POS: genitive marker 所有格标记</li>\n<li>PRP: pronoun, personal 人称代词</li>\n<li>RB: adverb 副词</li>\n<li>RBR: adverb, comparative 副词比较级</li>\n<li>RBS: adverb, superlative 副词最高级</li>\n<li>RP: particle 小品词 </li>\n<li>SYM: symbol 符号</li>\n<li>TO:”to” as preposition or infinitive marker 作为介词或不定式标记 </li>\n<li>WDT: WH-determiner WH限定词</li>\n<li>WP: WH-pronoun WH代词</li>\n<li>WP$: WH-pronoun, possessive WH所有格代词</li>\n<li>WRB:Wh-adverb WH副词</li>\n</ul>\n<p>关系表示</p>\n<ul>\n<li>abbrev: abbreviation modifier，缩写</li>\n<li>acomp: adjectival complement，形容词的补充；</li>\n<li>advcl : adverbial clause modifier，状语从句修饰词</li>\n<li>advmod: adverbial modifier状语</li>\n<li>agent: agent，代理，一般有by的时候会出现这个</li>\n<li>amod: adjectival modifier形容词</li>\n<li>appos: appositional modifier,同位词</li>\n<li>attr: attributive，属性</li>\n<li>aux: auxiliary，非主要动词和助词，如BE,HAVE SHOULD/COULD等到</li>\n<li>auxpass: passive auxiliary 被动词</li>\n<li>cc: coordination，并列关系，一般取第一个词</li>\n<li>ccomp: clausal complement从句补充</li>\n<li>complm: complementizer，引导从句的词好重聚中的主要动词</li>\n<li>conj : conjunct，连接两个并列的词。</li>\n<li>cop: copula。系动词（如be,seem,appear等），（命题主词与谓词间的）连系</li>\n<li>csubj : clausal subject，从主关系</li>\n<li>csubjpass: clausal passive subject 主从被动关系</li>\n<li>dep: dependent依赖关系</li>\n<li>det: determiner决定词，如冠词等</li>\n<li>dobj : direct object直接宾语</li>\n<li>expl: expletive，主要是抓取there</li>\n<li>infmod: infinitival modifier，动词不定式</li>\n<li>iobj : indirect object，非直接宾语，也就是所以的间接宾语；</li>\n<li>mark: marker，主要出现在有“that” or “whether”“because”, “when”,</li>\n<li>mwe: multi-word expression，多个词的表示</li>\n<li>neg: negation modifier否定词</li>\n<li>nn: noun compound modifier名词组合形式</li>\n<li>npadvmod: noun phrase as adverbial modifier名词作状语</li>\n<li>nsubj : nominal subject，名词主语</li>\n<li>nsubjpass: passive nominal subject，被动的名词主语</li>\n<li>num: numeric modifier，数值修饰</li>\n<li>number: element of compound number，组合数字</li>\n<li>parataxis: parataxis: parataxis，并列关系</li>\n<li>partmod: participial modifier动词形式的修饰</li>\n<li>pcomp: prepositional complement，介词补充</li>\n<li>pobj : object of a preposition，介词的宾语</li>\n<li>poss: possession modifier，所有形式，所有格，所属</li>\n<li>possessive: possessive modifier，这个表示所有者和那个’S的关系</li>\n<li>preconj : preconjunct，常常是出现在 “either”, “both”, “neither”的情况下</li>\n<li>predet: predeterminer，前缀决定，常常是表示所有</li>\n<li>prep: prepositional modifier</li>\n<li>prepc: prepositional clausal modifier</li>\n<li>prt: phrasal verb particle，动词短语</li>\n<li>punct: punctuation，这个很少见，但是保留下来了，结果当中不会出现这个</li>\n<li>purpcl : purpose clause modifier，目的从句</li>\n<li>quantmod: quantifier phrase modifier，数量短语</li>\n<li>rcmod: relative clause modifier相关关系</li>\n<li>ref : referent，指示物，指代</li>\n<li>rel : relative</li>\n<li>root: root，最重要的词，从它开始，根节点</li>\n<li>tmod: temporal modifier</li>\n<li>xcomp: open clausal complement</li>\n<li>xsubj : controlling subject 掌控者</li>\n</ul>\n<p>中心语为谓词</p>\n<ul>\n<li>subj — 主语</li>\n<li>nsubj — 名词性主语（nominal subject） （同步，建设）</li>\n<li>top — 主题（topic） （是，建筑）</li>\n<li>npsubj — 被动型主语（nominal passive subject），专指由“被”引导的被动句中的主语，一般是谓词语义上的受事 （称作，镍）</li>\n<li>csubj — 从句主语（clausal subject），中文不存在</li>\n<li>xsubj — x主语，一般是一个主语下面含多个从句 （完善，有些）</li>\n<li>中心语为谓词或介词   </li>\n<li>obj — 宾语</li>\n<li>dobj — 直接宾语 （颁布，文件）</li>\n<li>iobj — 间接宾语（indirect object），基本不存在</li>\n<li>range — 间接宾语为数量词，又称为与格 （成交，元）</li>\n<li>pobj — 介词宾语 （根据，要求）</li>\n<li>lobj — 时间介词 （来，近年）</li>\n</ul>\n<p>中心语为谓词</p>\n<ul>\n<li>comp — 补语</li>\n<li>ccomp — 从句补语，一般由两个动词构成，中心语引导后一个动词所在的从句(IP) （出现，纳入）</li>\n<li>xcomp — x从句补语（xclausal complement），不存在   </li>\n<li>acomp — 形容词补语（adjectival complement）</li>\n<li>tcomp — 时间补语（temporal complement） （遇到，以前）</li>\n<li>lccomp — 位置补语（localizer complement） （占，以上）</li>\n<li>— 结果补语（resultative complement）</li>\n</ul>\n<p>中心语为名词</p>\n<ul>\n<li>mod — 修饰语（modifier）</li>\n<li>pass — 被动修饰（passive）</li>\n<li>tmod — 时间修饰（temporal modifier）</li>\n<li>rcmod — 关系从句修饰（relative clause modifier） （问题，遇到）</li>\n<li>numod — 数量修饰（numeric modifier） （规定，若干）</li>\n<li>ornmod — 序数修饰（numeric modifier）</li>\n<li>clf — 类别修饰（classifier modifier） （文件，件）</li>\n<li>nmod — 复合名词修饰（noun compound modifier） （浦东，上海）</li>\n<li>amod — 形容词修饰（adjetive modifier） （情况，新）</li>\n<li>advmod — 副词修饰（adverbial modifier） （做到，基本）</li>\n<li>vmod — 动词修饰（verb modifier，participle modifier）</li>\n<li>prnmod — 插入词修饰（parenthetical modifier）</li>\n<li>neg — 不定修饰（negative modifier） (遇到，不)</li>\n<li>det — 限定词修饰（determiner modifier） （活动，这些）</li>\n<li>possm — 所属标记（possessive marker），NP</li>\n<li>poss — 所属修饰（possessive modifier），NP</li>\n<li>dvpm — DVP标记（dvp marker），DVP （简单，的）</li>\n<li>dvpmod — DVP修饰（dvp modifier），DVP （采取，简单）</li>\n<li>assm — 关联标记（associative marker），DNP （开发，的）</li>\n<li>assmod — 关联修饰（associative modifier），NP|QP （教训，特区）</li>\n<li>prep — 介词修饰（prepositional modifier） NP|VP|IP（采取，对）</li>\n<li>clmod — 从句修饰（clause modifier） （因为，开始）</li>\n<li>plmod — 介词性地点修饰（prepositional localizer modifier） （在，上）</li>\n<li>asp — 时态标词（aspect marker） （做到，了）</li>\n<li>partmod– 分词修饰（participial modifier） 不存在</li>\n<li>etc — 等关系（etc） （办法，等）</li>\n</ul>\n<p>中心语为实词</p>\n<ul>\n<li>conj — 联合(conjunct)</li>\n<li>cop — 系动(copula) 双指助动词？？？？</li>\n<li>cc — 连接(coordination)，指中心词与连词 （开发，与）</li>\n</ul>\n<p>其它</p>\n<ul>\n<li>attr — 属性关系 （是，工程）</li>\n<li>cordmod– 并列联合动词（coordinated verb compound） （颁布，实行）</li>\n<li>mmod — 情态动词（modal verb） （得到，能）</li>\n<li>ba — 把字关系</li>\n<li>tclaus — 时间从句 （以后，积累）</li>\n<li>— semantic dependent</li>\n<li>cpm — 补语化成分（complementizer），一般指“的”引导的CP （振兴，的</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h5 id=\"From-https-nlp-stanford-edu-nlp-javadoc-javanlp-edu-stanford-nlp-dcoref-CoNLL2011DocumentReader-html\"><a href=\"#From-https-nlp-stanford-edu-nlp-javadoc-javanlp-edu-stanford-nlp-dcoref-CoNLL2011DocumentReader-html\" class=\"headerlink\" title=\"From: https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html\"></a>From: <a href=\"https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html\" target=\"_blank\" rel=\"noopener\">https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/dcoref/CoNLL2011DocumentReader.html</a></h5><p><img src=\"http://note.youdao.com/yws/public/resource/27feab63e93fe65035dae3a82b0a0cd5/xmlnote/4E3C5A5EB28F402DA9F7FD3ADC16CD81/2000\" alt=\"image\"></p>\n<h4 id=\"http-conll-cemantix-org-2011-data-html\"><a href=\"#http-conll-cemantix-org-2011-data-html\" class=\"headerlink\" title=\"http://conll.cemantix.org/2011/data.html\"></a><a href=\"http://conll.cemantix.org/2011/data.html\" target=\"_blank\" rel=\"noopener\">http://conll.cemantix.org/2011/data.html</a></h4><p>Read _conll file format from CoNLL2011. See <a href=\"http://conll.bbn.com/index.php/data.html\" target=\"_blank\" rel=\"noopener\">http://conll.bbn.com/index.php/data.html</a>.<br>CoNLL2011 files are in /scr/nlp/data/conll-2011/v0/data/ dev train Contains <em>_auto_conll files (auto generated) and _gold_conll (hand labelled), default reads _gold_conll<br>There is also /scr/nlp/data/conll-2011/v0/conll.trial which has </em>.conll files (parse has _ at end) </p>\n<p>Column Type Description </p>\n<ul>\n<li>1 Document ID  | This is a variation on the document filename | 文档ID</li>\n<li>2 Part number |  Some files are divided into multiple parts numbered as 000, 001, 002, … etc. | 分块ID</li>\n<li>3 Word number     | 词语ID</li>\n<li>4 Word itself     | 词语本身</li>\n<li>5 Part-of-Speech  | 词语的词性</li>\n<li>6 Parse bit | This is the bracketed structure broken before the first open parenthesis in the parse, and the word/part-of-speech leaf replaced with a *. The full parse can be created by substituting the asterix with the “([pos] [word])” string (or leaf) and concatenating the items in the rows of that column.     | 句法树</li>\n<li>7 Predicate lemma | The predicate lemma is mentioned for the rows for which we have semantic role information. All other rows are marked with a “-“ | semantic role information→语义角色信息、predicate lemma→谓词引理</li>\n<li>8 Predicate Frameset ID   |   This is the PropBank frameset ID of the predicate in Column 7. | PropBank frameset ID→命题树库</li>\n<li>9 Word sense  |   This is the word sense of the word in Column 3. </li>\n<li>10 Speaker/Author |   This is the speaker or author name where available. Mostly in Broadcast Conversation and Web Log data.  | 说话的人</li>\n<li>11 Named Entities |  These columns identifies the spans representing various named entities. </li>\n<li>12:N Predicate Arguments  |  There is one column each of predicate argument structure information for the predicate mentioned in Column 7. N Coreference Coreference chain information encoded in a parenthesis structure.</li>\n<li>13：N    Coreference    Coreference chain information encoded in a parenthesis structure.</li>\n</ul>\n<hr>\n<p>ROOT：要处理文本的语句</p>\n<ul>\n<li>IP：简单从句</li>\n<li>NP：名词短语</li>\n<li>VP：动词短语</li>\n<li>PU：断句符，通常是句号、问号、感叹号等标点符号</li>\n<li>LCP：方位词短语</li>\n<li>PP：介词短语</li>\n<li>CP：由‘的’构成的表示修饰性关系的短语</li>\n<li>DNP：由‘的’构成的表示所属关系的短语</li>\n<li>ADVP：副词短语</li>\n<li>ADJP：形容词短语</li>\n<li>DP：限定词短语</li>\n<li>QP：量词短语</li>\n<li>NN：常用名词</li>\n<li>NR：固有名词</li>\n<li>NT：时间名词</li>\n<li>PN：代词</li>\n<li>VV：动词</li>\n<li>VC：是</li>\n<li>CC：表示连词</li>\n<li>VE：有</li>\n<li>VA：表语形容词</li>\n<li>AS：内容标记（如：了）</li>\n<li>VRD：动补复合词</li>\n<li>CD: 表示基数词</li>\n<li>DT: determiner 表示限定词</li>\n<li>EX: existential there 存在句</li>\n<li>FW: foreign word 外来词</li>\n<li>IN: preposition or conjunction, subordinating 介词或从属连词</li>\n<li>JJ: adjective or numeral, ordinal 形容词或序数词</li>\n<li>JJR: adjective, comparative 形容词比较级</li>\n<li>JJS: adjective, superlative 形容词最高级</li>\n<li>LS: list item marker 列表标识</li>\n<li>MD: modal auxiliary 情态助动词</li>\n<li>PDT: pre-determiner 前位限定词</li>\n<li>POS: genitive marker 所有格标记</li>\n<li>PRP: pronoun, personal 人称代词</li>\n<li>RB: adverb 副词</li>\n<li>RBR: adverb, comparative 副词比较级</li>\n<li>RBS: adverb, superlative 副词最高级</li>\n<li>RP: particle 小品词 </li>\n<li>SYM: symbol 符号</li>\n<li>TO:”to” as preposition or infinitive marker 作为介词或不定式标记 </li>\n<li>WDT: WH-determiner WH限定词</li>\n<li>WP: WH-pronoun WH代词</li>\n<li>WP$: WH-pronoun, possessive WH所有格代词</li>\n<li>WRB:Wh-adverb WH副词</li>\n</ul>\n<p>关系表示</p>\n<ul>\n<li>abbrev: abbreviation modifier，缩写</li>\n<li>acomp: adjectival complement，形容词的补充；</li>\n<li>advcl : adverbial clause modifier，状语从句修饰词</li>\n<li>advmod: adverbial modifier状语</li>\n<li>agent: agent，代理，一般有by的时候会出现这个</li>\n<li>amod: adjectival modifier形容词</li>\n<li>appos: appositional modifier,同位词</li>\n<li>attr: attributive，属性</li>\n<li>aux: auxiliary，非主要动词和助词，如BE,HAVE SHOULD/COULD等到</li>\n<li>auxpass: passive auxiliary 被动词</li>\n<li>cc: coordination，并列关系，一般取第一个词</li>\n<li>ccomp: clausal complement从句补充</li>\n<li>complm: complementizer，引导从句的词好重聚中的主要动词</li>\n<li>conj : conjunct，连接两个并列的词。</li>\n<li>cop: copula。系动词（如be,seem,appear等），（命题主词与谓词间的）连系</li>\n<li>csubj : clausal subject，从主关系</li>\n<li>csubjpass: clausal passive subject 主从被动关系</li>\n<li>dep: dependent依赖关系</li>\n<li>det: determiner决定词，如冠词等</li>\n<li>dobj : direct object直接宾语</li>\n<li>expl: expletive，主要是抓取there</li>\n<li>infmod: infinitival modifier，动词不定式</li>\n<li>iobj : indirect object，非直接宾语，也就是所以的间接宾语；</li>\n<li>mark: marker，主要出现在有“that” or “whether”“because”, “when”,</li>\n<li>mwe: multi-word expression，多个词的表示</li>\n<li>neg: negation modifier否定词</li>\n<li>nn: noun compound modifier名词组合形式</li>\n<li>npadvmod: noun phrase as adverbial modifier名词作状语</li>\n<li>nsubj : nominal subject，名词主语</li>\n<li>nsubjpass: passive nominal subject，被动的名词主语</li>\n<li>num: numeric modifier，数值修饰</li>\n<li>number: element of compound number，组合数字</li>\n<li>parataxis: parataxis: parataxis，并列关系</li>\n<li>partmod: participial modifier动词形式的修饰</li>\n<li>pcomp: prepositional complement，介词补充</li>\n<li>pobj : object of a preposition，介词的宾语</li>\n<li>poss: possession modifier，所有形式，所有格，所属</li>\n<li>possessive: possessive modifier，这个表示所有者和那个’S的关系</li>\n<li>preconj : preconjunct，常常是出现在 “either”, “both”, “neither”的情况下</li>\n<li>predet: predeterminer，前缀决定，常常是表示所有</li>\n<li>prep: prepositional modifier</li>\n<li>prepc: prepositional clausal modifier</li>\n<li>prt: phrasal verb particle，动词短语</li>\n<li>punct: punctuation，这个很少见，但是保留下来了，结果当中不会出现这个</li>\n<li>purpcl : purpose clause modifier，目的从句</li>\n<li>quantmod: quantifier phrase modifier，数量短语</li>\n<li>rcmod: relative clause modifier相关关系</li>\n<li>ref : referent，指示物，指代</li>\n<li>rel : relative</li>\n<li>root: root，最重要的词，从它开始，根节点</li>\n<li>tmod: temporal modifier</li>\n<li>xcomp: open clausal complement</li>\n<li>xsubj : controlling subject 掌控者</li>\n</ul>\n<p>中心语为谓词</p>\n<ul>\n<li>subj — 主语</li>\n<li>nsubj — 名词性主语（nominal subject） （同步，建设）</li>\n<li>top — 主题（topic） （是，建筑）</li>\n<li>npsubj — 被动型主语（nominal passive subject），专指由“被”引导的被动句中的主语，一般是谓词语义上的受事 （称作，镍）</li>\n<li>csubj — 从句主语（clausal subject），中文不存在</li>\n<li>xsubj — x主语，一般是一个主语下面含多个从句 （完善，有些）</li>\n<li>中心语为谓词或介词   </li>\n<li>obj — 宾语</li>\n<li>dobj — 直接宾语 （颁布，文件）</li>\n<li>iobj — 间接宾语（indirect object），基本不存在</li>\n<li>range — 间接宾语为数量词，又称为与格 （成交，元）</li>\n<li>pobj — 介词宾语 （根据，要求）</li>\n<li>lobj — 时间介词 （来，近年）</li>\n</ul>\n<p>中心语为谓词</p>\n<ul>\n<li>comp — 补语</li>\n<li>ccomp — 从句补语，一般由两个动词构成，中心语引导后一个动词所在的从句(IP) （出现，纳入）</li>\n<li>xcomp — x从句补语（xclausal complement），不存在   </li>\n<li>acomp — 形容词补语（adjectival complement）</li>\n<li>tcomp — 时间补语（temporal complement） （遇到，以前）</li>\n<li>lccomp — 位置补语（localizer complement） （占，以上）</li>\n<li>— 结果补语（resultative complement）</li>\n</ul>\n<p>中心语为名词</p>\n<ul>\n<li>mod — 修饰语（modifier）</li>\n<li>pass — 被动修饰（passive）</li>\n<li>tmod — 时间修饰（temporal modifier）</li>\n<li>rcmod — 关系从句修饰（relative clause modifier） （问题，遇到）</li>\n<li>numod — 数量修饰（numeric modifier） （规定，若干）</li>\n<li>ornmod — 序数修饰（numeric modifier）</li>\n<li>clf — 类别修饰（classifier modifier） （文件，件）</li>\n<li>nmod — 复合名词修饰（noun compound modifier） （浦东，上海）</li>\n<li>amod — 形容词修饰（adjetive modifier） （情况，新）</li>\n<li>advmod — 副词修饰（adverbial modifier） （做到，基本）</li>\n<li>vmod — 动词修饰（verb modifier，participle modifier）</li>\n<li>prnmod — 插入词修饰（parenthetical modifier）</li>\n<li>neg — 不定修饰（negative modifier） (遇到，不)</li>\n<li>det — 限定词修饰（determiner modifier） （活动，这些）</li>\n<li>possm — 所属标记（possessive marker），NP</li>\n<li>poss — 所属修饰（possessive modifier），NP</li>\n<li>dvpm — DVP标记（dvp marker），DVP （简单，的）</li>\n<li>dvpmod — DVP修饰（dvp modifier），DVP （采取，简单）</li>\n<li>assm — 关联标记（associative marker），DNP （开发，的）</li>\n<li>assmod — 关联修饰（associative modifier），NP|QP （教训，特区）</li>\n<li>prep — 介词修饰（prepositional modifier） NP|VP|IP（采取，对）</li>\n<li>clmod — 从句修饰（clause modifier） （因为，开始）</li>\n<li>plmod — 介词性地点修饰（prepositional localizer modifier） （在，上）</li>\n<li>asp — 时态标词（aspect marker） （做到，了）</li>\n<li>partmod– 分词修饰（participial modifier） 不存在</li>\n<li>etc — 等关系（etc） （办法，等）</li>\n</ul>\n<p>中心语为实词</p>\n<ul>\n<li>conj — 联合(conjunct)</li>\n<li>cop — 系动(copula) 双指助动词？？？？</li>\n<li>cc — 连接(coordination)，指中心词与连词 （开发，与）</li>\n</ul>\n<p>其它</p>\n<ul>\n<li>attr — 属性关系 （是，工程）</li>\n<li>cordmod– 并列联合动词（coordinated verb compound） （颁布，实行）</li>\n<li>mmod — 情态动词（modal verb） （得到，能）</li>\n<li>ba — 把字关系</li>\n<li>tclaus — 时间从句 （以后，积累）</li>\n<li>— semantic dependent</li>\n<li>cpm — 补语化成分（complementizer），一般指“的”引导的CP （振兴，的</li>\n</ul>\n"},{"title":"Hyporch-笔记","date":"2018-07-13T16:00:00.000Z","description":"学习笔记","top":1,"_content":"#### Abstract\n\n采用了一种混合方法来指代消解任务，结合了基于规则方法的优势和基于学习方法的优势。评分很高。\n\n#### Section 1. Introduction\n\n- 拓展了CoNLL2012中的任务，从单一语言→多语言。针对三种语言：英语、汉语、阿拉伯语。\n- 针对这三种语言都设计了系统\n- Four tracks：\n    - closed track：for three languages\n    - open track： for Chinese\n- 有两个明显的特征：\n    - 往年的用的是基于规则的方法**或**基于学习的方法，我们这回将他们的优势结合起来了。\n    - 往年的人没有开发“genre-specific”信息，我们对我们系统参数做出了优化，遵守了每个genre。\n- 我们采用混合方法的原因是因为rule-based method 和learning-based method都有唯一的长处。\n- 根据上届CoNLL任务的方法，可以采用一个简单规则的公平小集合。\n- 我们先前的在用ml方法解决指代消解任务的工作，可以在启发词汇特征，优化XXXX  **这段没懂**\n- 在本系统中，采用正常的架构来完成实体检测任务（在消解之前），‘期望评价方法’这两个组件被充分利用。\n- 本篇论文的布局安排：\n    - mention detection conponent(Section2)\n    - coreference resolution component(Section3)\n    - show how their parameters are jointly optimized(Section4)/展示决定因素是如何优化的\n    - 评估&实验（Section5）\n\n#### Section 2. Mention Detection\n\n- 为了保证准确率和召回率的平衡，我们采用了一种**两步走**的方法。——\"extraction step\" & \"pruning step\"\n    - 首先，在*extraction step* 中，我们**定位命名实体**，**采用启发式\"language-specific\"方法来提取mentions**，以上工作从一个句法分析树，尽量增加召回率。\n    - 然后，在*pruning step*中，我们**采用启发式language-specific剪枝和基于学习的language-independent剪枝**。\n    - 总结，在step1中，定位实体，提取mention，增加召回率。在step2中，采用基于启发式和基于学习的剪枝\n- Heuristic Extraction and Pruning（启发式提取和剪枝）\n    - 英语：\n        - 在提取工作中，我们从相邻文本**span s**中创建候选mention，如果span s满足：\n            1. s是一个在句法分析树种的PRP或者NP\n            2. s作为一个NE，但不是PERCENT，MONEY，QUANTITY或者CARDINAL\n        - 在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：\n            1. mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部\n            2. mk有一个量词或者一个表示部分的修饰语\n            3. mk是一个单数常见NP，我们保留与时间有关的mention（例如today）\n    - 中文\n        - 在提取工作中，我们从句法树中的所有NP，QP结点创建中文mention\n        - 在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：\n            1. mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部。除非如果mk和mj在一篇新闻通信中。不像其他的文件标注，中文新闻通信文档注释确实考虑了这些共指对。\n            2. mk是一个NE，比如说PERCENT，MONEY，QUANTITY和CARDINAL\n            3. mk是一个疑问代词，例如what，where\n    - 阿拉伯\n        - 略\n- Learning-Based Pruning（基于学习的剪枝）\n    - 当启发式方法定位了候选mention后，还不能确定这个候选mention就是可以共指的。为了增强剪枝效果（因此~mention detection的准确率），我们采用了基于学习的方法来剪枝。\n    - 描述：我们用train data来定位和随后丢弃这些可能不能共指的候选mentions\n    - 具体来说，对于每个在test数据集中，并在启发式剪枝中留下来的mention mk，我们计算它的*mention coreference probability*，表示mk的“head noun”与其他mention产生共指关系的可能性。如果这个可能性没有超过阈值tc，我们就将mk从候选mention列表中移除。Section4将讲解tc如何结合共指关系成分参数来优化共指评价方法，的时候共同学习（这句话是不是语序有问题）\n    - 我们从train data中计算mk的*mention 从reference probability*评估。具体来说，因为OntoNotes里仅有“无-单数”是被注释的。因此我们可以用mk的“head noun”被注释次数（在gold mentions），由mk的“head noun”所有的出现次数（分离）。\n    - 如果mk的“head noun”没有在train data中出现，我们就设定它的“mcp”为1.意味着我们让它从过滤器中先pass掉。换句话说，我们比较保守，并且对于不能计算出“mcp”的mention不做处理。\n\n#### Section 3. Coreference Resolution\n\n- 概述\n    - 像mention detection 组件中，我们的从reference resolution组件也采用了启发式和机器学习方法。\n    - 我们使用了stanford的多轮共指消解方法（2011，Lee）来作为启发式共指消解。然而这些轮都是非词汇性的，我们用ML方法并结合词汇信息来增强多轮共指消解方法。\n    - 尽管不同的轮是针对不同的语言使用的，但是我们吸收词汇信息后的方法每轮对于所有的语言是一样的（WTF？？？）\n- The Multi-Pass Sieve Approach\n    - 这轮由一个或者多个启发式规则组成。每个规则都基于一个或者多个条件，从两个mentions中提取一个共指关系。举个例子来说，一个在Standford的“discourse processing sieve”将按照以下规则判断两个mention是不是共指：\n        1. 他们都是代词\n        2. 他们都是同一个speaker说的（生产的/制造的）\n    - sieves由他们的准确率进行排序。（？）\n    - 为了确定一个文件中的mention集合，决策器对它们用了多轮（Multi-Pass）：\n        - 在i-th pass中，仅用i-th中的规则对每个mention mk找一个先行词。当找这个先行词的时候，它的候选先行词被按一个顺序来访问，（这个顺序）通过它们在相关句法分析树的位置决定。\n    - i-th中部分mentions的聚类，然后被传递（pass）给i+1-th pass。（人话，上次的聚类结果给下次用）\n    - 因此，后面的passes可以充分利用前面的信息，但是之前确立的共指链不会比后面的更重要（？）\n- The Sieves \n    - For English\n        - 参照Standford的方法，采用12-sieves（Lee是13sieves，有一个是mention detection）\n        - 因为我们加入了closed track，我们重复执行了10sieves，没有使用外部知识源。\n        - 这10个sieves在Table2种list了。\n        - 我们忽视了别名（Alias sieve）和词汇链（Lexical Chain sieve），这两个sieve用WordNet、WiKi和Freebase上的信息进行计算\n    - For Chinese\n        - 概述：\n            - 我们参加了closed track 和open track\n            - 这轮我们的应用对两个track都是相同的，除了我们对open track使用NE信息来改进某些系统。\n            - 为了自动获取NE注释，我们使用了一个在train data中，通过gold NE注释训练出的**NE模型**\n        - 中文决策器由**9轮**组成。(这里有个中英文对比的表格/Table2）\n        - 这些sieves在基本上和英语参照相同的方式被执行，除了它们中的一小部分，（这一小部分）被修饰用以证明 一些独有的特性或者中文指代注释。（这句话有点儿难翻译）\n        - 就像以下的详细介绍，（接下来会讲3个sieve）\n            - 我们**介绍了一个新sieve**，the Chinese Head Match sieve；\n            - **调整了两个现存的sieves**,the Precise Constructs sieve, the Pronoun sieve\n        - **Chinese Head Match sieve**\n            - 像Section2中说到的，一个mention和它的embedding mention能够称为共指关系是由于它们有相同头部。\n            - 为了定位这些指代关系，我们执行了**Same Head sieve**，如果两个mention mj和mk有相同的Head并且mk is embedded within mj，（这个sieve）就假设两个mentions是共指的。\n            - 这个规则有个**例外**，如果mj是一个由两个或者更多NP组成的同级（coordinate）NP，并且mk是这些NP之一，这两个mentions将不作为指代关系。\n        - **Precise Constructs sieve**\n            - 像Lee2011中的，Precise Construct sieve 判断两个mentions是不是共指关系的方法是基于信息，（这个信息）比如说一个mention是不是另一个mention的首字母缩略词，或者说它们是不是来自一个同位语关系或者系表关系。\n            - 我们给这个sieve吸收其他的规则来处理中文缩写中特殊的例子。——外国人名的缩写、中国人名的缩写\n        - **Pronouns sieves**\n            - 这个sieve通过利用像mention的*性别、数量*这样的语法信息，解决了代词关系。\n            - 这些语法信息主要针对英文，对于中文并不可信（not true）\n            - 为了获取针对中文的像这样的语法信息，我们设计了一个简单的方法。如下三步：\n                - 首先我们设定了简单的启发式来从这些中文NP中提取容易推断的语法信息。举例来说，我们可以启发式地确定性别，数量，物种，比如“ 她【she】是「Female，single，Animate」”；比如“ 它们【they】是「未知的，复数，无生命的」”。                再加上我们能决定一个有命名实体信息的mention的语法特征。举个例子，一个PERSON能够有指定的语法特性，「未知，单数，动物」\n                - 其次，我们仿真了这些有启发式地已确定的语法特征值。观察在一个共指链中所有的mentions，它们的性别，数量，物种应该统一。详细来说，给定一个train文本，如果在共指链的一个mention是启发式的被语法信息所标记，我们自动所有的还需处理的mentions注释相同的语法特征值。\n                - 最后，我们自动创建了六单词列表（six word lists），包含「1，物种词语；2，无生命词语；3，男性词语；4，女性词语；5，单数词；6，复数词」。\n\t                - Specifically,we populate these word lists with the grammatically annotated mentions from the previous step,where each element of a word list is composed of the head of a mention and a count indicating the number of time the mention is annotated with the corresponding grammatical attribute value.\n                - 然后我们可以在test文本上用这些词链来确定mentions的属性值。由于这些词链规模较小，而且我们的目的是增加准确率。如果这三个属性中的每一个(?)，一个mention有一个未知(*Unknown*)属性而其他的mentions有已知(*Known*)属性，我们就将两个mentions考虑为共指关系。\n        - 总结\n            - 从Table2中可以看到，我们的中文系统没有*Relaxed String Match sieve*，不像英语有对应的。回顾这个sieve对两个mentions，如果跟在落下的文本的字符串紧跟着它们的head words是相同的，就把他们当作共指关系。（例子：*Michael Wolf*,and *Michael Wolf,a contributing editor for \"New York\".*)\n            - 由于中文人名经常有单个字符组成，并且heads很少跟在别的中文词语后面，我们相信*Relaxed Head Match*对于确定中文的共指关系没什么用。\n            - 因此，中文人名缩写这样的将会交给*Precise Constructs sieve*处理 \n    - For Arabic\n\t    - 仅用了一轮，the exact match sieve。因为用别的（上面的）效果不好\n\n- Incorporating Lexical Information(结合词法信息)\n\t- 像之前提到的一样，我们结合词法信息增强了这个sieve。\n\t- 为了利用词法信息，我们首先计算了*词法概率*。具体来说，对于文本中的每个mention mj和mk，我们首先计算两个概率：\n\t\t1. *string-pair（SP-Prob）*概率，也就是包含两个mentions的字符串sj和sk是共指的概率。\n\t\t2. *head-pair（HP-Prob）*概率，也就是两个mentions的head 名词hj和hk是共指的概率。\n\t- 为了更好的评价，我们对training data和这两个mentions进行预处理：\n\t\t1. 对每个英文单词进行小写转换\n\t\t2. 通过一个由\t字符串形式对每个阿拉伯单词w进行替换，——**这句话是讲处理阿拉伯语的，看不懂，略**\n\t- 如果sj(hj)和sk(hk)没有在training数据集中出现的话，我们就将SP-Prob(mj,mk)(HP-Prob(mj,mk))标记为*Undefined*\n\t- 然后，我们通过对sieve方法提出两种扩展，然后用这些词法概率来增强mj和mk的处理结果。\n\t\t- 第一种扩展着力于增强sieve方法的*准确率*。具体来说，在进入任何sieve之前，我们需要确认SP-Prob(mj,mk)<=tSPL 或者HP-Prob(mj,mk)<=tHPL。如果是这样，我们的解析器将绕过所有的sieves，并且认为mj，mk不是共指的。\n\t\t我们用了词汇概率来增进准确度，具体来说通过假设两个mentions不是共指的，如果在training data中有“sufficient/足够的”信息来让我们做出这个决定。如果两个概率都没有定义，那么这个mentions对儿在过滤器中留存，然后给sieve管道做后续处理。\n\t\t- 第二种扩展着力于增强sieve方法的*召回率*。具体来说，我们创建了一个新的sieve，叫**”Lexical Pair sieve**，将这个sieve添加到sieve管道的最后面，如果SP-Prob(mj,mk)>=tSPU or HP-Prob(mj,mk)>=tHPU，就假设mj和mk是共指的。\n\t\t其他的类似第一种扩展，如果这两个概率都没有定义，那么这个sieve将不会处理这个mentions对儿。\n\t\t- 这四个阈值**tSPL，tHPL；tSPU，tHPU**，将在development 数据集上被优化\n\n#### Section 4. Parameter Estimation（参数估计）\n就像之前讨论的一样，我们在development数据上训练系统的参数来优化消解性能。我们的系统有两套可调节的参数。目前为止，我们先看一套参数，命名为*五词汇概率阈值*，包含tc、tSPL、tHPL、tSPU、tHPU。 再看第二套参数包含*rule relaxation parameters*.<p>\n回顾一下，一个sieve中的每个规则都由一个或多个条件组成。我们联合条件 i 和参数 入i ，（这个参数）是一个控制条件i是否应该被去除的二值。特别的，如果 入i=0，条件i将被从相对应的规则中移除。\n\n缓和参数的目的应当明确：他们允许我们用机器学习的方法去优化 hand-craft 规则。这一节提出了在development数据集上得到参数的的两种算法。\n\n在讨论参数优化算法之前，先回顾一下简介。（简介是指）我们分辨特征的方法是我们建立了*genre-specific*分解器。换句话说，对每种语言的每个genre，我们\n\n\t1. 从相应的training数据中，学习词汇概率\n\t2. 获取最理想的参数值O1，O2，各自/依次用算参数估计算法1和2给development数据估计\n\t3. O1和O2之一，选择其中可以在development数据集上更好表现的一个来作为最后参数估计的set（？）\n\n- 算法1\n\t- pass\n\n- 算法2\n\t- pass\n\n#### Section 5. Results and Discussion\n\n完整的结果在table3中。\n\n- mention detection results & coreference results 由准确率/召回率/F值来描述。\n- 为了更好理解这两个参数sets的角色，我们展示了独立的实验。对每个语言-track","source":"_posts/论文笔记.md","raw":"---\ntitle: Hyporch-笔记\ndate: 2017-19-14\ntags:\n    - CR\n    - NLP\ndescription: 学习笔记\ntop: 1\n---\n#### Abstract\n\n采用了一种混合方法来指代消解任务，结合了基于规则方法的优势和基于学习方法的优势。评分很高。\n\n#### Section 1. Introduction\n\n- 拓展了CoNLL2012中的任务，从单一语言→多语言。针对三种语言：英语、汉语、阿拉伯语。\n- 针对这三种语言都设计了系统\n- Four tracks：\n    - closed track：for three languages\n    - open track： for Chinese\n- 有两个明显的特征：\n    - 往年的用的是基于规则的方法**或**基于学习的方法，我们这回将他们的优势结合起来了。\n    - 往年的人没有开发“genre-specific”信息，我们对我们系统参数做出了优化，遵守了每个genre。\n- 我们采用混合方法的原因是因为rule-based method 和learning-based method都有唯一的长处。\n- 根据上届CoNLL任务的方法，可以采用一个简单规则的公平小集合。\n- 我们先前的在用ml方法解决指代消解任务的工作，可以在启发词汇特征，优化XXXX  **这段没懂**\n- 在本系统中，采用正常的架构来完成实体检测任务（在消解之前），‘期望评价方法’这两个组件被充分利用。\n- 本篇论文的布局安排：\n    - mention detection conponent(Section2)\n    - coreference resolution component(Section3)\n    - show how their parameters are jointly optimized(Section4)/展示决定因素是如何优化的\n    - 评估&实验（Section5）\n\n#### Section 2. Mention Detection\n\n- 为了保证准确率和召回率的平衡，我们采用了一种**两步走**的方法。——\"extraction step\" & \"pruning step\"\n    - 首先，在*extraction step* 中，我们**定位命名实体**，**采用启发式\"language-specific\"方法来提取mentions**，以上工作从一个句法分析树，尽量增加召回率。\n    - 然后，在*pruning step*中，我们**采用启发式language-specific剪枝和基于学习的language-independent剪枝**。\n    - 总结，在step1中，定位实体，提取mention，增加召回率。在step2中，采用基于启发式和基于学习的剪枝\n- Heuristic Extraction and Pruning（启发式提取和剪枝）\n    - 英语：\n        - 在提取工作中，我们从相邻文本**span s**中创建候选mention，如果span s满足：\n            1. s是一个在句法分析树种的PRP或者NP\n            2. s作为一个NE，但不是PERCENT，MONEY，QUANTITY或者CARDINAL\n        - 在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：\n            1. mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部\n            2. mk有一个量词或者一个表示部分的修饰语\n            3. mk是一个单数常见NP，我们保留与时间有关的mention（例如today）\n    - 中文\n        - 在提取工作中，我们从句法树中的所有NP，QP结点创建中文mention\n        - 在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：\n            1. mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部。除非如果mk和mj在一篇新闻通信中。不像其他的文件标注，中文新闻通信文档注释确实考虑了这些共指对。\n            2. mk是一个NE，比如说PERCENT，MONEY，QUANTITY和CARDINAL\n            3. mk是一个疑问代词，例如what，where\n    - 阿拉伯\n        - 略\n- Learning-Based Pruning（基于学习的剪枝）\n    - 当启发式方法定位了候选mention后，还不能确定这个候选mention就是可以共指的。为了增强剪枝效果（因此~mention detection的准确率），我们采用了基于学习的方法来剪枝。\n    - 描述：我们用train data来定位和随后丢弃这些可能不能共指的候选mentions\n    - 具体来说，对于每个在test数据集中，并在启发式剪枝中留下来的mention mk，我们计算它的*mention coreference probability*，表示mk的“head noun”与其他mention产生共指关系的可能性。如果这个可能性没有超过阈值tc，我们就将mk从候选mention列表中移除。Section4将讲解tc如何结合共指关系成分参数来优化共指评价方法，的时候共同学习（这句话是不是语序有问题）\n    - 我们从train data中计算mk的*mention 从reference probability*评估。具体来说，因为OntoNotes里仅有“无-单数”是被注释的。因此我们可以用mk的“head noun”被注释次数（在gold mentions），由mk的“head noun”所有的出现次数（分离）。\n    - 如果mk的“head noun”没有在train data中出现，我们就设定它的“mcp”为1.意味着我们让它从过滤器中先pass掉。换句话说，我们比较保守，并且对于不能计算出“mcp”的mention不做处理。\n\n#### Section 3. Coreference Resolution\n\n- 概述\n    - 像mention detection 组件中，我们的从reference resolution组件也采用了启发式和机器学习方法。\n    - 我们使用了stanford的多轮共指消解方法（2011，Lee）来作为启发式共指消解。然而这些轮都是非词汇性的，我们用ML方法并结合词汇信息来增强多轮共指消解方法。\n    - 尽管不同的轮是针对不同的语言使用的，但是我们吸收词汇信息后的方法每轮对于所有的语言是一样的（WTF？？？）\n- The Multi-Pass Sieve Approach\n    - 这轮由一个或者多个启发式规则组成。每个规则都基于一个或者多个条件，从两个mentions中提取一个共指关系。举个例子来说，一个在Standford的“discourse processing sieve”将按照以下规则判断两个mention是不是共指：\n        1. 他们都是代词\n        2. 他们都是同一个speaker说的（生产的/制造的）\n    - sieves由他们的准确率进行排序。（？）\n    - 为了确定一个文件中的mention集合，决策器对它们用了多轮（Multi-Pass）：\n        - 在i-th pass中，仅用i-th中的规则对每个mention mk找一个先行词。当找这个先行词的时候，它的候选先行词被按一个顺序来访问，（这个顺序）通过它们在相关句法分析树的位置决定。\n    - i-th中部分mentions的聚类，然后被传递（pass）给i+1-th pass。（人话，上次的聚类结果给下次用）\n    - 因此，后面的passes可以充分利用前面的信息，但是之前确立的共指链不会比后面的更重要（？）\n- The Sieves \n    - For English\n        - 参照Standford的方法，采用12-sieves（Lee是13sieves，有一个是mention detection）\n        - 因为我们加入了closed track，我们重复执行了10sieves，没有使用外部知识源。\n        - 这10个sieves在Table2种list了。\n        - 我们忽视了别名（Alias sieve）和词汇链（Lexical Chain sieve），这两个sieve用WordNet、WiKi和Freebase上的信息进行计算\n    - For Chinese\n        - 概述：\n            - 我们参加了closed track 和open track\n            - 这轮我们的应用对两个track都是相同的，除了我们对open track使用NE信息来改进某些系统。\n            - 为了自动获取NE注释，我们使用了一个在train data中，通过gold NE注释训练出的**NE模型**\n        - 中文决策器由**9轮**组成。(这里有个中英文对比的表格/Table2）\n        - 这些sieves在基本上和英语参照相同的方式被执行，除了它们中的一小部分，（这一小部分）被修饰用以证明 一些独有的特性或者中文指代注释。（这句话有点儿难翻译）\n        - 就像以下的详细介绍，（接下来会讲3个sieve）\n            - 我们**介绍了一个新sieve**，the Chinese Head Match sieve；\n            - **调整了两个现存的sieves**,the Precise Constructs sieve, the Pronoun sieve\n        - **Chinese Head Match sieve**\n            - 像Section2中说到的，一个mention和它的embedding mention能够称为共指关系是由于它们有相同头部。\n            - 为了定位这些指代关系，我们执行了**Same Head sieve**，如果两个mention mj和mk有相同的Head并且mk is embedded within mj，（这个sieve）就假设两个mentions是共指的。\n            - 这个规则有个**例外**，如果mj是一个由两个或者更多NP组成的同级（coordinate）NP，并且mk是这些NP之一，这两个mentions将不作为指代关系。\n        - **Precise Constructs sieve**\n            - 像Lee2011中的，Precise Construct sieve 判断两个mentions是不是共指关系的方法是基于信息，（这个信息）比如说一个mention是不是另一个mention的首字母缩略词，或者说它们是不是来自一个同位语关系或者系表关系。\n            - 我们给这个sieve吸收其他的规则来处理中文缩写中特殊的例子。——外国人名的缩写、中国人名的缩写\n        - **Pronouns sieves**\n            - 这个sieve通过利用像mention的*性别、数量*这样的语法信息，解决了代词关系。\n            - 这些语法信息主要针对英文，对于中文并不可信（not true）\n            - 为了获取针对中文的像这样的语法信息，我们设计了一个简单的方法。如下三步：\n                - 首先我们设定了简单的启发式来从这些中文NP中提取容易推断的语法信息。举例来说，我们可以启发式地确定性别，数量，物种，比如“ 她【she】是「Female，single，Animate」”；比如“ 它们【they】是「未知的，复数，无生命的」”。                再加上我们能决定一个有命名实体信息的mention的语法特征。举个例子，一个PERSON能够有指定的语法特性，「未知，单数，动物」\n                - 其次，我们仿真了这些有启发式地已确定的语法特征值。观察在一个共指链中所有的mentions，它们的性别，数量，物种应该统一。详细来说，给定一个train文本，如果在共指链的一个mention是启发式的被语法信息所标记，我们自动所有的还需处理的mentions注释相同的语法特征值。\n                - 最后，我们自动创建了六单词列表（six word lists），包含「1，物种词语；2，无生命词语；3，男性词语；4，女性词语；5，单数词；6，复数词」。\n\t                - Specifically,we populate these word lists with the grammatically annotated mentions from the previous step,where each element of a word list is composed of the head of a mention and a count indicating the number of time the mention is annotated with the corresponding grammatical attribute value.\n                - 然后我们可以在test文本上用这些词链来确定mentions的属性值。由于这些词链规模较小，而且我们的目的是增加准确率。如果这三个属性中的每一个(?)，一个mention有一个未知(*Unknown*)属性而其他的mentions有已知(*Known*)属性，我们就将两个mentions考虑为共指关系。\n        - 总结\n            - 从Table2中可以看到，我们的中文系统没有*Relaxed String Match sieve*，不像英语有对应的。回顾这个sieve对两个mentions，如果跟在落下的文本的字符串紧跟着它们的head words是相同的，就把他们当作共指关系。（例子：*Michael Wolf*,and *Michael Wolf,a contributing editor for \"New York\".*)\n            - 由于中文人名经常有单个字符组成，并且heads很少跟在别的中文词语后面，我们相信*Relaxed Head Match*对于确定中文的共指关系没什么用。\n            - 因此，中文人名缩写这样的将会交给*Precise Constructs sieve*处理 \n    - For Arabic\n\t    - 仅用了一轮，the exact match sieve。因为用别的（上面的）效果不好\n\n- Incorporating Lexical Information(结合词法信息)\n\t- 像之前提到的一样，我们结合词法信息增强了这个sieve。\n\t- 为了利用词法信息，我们首先计算了*词法概率*。具体来说，对于文本中的每个mention mj和mk，我们首先计算两个概率：\n\t\t1. *string-pair（SP-Prob）*概率，也就是包含两个mentions的字符串sj和sk是共指的概率。\n\t\t2. *head-pair（HP-Prob）*概率，也就是两个mentions的head 名词hj和hk是共指的概率。\n\t- 为了更好的评价，我们对training data和这两个mentions进行预处理：\n\t\t1. 对每个英文单词进行小写转换\n\t\t2. 通过一个由\t字符串形式对每个阿拉伯单词w进行替换，——**这句话是讲处理阿拉伯语的，看不懂，略**\n\t- 如果sj(hj)和sk(hk)没有在training数据集中出现的话，我们就将SP-Prob(mj,mk)(HP-Prob(mj,mk))标记为*Undefined*\n\t- 然后，我们通过对sieve方法提出两种扩展，然后用这些词法概率来增强mj和mk的处理结果。\n\t\t- 第一种扩展着力于增强sieve方法的*准确率*。具体来说，在进入任何sieve之前，我们需要确认SP-Prob(mj,mk)<=tSPL 或者HP-Prob(mj,mk)<=tHPL。如果是这样，我们的解析器将绕过所有的sieves，并且认为mj，mk不是共指的。\n\t\t我们用了词汇概率来增进准确度，具体来说通过假设两个mentions不是共指的，如果在training data中有“sufficient/足够的”信息来让我们做出这个决定。如果两个概率都没有定义，那么这个mentions对儿在过滤器中留存，然后给sieve管道做后续处理。\n\t\t- 第二种扩展着力于增强sieve方法的*召回率*。具体来说，我们创建了一个新的sieve，叫**”Lexical Pair sieve**，将这个sieve添加到sieve管道的最后面，如果SP-Prob(mj,mk)>=tSPU or HP-Prob(mj,mk)>=tHPU，就假设mj和mk是共指的。\n\t\t其他的类似第一种扩展，如果这两个概率都没有定义，那么这个sieve将不会处理这个mentions对儿。\n\t\t- 这四个阈值**tSPL，tHPL；tSPU，tHPU**，将在development 数据集上被优化\n\n#### Section 4. Parameter Estimation（参数估计）\n就像之前讨论的一样，我们在development数据上训练系统的参数来优化消解性能。我们的系统有两套可调节的参数。目前为止，我们先看一套参数，命名为*五词汇概率阈值*，包含tc、tSPL、tHPL、tSPU、tHPU。 再看第二套参数包含*rule relaxation parameters*.<p>\n回顾一下，一个sieve中的每个规则都由一个或多个条件组成。我们联合条件 i 和参数 入i ，（这个参数）是一个控制条件i是否应该被去除的二值。特别的，如果 入i=0，条件i将被从相对应的规则中移除。\n\n缓和参数的目的应当明确：他们允许我们用机器学习的方法去优化 hand-craft 规则。这一节提出了在development数据集上得到参数的的两种算法。\n\n在讨论参数优化算法之前，先回顾一下简介。（简介是指）我们分辨特征的方法是我们建立了*genre-specific*分解器。换句话说，对每种语言的每个genre，我们\n\n\t1. 从相应的training数据中，学习词汇概率\n\t2. 获取最理想的参数值O1，O2，各自/依次用算参数估计算法1和2给development数据估计\n\t3. O1和O2之一，选择其中可以在development数据集上更好表现的一个来作为最后参数估计的set（？）\n\n- 算法1\n\t- pass\n\n- 算法2\n\t- pass\n\n#### Section 5. Results and Discussion\n\n完整的结果在table3中。\n\n- mention detection results & coreference results 由准确率/召回率/F值来描述。\n- 为了更好理解这两个参数sets的角色，我们展示了独立的实验。对每个语言-track","slug":"论文笔记","published":1,"updated":"2019-01-22T04:59:24.960Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2o6001escftou26fis5","content":"<h4 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h4><p>采用了一种混合方法来指代消解任务，结合了基于规则方法的优势和基于学习方法的优势。评分很高。</p>\n<h4 id=\"Section-1-Introduction\"><a href=\"#Section-1-Introduction\" class=\"headerlink\" title=\"Section 1. Introduction\"></a>Section 1. Introduction</h4><ul>\n<li>拓展了CoNLL2012中的任务，从单一语言→多语言。针对三种语言：英语、汉语、阿拉伯语。</li>\n<li>针对这三种语言都设计了系统</li>\n<li>Four tracks：<ul>\n<li>closed track：for three languages</li>\n<li>open track： for Chinese</li>\n</ul>\n</li>\n<li>有两个明显的特征：<ul>\n<li>往年的用的是基于规则的方法<strong>或</strong>基于学习的方法，我们这回将他们的优势结合起来了。</li>\n<li>往年的人没有开发“genre-specific”信息，我们对我们系统参数做出了优化，遵守了每个genre。</li>\n</ul>\n</li>\n<li>我们采用混合方法的原因是因为rule-based method 和learning-based method都有唯一的长处。</li>\n<li>根据上届CoNLL任务的方法，可以采用一个简单规则的公平小集合。</li>\n<li>我们先前的在用ml方法解决指代消解任务的工作，可以在启发词汇特征，优化XXXX  <strong>这段没懂</strong></li>\n<li>在本系统中，采用正常的架构来完成实体检测任务（在消解之前），‘期望评价方法’这两个组件被充分利用。</li>\n<li>本篇论文的布局安排：<ul>\n<li>mention detection conponent(Section2)</li>\n<li>coreference resolution component(Section3)</li>\n<li>show how their parameters are jointly optimized(Section4)/展示决定因素是如何优化的</li>\n<li>评估&amp;实验（Section5）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-2-Mention-Detection\"><a href=\"#Section-2-Mention-Detection\" class=\"headerlink\" title=\"Section 2. Mention Detection\"></a>Section 2. Mention Detection</h4><ul>\n<li>为了保证准确率和召回率的平衡，我们采用了一种<strong>两步走</strong>的方法。——“extraction step” &amp; “pruning step”<ul>\n<li>首先，在<em>extraction step</em> 中，我们<strong>定位命名实体</strong>，<strong>采用启发式”language-specific”方法来提取mentions</strong>，以上工作从一个句法分析树，尽量增加召回率。</li>\n<li>然后，在<em>pruning step</em>中，我们<strong>采用启发式language-specific剪枝和基于学习的language-independent剪枝</strong>。</li>\n<li>总结，在step1中，定位实体，提取mention，增加召回率。在step2中，采用基于启发式和基于学习的剪枝</li>\n</ul>\n</li>\n<li>Heuristic Extraction and Pruning（启发式提取和剪枝）<ul>\n<li>英语：<ul>\n<li>在提取工作中，我们从相邻文本<strong>span s</strong>中创建候选mention，如果span s满足：<ol>\n<li>s是一个在句法分析树种的PRP或者NP</li>\n<li>s作为一个NE，但不是PERCENT，MONEY，QUANTITY或者CARDINAL</li>\n</ol>\n</li>\n<li>在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：<ol>\n<li>mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部</li>\n<li>mk有一个量词或者一个表示部分的修饰语</li>\n<li>mk是一个单数常见NP，我们保留与时间有关的mention（例如today）</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>中文<ul>\n<li>在提取工作中，我们从句法树中的所有NP，QP结点创建中文mention</li>\n<li>在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：<ol>\n<li>mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部。除非如果mk和mj在一篇新闻通信中。不像其他的文件标注，中文新闻通信文档注释确实考虑了这些共指对。</li>\n<li>mk是一个NE，比如说PERCENT，MONEY，QUANTITY和CARDINAL</li>\n<li>mk是一个疑问代词，例如what，where</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>阿拉伯<ul>\n<li>略</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Learning-Based Pruning（基于学习的剪枝）<ul>\n<li>当启发式方法定位了候选mention后，还不能确定这个候选mention就是可以共指的。为了增强剪枝效果（因此~mention detection的准确率），我们采用了基于学习的方法来剪枝。</li>\n<li>描述：我们用train data来定位和随后丢弃这些可能不能共指的候选mentions</li>\n<li>具体来说，对于每个在test数据集中，并在启发式剪枝中留下来的mention mk，我们计算它的<em>mention coreference probability</em>，表示mk的“head noun”与其他mention产生共指关系的可能性。如果这个可能性没有超过阈值tc，我们就将mk从候选mention列表中移除。Section4将讲解tc如何结合共指关系成分参数来优化共指评价方法，的时候共同学习（这句话是不是语序有问题）</li>\n<li>我们从train data中计算mk的<em>mention 从reference probability</em>评估。具体来说，因为OntoNotes里仅有“无-单数”是被注释的。因此我们可以用mk的“head noun”被注释次数（在gold mentions），由mk的“head noun”所有的出现次数（分离）。</li>\n<li>如果mk的“head noun”没有在train data中出现，我们就设定它的“mcp”为1.意味着我们让它从过滤器中先pass掉。换句话说，我们比较保守，并且对于不能计算出“mcp”的mention不做处理。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-3-Coreference-Resolution\"><a href=\"#Section-3-Coreference-Resolution\" class=\"headerlink\" title=\"Section 3. Coreference Resolution\"></a>Section 3. Coreference Resolution</h4><ul>\n<li>概述<ul>\n<li>像mention detection 组件中，我们的从reference resolution组件也采用了启发式和机器学习方法。</li>\n<li>我们使用了stanford的多轮共指消解方法（2011，Lee）来作为启发式共指消解。然而这些轮都是非词汇性的，我们用ML方法并结合词汇信息来增强多轮共指消解方法。</li>\n<li>尽管不同的轮是针对不同的语言使用的，但是我们吸收词汇信息后的方法每轮对于所有的语言是一样的（WTF？？？）</li>\n</ul>\n</li>\n<li>The Multi-Pass Sieve Approach<ul>\n<li>这轮由一个或者多个启发式规则组成。每个规则都基于一个或者多个条件，从两个mentions中提取一个共指关系。举个例子来说，一个在Standford的“discourse processing sieve”将按照以下规则判断两个mention是不是共指：<ol>\n<li>他们都是代词</li>\n<li>他们都是同一个speaker说的（生产的/制造的）</li>\n</ol>\n</li>\n<li>sieves由他们的准确率进行排序。（？）</li>\n<li>为了确定一个文件中的mention集合，决策器对它们用了多轮（Multi-Pass）：<ul>\n<li>在i-th pass中，仅用i-th中的规则对每个mention mk找一个先行词。当找这个先行词的时候，它的候选先行词被按一个顺序来访问，（这个顺序）通过它们在相关句法分析树的位置决定。</li>\n</ul>\n</li>\n<li>i-th中部分mentions的聚类，然后被传递（pass）给i+1-th pass。（人话，上次的聚类结果给下次用）</li>\n<li>因此，后面的passes可以充分利用前面的信息，但是之前确立的共指链不会比后面的更重要（？）</li>\n</ul>\n</li>\n<li><p>The Sieves </p>\n<ul>\n<li>For English<ul>\n<li>参照Standford的方法，采用12-sieves（Lee是13sieves，有一个是mention detection）</li>\n<li>因为我们加入了closed track，我们重复执行了10sieves，没有使用外部知识源。</li>\n<li>这10个sieves在Table2种list了。</li>\n<li>我们忽视了别名（Alias sieve）和词汇链（Lexical Chain sieve），这两个sieve用WordNet、WiKi和Freebase上的信息进行计算</li>\n</ul>\n</li>\n<li>For Chinese<ul>\n<li>概述：<ul>\n<li>我们参加了closed track 和open track</li>\n<li>这轮我们的应用对两个track都是相同的，除了我们对open track使用NE信息来改进某些系统。</li>\n<li>为了自动获取NE注释，我们使用了一个在train data中，通过gold NE注释训练出的<strong>NE模型</strong></li>\n</ul>\n</li>\n<li>中文决策器由<strong>9轮</strong>组成。(这里有个中英文对比的表格/Table2）</li>\n<li>这些sieves在基本上和英语参照相同的方式被执行，除了它们中的一小部分，（这一小部分）被修饰用以证明 一些独有的特性或者中文指代注释。（这句话有点儿难翻译）</li>\n<li>就像以下的详细介绍，（接下来会讲3个sieve）<ul>\n<li>我们<strong>介绍了一个新sieve</strong>，the Chinese Head Match sieve；</li>\n<li><strong>调整了两个现存的sieves</strong>,the Precise Constructs sieve, the Pronoun sieve</li>\n</ul>\n</li>\n<li><strong>Chinese Head Match sieve</strong><ul>\n<li>像Section2中说到的，一个mention和它的embedding mention能够称为共指关系是由于它们有相同头部。</li>\n<li>为了定位这些指代关系，我们执行了<strong>Same Head sieve</strong>，如果两个mention mj和mk有相同的Head并且mk is embedded within mj，（这个sieve）就假设两个mentions是共指的。</li>\n<li>这个规则有个<strong>例外</strong>，如果mj是一个由两个或者更多NP组成的同级（coordinate）NP，并且mk是这些NP之一，这两个mentions将不作为指代关系。</li>\n</ul>\n</li>\n<li><strong>Precise Constructs sieve</strong><ul>\n<li>像Lee2011中的，Precise Construct sieve 判断两个mentions是不是共指关系的方法是基于信息，（这个信息）比如说一个mention是不是另一个mention的首字母缩略词，或者说它们是不是来自一个同位语关系或者系表关系。</li>\n<li>我们给这个sieve吸收其他的规则来处理中文缩写中特殊的例子。——外国人名的缩写、中国人名的缩写</li>\n</ul>\n</li>\n<li><strong>Pronouns sieves</strong><ul>\n<li>这个sieve通过利用像mention的<em>性别、数量</em>这样的语法信息，解决了代词关系。</li>\n<li>这些语法信息主要针对英文，对于中文并不可信（not true）</li>\n<li>为了获取针对中文的像这样的语法信息，我们设计了一个简单的方法。如下三步：<ul>\n<li>首先我们设定了简单的启发式来从这些中文NP中提取容易推断的语法信息。举例来说，我们可以启发式地确定性别，数量，物种，比如“ 她【she】是「Female，single，Animate」”；比如“ 它们【they】是「未知的，复数，无生命的」”。                再加上我们能决定一个有命名实体信息的mention的语法特征。举个例子，一个PERSON能够有指定的语法特性，「未知，单数，动物」</li>\n<li>其次，我们仿真了这些有启发式地已确定的语法特征值。观察在一个共指链中所有的mentions，它们的性别，数量，物种应该统一。详细来说，给定一个train文本，如果在共指链的一个mention是启发式的被语法信息所标记，我们自动所有的还需处理的mentions注释相同的语法特征值。</li>\n<li>最后，我们自动创建了六单词列表（six word lists），包含「1，物种词语；2，无生命词语；3，男性词语；4，女性词语；5，单数词；6，复数词」。<ul>\n<li>Specifically,we populate these word lists with the grammatically annotated mentions from the previous step,where each element of a word list is composed of the head of a mention and a count indicating the number of time the mention is annotated with the corresponding grammatical attribute value.</li>\n</ul>\n</li>\n<li>然后我们可以在test文本上用这些词链来确定mentions的属性值。由于这些词链规模较小，而且我们的目的是增加准确率。如果这三个属性中的每一个(?)，一个mention有一个未知(<em>Unknown</em>)属性而其他的mentions有已知(<em>Known</em>)属性，我们就将两个mentions考虑为共指关系。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>总结<ul>\n<li>从Table2中可以看到，我们的中文系统没有<em>Relaxed String Match sieve</em>，不像英语有对应的。回顾这个sieve对两个mentions，如果跟在落下的文本的字符串紧跟着它们的head words是相同的，就把他们当作共指关系。（例子：<em>Michael Wolf</em>,and <em>Michael Wolf,a contributing editor for “New York”.</em>)</li>\n<li>由于中文人名经常有单个字符组成，并且heads很少跟在别的中文词语后面，我们相信<em>Relaxed Head Match</em>对于确定中文的共指关系没什么用。</li>\n<li>因此，中文人名缩写这样的将会交给<em>Precise Constructs sieve</em>处理 </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>For Arabic<ul>\n<li>仅用了一轮，the exact match sieve。因为用别的（上面的）效果不好</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Incorporating Lexical Information(结合词法信息)</p>\n<ul>\n<li>像之前提到的一样，我们结合词法信息增强了这个sieve。</li>\n<li>为了利用词法信息，我们首先计算了<em>词法概率</em>。具体来说，对于文本中的每个mention mj和mk，我们首先计算两个概率：<ol>\n<li><em>string-pair（SP-Prob）</em>概率，也就是包含两个mentions的字符串sj和sk是共指的概率。</li>\n<li><em>head-pair（HP-Prob）</em>概率，也就是两个mentions的head 名词hj和hk是共指的概率。</li>\n</ol>\n</li>\n<li>为了更好的评价，我们对training data和这两个mentions进行预处理：<ol>\n<li>对每个英文单词进行小写转换</li>\n<li>通过一个由    字符串形式对每个阿拉伯单词w进行替换，——<strong>这句话是讲处理阿拉伯语的，看不懂，略</strong></li>\n</ol>\n</li>\n<li>如果sj(hj)和sk(hk)没有在training数据集中出现的话，我们就将SP-Prob(mj,mk)(HP-Prob(mj,mk))标记为<em>Undefined</em></li>\n<li>然后，我们通过对sieve方法提出两种扩展，然后用这些词法概率来增强mj和mk的处理结果。<ul>\n<li>第一种扩展着力于增强sieve方法的<em>准确率</em>。具体来说，在进入任何sieve之前，我们需要确认SP-Prob(mj,mk)&lt;=tSPL 或者HP-Prob(mj,mk)&lt;=tHPL。如果是这样，我们的解析器将绕过所有的sieves，并且认为mj，mk不是共指的。<br>我们用了词汇概率来增进准确度，具体来说通过假设两个mentions不是共指的，如果在training data中有“sufficient/足够的”信息来让我们做出这个决定。如果两个概率都没有定义，那么这个mentions对儿在过滤器中留存，然后给sieve管道做后续处理。</li>\n<li>第二种扩展着力于增强sieve方法的<em>召回率</em>。具体来说，我们创建了一个新的sieve，叫<strong>”Lexical Pair sieve</strong>，将这个sieve添加到sieve管道的最后面，如果SP-Prob(mj,mk)&gt;=tSPU or HP-Prob(mj,mk)&gt;=tHPU，就假设mj和mk是共指的。<br>其他的类似第一种扩展，如果这两个概率都没有定义，那么这个sieve将不会处理这个mentions对儿。</li>\n<li>这四个阈值<strong>tSPL，tHPL；tSPU，tHPU</strong>，将在development 数据集上被优化</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-4-Parameter-Estimation（参数估计）\"><a href=\"#Section-4-Parameter-Estimation（参数估计）\" class=\"headerlink\" title=\"Section 4. Parameter Estimation（参数估计）\"></a>Section 4. Parameter Estimation（参数估计）</h4><p>就像之前讨论的一样，我们在development数据上训练系统的参数来优化消解性能。我们的系统有两套可调节的参数。目前为止，我们先看一套参数，命名为<em>五词汇概率阈值</em>，包含tc、tSPL、tHPL、tSPU、tHPU。 再看第二套参数包含<em>rule relaxation parameters</em>.</p><p><br>回顾一下，一个sieve中的每个规则都由一个或多个条件组成。我们联合条件 i 和参数 入i ，（这个参数）是一个控制条件i是否应该被去除的二值。特别的，如果 入i=0，条件i将被从相对应的规则中移除。</p>\n<p>缓和参数的目的应当明确：他们允许我们用机器学习的方法去优化 hand-craft 规则。这一节提出了在development数据集上得到参数的的两种算法。</p>\n<p>在讨论参数优化算法之前，先回顾一下简介。（简介是指）我们分辨特征的方法是我们建立了<em>genre-specific</em>分解器。换句话说，对每种语言的每个genre，我们</p>\n<pre><code>1. 从相应的training数据中，学习词汇概率\n2. 获取最理想的参数值O1，O2，各自/依次用算参数估计算法1和2给development数据估计\n3. O1和O2之一，选择其中可以在development数据集上更好表现的一个来作为最后参数估计的set（？）\n</code></pre><ul>\n<li><p>算法1</p>\n<ul>\n<li>pass</li>\n</ul>\n</li>\n<li><p>算法2</p>\n<ul>\n<li>pass</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-5-Results-and-Discussion\"><a href=\"#Section-5-Results-and-Discussion\" class=\"headerlink\" title=\"Section 5. Results and Discussion\"></a>Section 5. Results and Discussion</h4><p>完整的结果在table3中。</p>\n<ul>\n<li>mention detection results &amp; coreference results 由准确率/召回率/F值来描述。</li>\n<li>为了更好理解这两个参数sets的角色，我们展示了独立的实验。对每个语言-track</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h4 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h4><p>采用了一种混合方法来指代消解任务，结合了基于规则方法的优势和基于学习方法的优势。评分很高。</p>\n<h4 id=\"Section-1-Introduction\"><a href=\"#Section-1-Introduction\" class=\"headerlink\" title=\"Section 1. Introduction\"></a>Section 1. Introduction</h4><ul>\n<li>拓展了CoNLL2012中的任务，从单一语言→多语言。针对三种语言：英语、汉语、阿拉伯语。</li>\n<li>针对这三种语言都设计了系统</li>\n<li>Four tracks：<ul>\n<li>closed track：for three languages</li>\n<li>open track： for Chinese</li>\n</ul>\n</li>\n<li>有两个明显的特征：<ul>\n<li>往年的用的是基于规则的方法<strong>或</strong>基于学习的方法，我们这回将他们的优势结合起来了。</li>\n<li>往年的人没有开发“genre-specific”信息，我们对我们系统参数做出了优化，遵守了每个genre。</li>\n</ul>\n</li>\n<li>我们采用混合方法的原因是因为rule-based method 和learning-based method都有唯一的长处。</li>\n<li>根据上届CoNLL任务的方法，可以采用一个简单规则的公平小集合。</li>\n<li>我们先前的在用ml方法解决指代消解任务的工作，可以在启发词汇特征，优化XXXX  <strong>这段没懂</strong></li>\n<li>在本系统中，采用正常的架构来完成实体检测任务（在消解之前），‘期望评价方法’这两个组件被充分利用。</li>\n<li>本篇论文的布局安排：<ul>\n<li>mention detection conponent(Section2)</li>\n<li>coreference resolution component(Section3)</li>\n<li>show how their parameters are jointly optimized(Section4)/展示决定因素是如何优化的</li>\n<li>评估&amp;实验（Section5）</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-2-Mention-Detection\"><a href=\"#Section-2-Mention-Detection\" class=\"headerlink\" title=\"Section 2. Mention Detection\"></a>Section 2. Mention Detection</h4><ul>\n<li>为了保证准确率和召回率的平衡，我们采用了一种<strong>两步走</strong>的方法。——“extraction step” &amp; “pruning step”<ul>\n<li>首先，在<em>extraction step</em> 中，我们<strong>定位命名实体</strong>，<strong>采用启发式”language-specific”方法来提取mentions</strong>，以上工作从一个句法分析树，尽量增加召回率。</li>\n<li>然后，在<em>pruning step</em>中，我们<strong>采用启发式language-specific剪枝和基于学习的language-independent剪枝</strong>。</li>\n<li>总结，在step1中，定位实体，提取mention，增加召回率。在step2中，采用基于启发式和基于学习的剪枝</li>\n</ul>\n</li>\n<li>Heuristic Extraction and Pruning（启发式提取和剪枝）<ul>\n<li>英语：<ul>\n<li>在提取工作中，我们从相邻文本<strong>span s</strong>中创建候选mention，如果span s满足：<ol>\n<li>s是一个在句法分析树种的PRP或者NP</li>\n<li>s作为一个NE，但不是PERCENT，MONEY，QUANTITY或者CARDINAL</li>\n</ol>\n</li>\n<li>在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：<ol>\n<li>mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部</li>\n<li>mk有一个量词或者一个表示部分的修饰语</li>\n<li>mk是一个单数常见NP，我们保留与时间有关的mention（例如today）</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>中文<ul>\n<li>在提取工作中，我们从句法树中的所有NP，QP结点创建中文mention</li>\n<li>在剪枝工作中，我们将移除一个候选mention mk，如果mk满足：<ol>\n<li>mk夹在一个很大的mention mj中，并且这个mk和mj有共同的头部。除非如果mk和mj在一篇新闻通信中。不像其他的文件标注，中文新闻通信文档注释确实考虑了这些共指对。</li>\n<li>mk是一个NE，比如说PERCENT，MONEY，QUANTITY和CARDINAL</li>\n<li>mk是一个疑问代词，例如what，where</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>阿拉伯<ul>\n<li>略</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>Learning-Based Pruning（基于学习的剪枝）<ul>\n<li>当启发式方法定位了候选mention后，还不能确定这个候选mention就是可以共指的。为了增强剪枝效果（因此~mention detection的准确率），我们采用了基于学习的方法来剪枝。</li>\n<li>描述：我们用train data来定位和随后丢弃这些可能不能共指的候选mentions</li>\n<li>具体来说，对于每个在test数据集中，并在启发式剪枝中留下来的mention mk，我们计算它的<em>mention coreference probability</em>，表示mk的“head noun”与其他mention产生共指关系的可能性。如果这个可能性没有超过阈值tc，我们就将mk从候选mention列表中移除。Section4将讲解tc如何结合共指关系成分参数来优化共指评价方法，的时候共同学习（这句话是不是语序有问题）</li>\n<li>我们从train data中计算mk的<em>mention 从reference probability</em>评估。具体来说，因为OntoNotes里仅有“无-单数”是被注释的。因此我们可以用mk的“head noun”被注释次数（在gold mentions），由mk的“head noun”所有的出现次数（分离）。</li>\n<li>如果mk的“head noun”没有在train data中出现，我们就设定它的“mcp”为1.意味着我们让它从过滤器中先pass掉。换句话说，我们比较保守，并且对于不能计算出“mcp”的mention不做处理。</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-3-Coreference-Resolution\"><a href=\"#Section-3-Coreference-Resolution\" class=\"headerlink\" title=\"Section 3. Coreference Resolution\"></a>Section 3. Coreference Resolution</h4><ul>\n<li>概述<ul>\n<li>像mention detection 组件中，我们的从reference resolution组件也采用了启发式和机器学习方法。</li>\n<li>我们使用了stanford的多轮共指消解方法（2011，Lee）来作为启发式共指消解。然而这些轮都是非词汇性的，我们用ML方法并结合词汇信息来增强多轮共指消解方法。</li>\n<li>尽管不同的轮是针对不同的语言使用的，但是我们吸收词汇信息后的方法每轮对于所有的语言是一样的（WTF？？？）</li>\n</ul>\n</li>\n<li>The Multi-Pass Sieve Approach<ul>\n<li>这轮由一个或者多个启发式规则组成。每个规则都基于一个或者多个条件，从两个mentions中提取一个共指关系。举个例子来说，一个在Standford的“discourse processing sieve”将按照以下规则判断两个mention是不是共指：<ol>\n<li>他们都是代词</li>\n<li>他们都是同一个speaker说的（生产的/制造的）</li>\n</ol>\n</li>\n<li>sieves由他们的准确率进行排序。（？）</li>\n<li>为了确定一个文件中的mention集合，决策器对它们用了多轮（Multi-Pass）：<ul>\n<li>在i-th pass中，仅用i-th中的规则对每个mention mk找一个先行词。当找这个先行词的时候，它的候选先行词被按一个顺序来访问，（这个顺序）通过它们在相关句法分析树的位置决定。</li>\n</ul>\n</li>\n<li>i-th中部分mentions的聚类，然后被传递（pass）给i+1-th pass。（人话，上次的聚类结果给下次用）</li>\n<li>因此，后面的passes可以充分利用前面的信息，但是之前确立的共指链不会比后面的更重要（？）</li>\n</ul>\n</li>\n<li><p>The Sieves </p>\n<ul>\n<li>For English<ul>\n<li>参照Standford的方法，采用12-sieves（Lee是13sieves，有一个是mention detection）</li>\n<li>因为我们加入了closed track，我们重复执行了10sieves，没有使用外部知识源。</li>\n<li>这10个sieves在Table2种list了。</li>\n<li>我们忽视了别名（Alias sieve）和词汇链（Lexical Chain sieve），这两个sieve用WordNet、WiKi和Freebase上的信息进行计算</li>\n</ul>\n</li>\n<li>For Chinese<ul>\n<li>概述：<ul>\n<li>我们参加了closed track 和open track</li>\n<li>这轮我们的应用对两个track都是相同的，除了我们对open track使用NE信息来改进某些系统。</li>\n<li>为了自动获取NE注释，我们使用了一个在train data中，通过gold NE注释训练出的<strong>NE模型</strong></li>\n</ul>\n</li>\n<li>中文决策器由<strong>9轮</strong>组成。(这里有个中英文对比的表格/Table2）</li>\n<li>这些sieves在基本上和英语参照相同的方式被执行，除了它们中的一小部分，（这一小部分）被修饰用以证明 一些独有的特性或者中文指代注释。（这句话有点儿难翻译）</li>\n<li>就像以下的详细介绍，（接下来会讲3个sieve）<ul>\n<li>我们<strong>介绍了一个新sieve</strong>，the Chinese Head Match sieve；</li>\n<li><strong>调整了两个现存的sieves</strong>,the Precise Constructs sieve, the Pronoun sieve</li>\n</ul>\n</li>\n<li><strong>Chinese Head Match sieve</strong><ul>\n<li>像Section2中说到的，一个mention和它的embedding mention能够称为共指关系是由于它们有相同头部。</li>\n<li>为了定位这些指代关系，我们执行了<strong>Same Head sieve</strong>，如果两个mention mj和mk有相同的Head并且mk is embedded within mj，（这个sieve）就假设两个mentions是共指的。</li>\n<li>这个规则有个<strong>例外</strong>，如果mj是一个由两个或者更多NP组成的同级（coordinate）NP，并且mk是这些NP之一，这两个mentions将不作为指代关系。</li>\n</ul>\n</li>\n<li><strong>Precise Constructs sieve</strong><ul>\n<li>像Lee2011中的，Precise Construct sieve 判断两个mentions是不是共指关系的方法是基于信息，（这个信息）比如说一个mention是不是另一个mention的首字母缩略词，或者说它们是不是来自一个同位语关系或者系表关系。</li>\n<li>我们给这个sieve吸收其他的规则来处理中文缩写中特殊的例子。——外国人名的缩写、中国人名的缩写</li>\n</ul>\n</li>\n<li><strong>Pronouns sieves</strong><ul>\n<li>这个sieve通过利用像mention的<em>性别、数量</em>这样的语法信息，解决了代词关系。</li>\n<li>这些语法信息主要针对英文，对于中文并不可信（not true）</li>\n<li>为了获取针对中文的像这样的语法信息，我们设计了一个简单的方法。如下三步：<ul>\n<li>首先我们设定了简单的启发式来从这些中文NP中提取容易推断的语法信息。举例来说，我们可以启发式地确定性别，数量，物种，比如“ 她【she】是「Female，single，Animate」”；比如“ 它们【they】是「未知的，复数，无生命的」”。                再加上我们能决定一个有命名实体信息的mention的语法特征。举个例子，一个PERSON能够有指定的语法特性，「未知，单数，动物」</li>\n<li>其次，我们仿真了这些有启发式地已确定的语法特征值。观察在一个共指链中所有的mentions，它们的性别，数量，物种应该统一。详细来说，给定一个train文本，如果在共指链的一个mention是启发式的被语法信息所标记，我们自动所有的还需处理的mentions注释相同的语法特征值。</li>\n<li>最后，我们自动创建了六单词列表（six word lists），包含「1，物种词语；2，无生命词语；3，男性词语；4，女性词语；5，单数词；6，复数词」。<ul>\n<li>Specifically,we populate these word lists with the grammatically annotated mentions from the previous step,where each element of a word list is composed of the head of a mention and a count indicating the number of time the mention is annotated with the corresponding grammatical attribute value.</li>\n</ul>\n</li>\n<li>然后我们可以在test文本上用这些词链来确定mentions的属性值。由于这些词链规模较小，而且我们的目的是增加准确率。如果这三个属性中的每一个(?)，一个mention有一个未知(<em>Unknown</em>)属性而其他的mentions有已知(<em>Known</em>)属性，我们就将两个mentions考虑为共指关系。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>总结<ul>\n<li>从Table2中可以看到，我们的中文系统没有<em>Relaxed String Match sieve</em>，不像英语有对应的。回顾这个sieve对两个mentions，如果跟在落下的文本的字符串紧跟着它们的head words是相同的，就把他们当作共指关系。（例子：<em>Michael Wolf</em>,and <em>Michael Wolf,a contributing editor for “New York”.</em>)</li>\n<li>由于中文人名经常有单个字符组成，并且heads很少跟在别的中文词语后面，我们相信<em>Relaxed Head Match</em>对于确定中文的共指关系没什么用。</li>\n<li>因此，中文人名缩写这样的将会交给<em>Precise Constructs sieve</em>处理 </li>\n</ul>\n</li>\n</ul>\n</li>\n<li>For Arabic<ul>\n<li>仅用了一轮，the exact match sieve。因为用别的（上面的）效果不好</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>Incorporating Lexical Information(结合词法信息)</p>\n<ul>\n<li>像之前提到的一样，我们结合词法信息增强了这个sieve。</li>\n<li>为了利用词法信息，我们首先计算了<em>词法概率</em>。具体来说，对于文本中的每个mention mj和mk，我们首先计算两个概率：<ol>\n<li><em>string-pair（SP-Prob）</em>概率，也就是包含两个mentions的字符串sj和sk是共指的概率。</li>\n<li><em>head-pair（HP-Prob）</em>概率，也就是两个mentions的head 名词hj和hk是共指的概率。</li>\n</ol>\n</li>\n<li>为了更好的评价，我们对training data和这两个mentions进行预处理：<ol>\n<li>对每个英文单词进行小写转换</li>\n<li>通过一个由    字符串形式对每个阿拉伯单词w进行替换，——<strong>这句话是讲处理阿拉伯语的，看不懂，略</strong></li>\n</ol>\n</li>\n<li>如果sj(hj)和sk(hk)没有在training数据集中出现的话，我们就将SP-Prob(mj,mk)(HP-Prob(mj,mk))标记为<em>Undefined</em></li>\n<li>然后，我们通过对sieve方法提出两种扩展，然后用这些词法概率来增强mj和mk的处理结果。<ul>\n<li>第一种扩展着力于增强sieve方法的<em>准确率</em>。具体来说，在进入任何sieve之前，我们需要确认SP-Prob(mj,mk)&lt;=tSPL 或者HP-Prob(mj,mk)&lt;=tHPL。如果是这样，我们的解析器将绕过所有的sieves，并且认为mj，mk不是共指的。<br>我们用了词汇概率来增进准确度，具体来说通过假设两个mentions不是共指的，如果在training data中有“sufficient/足够的”信息来让我们做出这个决定。如果两个概率都没有定义，那么这个mentions对儿在过滤器中留存，然后给sieve管道做后续处理。</li>\n<li>第二种扩展着力于增强sieve方法的<em>召回率</em>。具体来说，我们创建了一个新的sieve，叫<strong>”Lexical Pair sieve</strong>，将这个sieve添加到sieve管道的最后面，如果SP-Prob(mj,mk)&gt;=tSPU or HP-Prob(mj,mk)&gt;=tHPU，就假设mj和mk是共指的。<br>其他的类似第一种扩展，如果这两个概率都没有定义，那么这个sieve将不会处理这个mentions对儿。</li>\n<li>这四个阈值<strong>tSPL，tHPL；tSPU，tHPU</strong>，将在development 数据集上被优化</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-4-Parameter-Estimation（参数估计）\"><a href=\"#Section-4-Parameter-Estimation（参数估计）\" class=\"headerlink\" title=\"Section 4. Parameter Estimation（参数估计）\"></a>Section 4. Parameter Estimation（参数估计）</h4><p>就像之前讨论的一样，我们在development数据上训练系统的参数来优化消解性能。我们的系统有两套可调节的参数。目前为止，我们先看一套参数，命名为<em>五词汇概率阈值</em>，包含tc、tSPL、tHPL、tSPU、tHPU。 再看第二套参数包含<em>rule relaxation parameters</em>.</p><p><br>回顾一下，一个sieve中的每个规则都由一个或多个条件组成。我们联合条件 i 和参数 入i ，（这个参数）是一个控制条件i是否应该被去除的二值。特别的，如果 入i=0，条件i将被从相对应的规则中移除。</p>\n<p>缓和参数的目的应当明确：他们允许我们用机器学习的方法去优化 hand-craft 规则。这一节提出了在development数据集上得到参数的的两种算法。</p>\n<p>在讨论参数优化算法之前，先回顾一下简介。（简介是指）我们分辨特征的方法是我们建立了<em>genre-specific</em>分解器。换句话说，对每种语言的每个genre，我们</p>\n<pre><code>1. 从相应的training数据中，学习词汇概率\n2. 获取最理想的参数值O1，O2，各自/依次用算参数估计算法1和2给development数据估计\n3. O1和O2之一，选择其中可以在development数据集上更好表现的一个来作为最后参数估计的set（？）\n</code></pre><ul>\n<li><p>算法1</p>\n<ul>\n<li>pass</li>\n</ul>\n</li>\n<li><p>算法2</p>\n<ul>\n<li>pass</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"Section-5-Results-and-Discussion\"><a href=\"#Section-5-Results-and-Discussion\" class=\"headerlink\" title=\"Section 5. Results and Discussion\"></a>Section 5. Results and Discussion</h4><p>完整的结果在table3中。</p>\n<ul>\n<li>mention detection results &amp; coreference results 由准确率/召回率/F值来描述。</li>\n<li>为了更好理解这两个参数sets的角色，我们展示了独立的实验。对每个语言-track</li>\n</ul>\n"},{"title":"组会分享","date":"2018-04-23T16:00:00.000Z","description":"组会相关内容分享","top":1,"_content":"\n\n## 目录\n\n1. [一些概念](#gainian)\n    1. [指代概念](#01)\n    2. [表述与实体](#mention)\n    3. [消解任务](#mission)\n2. [各类方法](#function)\n    1. [表述提取](#md)\n        - [利用句法树](#parse)\n        - [利用神经网络](#nn)\n    2. [消解](#cr)\n        - [语言学](#language)\n        - [机器学习](#ml)\n3. [CR——Multi sieve](#multisieve)\n    1. [想法](#idea)\n    2. [表述提取](#ms-md)\n    3. [消解](#ms-cr/)\n4. [CR——End2End](#end2end)\n    1. [目标任务](#e2e-mission)\n    2. [论文方法](#e2e-function)\n    3. [讨论](#answer)\n        - [优点](#strength)\n        - [缺点](#weakness)\n        - [改进](#improved)\n    4. [实验](#experiment)\n5. [可能有用的网址](#url)\n6. [私货](#recommend)\n\n\n---\n\n### <span id=\"gainian\">一、 一些概念 </span>\n\n#### <span id=\"01\">1. 指代概念\n\n指代（coreference）为语言学中为了避免已经出现的字词重复出现在文章的句子上，导致语句结构过于赘述和语意不够清晰，所以使用代词（pronouns）或是普通名词（common nouns）来代替已经出现过的字词。\n\n主要分为两类：**回指**和**共指**\n\n- 回指：指句子中出现的短语存在着密切的语义关联性，指代同一事物。这种关系依赖上下文语义。其中被指向的语言单位被称为先行语\n- 共指：指句子中出现的短语指向真实世界的同一参照体，这种指代脱离上下文仍然成立。\n\n    ```\n    例如：\n    回指：“小明很帅，同学们都很喜欢他。”\n    共指：“特朗普就任总统以来第一次访问中国，他也将成为历史上第8位在任时访华的美国总统。”\n    \n    句子一中，'他'指代'小明'，'小明'是'他'的先行语。\n    句子二中，'特朗普'和'美国总统'均指代在美国白宫坐着的那个人类。\n    ```\n\n<p>\n\n#### <span id=\"mention\">2. 表述与实体\n\n实体（Entity）是存在于客观世界的某种事物，是一种切实存在的概念。文本中会出现很多的名词短语以及代词，这些句子成分被称作表述（Mention），是一个事物在文本中的一种体现。\n> In coreference resolution, an entity is an object or set of objects in the world, while a mention is the textual reference to an entity (Doddington et al., 2004). \n\n![image](/组会分享/mention_and_entity.png)\n\n#### <span id=\"mission\">3. 任务描述\n    \n指代消解定义：为文中的表述确定其在真实世界中所指向实体的过程。\n> Coreference resolution, the task of identifying which mentions in a text refer to the same real world entity, is fundamentally a clustering problem. \n\n研究步骤：\n1. Mention detection/表述提取——对于一段文本，抽取其中的表述。\n2. Coreference Resolution/指代消解——对上一步得到的表述进行分析，确定它们之间是否存在指代关系。\n\n### <span id=\"function\">二. 各类方法</span>\n\n#### <span id=\"md\">1. 表述提取</span>\n- <span id=\"parse\">利用句法分析树</span>\n\n    对输入的文本进行分词、词性标注等工作，以句子为单位构建句法树。对产生的句法树进行分析，获取其中的各个子树。选取所有的根节点为名词短语（或代词）的子树，抽取其中的词语组合成候选表述\n\t\n\t输入：\n\t\n    > (TOP(IP(NP(QP0) (NP1) (NP2)) (VP(PP3 (IP(VP(ADVP4) (VP5 (NP6))))) (VP7 (NP8))) 9))\n    \n    输出：\n    \n    > 0、1、01、5、45、8\n\n    ![image](/组会分享/tree.png)\n\t\n- <span id=\"nn\">利用神经网络\n    \n\t对输入的文本进行分词工作，将每个词语作为一个基本单位进行保存。通过词嵌入（word embedding）构建包含更多词信息的词向量，然后输入双向LSTM网络获取对应的候选表述。（详见后文）\n\n\n#### <span id=\"cr\">2. 消解\n- <span id=\"language\">语言学方法\n\t- Hobbs算法是最早的代词消解方法之一，主要依赖于句法解析树进行分析。\n\t- Grosz提出并发展起来的中心理论主要针对篇章结构中的焦点转移以及话语一致性等问题，常用于代词消解研究。\n    - Raghunathan(2010)针对英文数据集构建了一种多轮迭代(MultiSieves)处理的方法，该方法根据准确率由高到低构建了多个筛子（sieve）用来为文本中的表述选取先行语。[Multi-sieves论文：http://www.aclweb.org/anthology/D10-1048](http://www.aclweb.org/anthology/D10-1048)\n\n- <span id=\"ml\">机器学习方法\n\t- 常见的四类框架包括实体-表述模型、表述对模型、表述排序模型和实体排序模型。\n\t- 其中表述对模型是最常见的模型之一，该模型将消解问题看作是表述对之间的二分类问题，判断系统当前处理的两个表述之间是否存在着共指关系。\n\t- 人们在此基础上进行修改，提出了实体-表述模型和表述排序模型。\n\t- 为了组合实体-表述模型和表述排序模型的优势，提出了实体排序模型。\n\t- 关于表述排序模型(Mention-Ranking)可以参考：[Improving Coreference Resolution by Learning Entity-Level Distributed](https://nlp.stanford.edu/pubs/clark2016improving.pdf)\n\n### <span id=\"multisieve\">三. CR——MultiSieves</span>\n\t\n#### 0. <span id=\"idea\">想法\n\n通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。\n\n参考论文：\n- Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.\n- Lee H, Peirsman Y, Chang A, et al. Stanford's multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.\n- Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.\n- Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.\n\n#### 1. <span id=\"ms-md\">Mention Detection\n    \n使用了Stanford的工具包对文本进行预处理\n\n- https://github.com/Lynten/stanford-corenlp\n- https://stanfordnlp.github.io/CoreNLP/#download\n- https://stanfordnlp.github.io/CoreNLP/history.html\n\n**使用示例：**\n\n![image](/组会分享/clipboard.png)\n![image](/组会分享/clipboard_1.png)\n![image](/组会分享/clipboard_2.png)\n\n输入\n> “小明很帅，同学们都很喜欢他”\n\n得到：\n> 明 小明 很帅 小明很帅 同学们 他\n\n#### 2. <span id=\"ms-cr\">Coreference Resolution\n\n- 主要想法：\n通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。\n\n- 参考论文：\n    - Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.\n    - Lee H, Peirsman Y, Chang A, et al. Stanford's multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.\n    - Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.\n    - Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.\n- 常用的sieves\n    ```\n    1. Mention Detection Sieve \n    2. Discourse Processing Sieve\n    3. Exact String Match Sieve \n    4. Relaxed String Match Sieve \n    5. Precise Constructs Sieve \n    6-8. Strict Head Matching Sieves A-C\n    9. Proper HeadWord Match Sieve \n    10. Alias Sieve \n    11. Relaxed Head Matching Sieve \n    12. Lexical Chain Sieve \n    13. Pronouns Sieve\n    ```\n\n### 4. <span id=\"end2end\">CR——End2End</span>\n\n论文名城：End-to-end Neural Coreference Resolution\n\n#### 1. <span id=\"e2e-mission\">目标任务\n\n对于指代消解的任务定义为：\n给定一段文本，找到其中名词/代词，判断这些词语是否存在着指代关系。\n\n因此给定一个包含T个词语的文档D，通过某种方法从中得到N个span(即将这些span作为潜在的表述)。==论文目标是对于每个span 𝑖确定一个先行语y_𝑖==\n\n𝑦_𝑖可能的取值集合: 𝒴(𝑖)={𝜖,1,…,𝑖−1}\n𝜖：表示当前span没有先行语\n\n其中span是由几个单词组成的一个单元，可以理解为一个短语/词组\n\n#### 2. <span id=\"e2e-function\">论文方法\n![image](/组会分享/clipboard_4.png)\n\n顺序 | 任务 | 得到\n---|---|---|\n1 | 对于每个单词，使用300维GloVe embedding和50维Turian embedding构成向量，进行正则化为单元向量。对于每个字符，通过一个卷积网络得到一个8维向量进行表示 | word & character embedding 𝒙\n2 | 将上一步中的向量表示输入双向LSTM网络，对文本中每个词进行编码和计算，得到span 𝑖的边界的表示向量x_(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和x_(𝐸𝑁𝐷(𝑖))^∗ | 𝒙_(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和𝒙_(𝐸𝑁𝐷(𝑖))^∗\n3 | 利用attention机制求取每个候选span的中心词𝑥 ̂_𝑖| span 𝑖 的中心词𝒙 ̂_𝒊\n4 | 构建对于span的向量表示：𝑔_𝑖=[x_𝑆𝑇𝐴𝑅𝑇(𝑖)^ ∗,x_𝐸𝑁𝐷(𝑖)^∗,𝑥 ̂_𝑖,∅(𝑖)] | span的向量表示𝑔_𝑖\n\n![image](/组会分享/clipboard_5.png)\n\n顺序 | 任务 | 得到\n---|---|---|\n1 | 对于每个span 𝑖，计算𝑠_𝑚，其中s_𝑚=𝜔_𝑚∙[𝐹𝐹𝑁𝑁]_𝑚 (𝑔_𝑖) FFNN为一个非线性映射的前馈神经网络。s_𝑚的意义在于对span空间进行剪枝，删去过多的表述。 | 𝑠_𝑚\n2 | 对于span 𝑖，𝑗，计算𝑠_a (𝑖,𝑗)，其中𝑠_a (𝑖,𝑗)=𝜔_𝑎∙[𝐹𝐹𝑁𝑁]_𝑎 ([𝑔_𝑖,𝑔_𝑗,𝑔_𝑖°𝑔_𝑗,∅(𝑖,𝑗)])，s_𝑎 的意义即span 𝑗是span 𝑖的先行词的score | 𝑠_a (𝑖,𝑗)\n3 | 计算S(𝑖,𝑗) | S(𝑖,𝑗)\n\n#### 小结—计算流程\n1. word representation 𝒙\n1. LSTM 𝑥^∗\n1. head 𝑥 ̂_𝑖\n1. span representation 𝑔_𝑖\n1. Mention score s_𝑚(pruning)\n1. Antecedent score 𝑠_a (𝑖,𝑗)\n1. Coreference score s(𝑖,𝑗)\n\n\n#### 3. <span id=\"answer\">讨论\n\n```\n- 以前方法的缺陷\n    在之前的各类方法中，大部分是依赖语法分析树和手工构建的各种特征。\n    存在以下问题：\n        1. 句法分析器的错误可能导致一连串的错误\n        2. 许多手工构建的规则对于新的语言或任务不具有普适性\n- 本论文不依赖于句法分析器，不依赖于手工构建的各类规则\n\n```\n\n<span id=\"strength\">**优点**</span>\n\n1. 本文的模型可以有效提取较长和复杂的词语作为潜在的表述\n2. Attention机制在确定指代关系的工作中有重要作用（通过查找中心词）\n3. 由双向LSTM提供的上下文信息在确定指代关系中也起到了重要作用\n\n \n<span id=\"weakness\">**缺点**</span>\n\n1. 论文可以利用embeedding计算词语之间的相似度，这是基于传统的特征的系统所不能解决的。但是有时在embedding相似的情况下，但是语义并不相近。\n2. 本模型缺少关于现实世界的先验知识\n\n\n<span id=\"improved\">**改进**</span>\n- 将模型与外部世界的知识相结合\n- 用更大量的训练数据克服这些缺陷\n\n\n### 5. <span id=\"urls\">可能有用的Urls\n\n网址 | 备注\n---|---\nhttps://github.com/Lynten/stanford-coren | Stanfordcorenlp项目地址\nhttps://stanfordnlp.github.io/CoreNLP/#download | 最新版CoreNLP模型下载地址\nhttps://stanfordnlp.github.io/CoreNLP/history.html | CoreNLP模型历史版本下载地址\nhttp://allennlp.org/ | 一个基于PyTorch的新的开源NLP研究库。 \nhttps://github.com/kentonl/e2e-coref | End2End CR论文的项目地址\n\n#### 6. <span id=\"recommend\">私货<span>\n\n- [https://weibo.com/52nlp](https://weibo.com/52nlp)\n- [https://weibo.com/u/1657470871](https://weibo.com/u/1657470871)\n- [https://weibo.com/paperweekly](https://weibo.com/paperweekly)\n- [https://weibo.com/fly51fly](https://weibo.com/fly51fly)\n- [https://weibo.com/jietangthu](https://weibo.com/jietangthu)\n- [https://weibo.com/u/1970879995](https://weibo.com/u/1970879995)\n- [https://weibo.com/zibuyu9](https://weibo.com/zibuyu9)\n- [https://weibo.com/clnlp](https://weibo.com/clnlp)\n- [https://weibo.com/nlpjob](https://weibo.com/nlpjob)\n- [https://weibo.com/arnetminer](https://weibo.com/arnetminer)\n- [http://blog.knownsec.com/Knownsec_RD_Checklist/index.html](http://blog.knownsec.com/Knownsec_RD_Checklist/index.html)\n- [https://github.com/dongxiexidian/Chinese](https://github.com/dongxiexidian/Chinese)\n- [https://github.com/justjavac/free-programming-books-zh_CN](https://github.com/justjavac/free-programming-books-zh_CN)\n- [https://www.jianshu.com/u/ZQtGe6](https://www.jianshu.com/u/ZQtGe6)","source":"_posts/组会分享.md","raw":"---\ntitle: 组会分享\ndate: 2018-4-24\ntags:\n    - CR\n    - NLP\ndescription: 组会相关内容分享\ntop: 1\n---\n\n\n## 目录\n\n1. [一些概念](#gainian)\n    1. [指代概念](#01)\n    2. [表述与实体](#mention)\n    3. [消解任务](#mission)\n2. [各类方法](#function)\n    1. [表述提取](#md)\n        - [利用句法树](#parse)\n        - [利用神经网络](#nn)\n    2. [消解](#cr)\n        - [语言学](#language)\n        - [机器学习](#ml)\n3. [CR——Multi sieve](#multisieve)\n    1. [想法](#idea)\n    2. [表述提取](#ms-md)\n    3. [消解](#ms-cr/)\n4. [CR——End2End](#end2end)\n    1. [目标任务](#e2e-mission)\n    2. [论文方法](#e2e-function)\n    3. [讨论](#answer)\n        - [优点](#strength)\n        - [缺点](#weakness)\n        - [改进](#improved)\n    4. [实验](#experiment)\n5. [可能有用的网址](#url)\n6. [私货](#recommend)\n\n\n---\n\n### <span id=\"gainian\">一、 一些概念 </span>\n\n#### <span id=\"01\">1. 指代概念\n\n指代（coreference）为语言学中为了避免已经出现的字词重复出现在文章的句子上，导致语句结构过于赘述和语意不够清晰，所以使用代词（pronouns）或是普通名词（common nouns）来代替已经出现过的字词。\n\n主要分为两类：**回指**和**共指**\n\n- 回指：指句子中出现的短语存在着密切的语义关联性，指代同一事物。这种关系依赖上下文语义。其中被指向的语言单位被称为先行语\n- 共指：指句子中出现的短语指向真实世界的同一参照体，这种指代脱离上下文仍然成立。\n\n    ```\n    例如：\n    回指：“小明很帅，同学们都很喜欢他。”\n    共指：“特朗普就任总统以来第一次访问中国，他也将成为历史上第8位在任时访华的美国总统。”\n    \n    句子一中，'他'指代'小明'，'小明'是'他'的先行语。\n    句子二中，'特朗普'和'美国总统'均指代在美国白宫坐着的那个人类。\n    ```\n\n<p>\n\n#### <span id=\"mention\">2. 表述与实体\n\n实体（Entity）是存在于客观世界的某种事物，是一种切实存在的概念。文本中会出现很多的名词短语以及代词，这些句子成分被称作表述（Mention），是一个事物在文本中的一种体现。\n> In coreference resolution, an entity is an object or set of objects in the world, while a mention is the textual reference to an entity (Doddington et al., 2004). \n\n![image](/组会分享/mention_and_entity.png)\n\n#### <span id=\"mission\">3. 任务描述\n    \n指代消解定义：为文中的表述确定其在真实世界中所指向实体的过程。\n> Coreference resolution, the task of identifying which mentions in a text refer to the same real world entity, is fundamentally a clustering problem. \n\n研究步骤：\n1. Mention detection/表述提取——对于一段文本，抽取其中的表述。\n2. Coreference Resolution/指代消解——对上一步得到的表述进行分析，确定它们之间是否存在指代关系。\n\n### <span id=\"function\">二. 各类方法</span>\n\n#### <span id=\"md\">1. 表述提取</span>\n- <span id=\"parse\">利用句法分析树</span>\n\n    对输入的文本进行分词、词性标注等工作，以句子为单位构建句法树。对产生的句法树进行分析，获取其中的各个子树。选取所有的根节点为名词短语（或代词）的子树，抽取其中的词语组合成候选表述\n\t\n\t输入：\n\t\n    > (TOP(IP(NP(QP0) (NP1) (NP2)) (VP(PP3 (IP(VP(ADVP4) (VP5 (NP6))))) (VP7 (NP8))) 9))\n    \n    输出：\n    \n    > 0、1、01、5、45、8\n\n    ![image](/组会分享/tree.png)\n\t\n- <span id=\"nn\">利用神经网络\n    \n\t对输入的文本进行分词工作，将每个词语作为一个基本单位进行保存。通过词嵌入（word embedding）构建包含更多词信息的词向量，然后输入双向LSTM网络获取对应的候选表述。（详见后文）\n\n\n#### <span id=\"cr\">2. 消解\n- <span id=\"language\">语言学方法\n\t- Hobbs算法是最早的代词消解方法之一，主要依赖于句法解析树进行分析。\n\t- Grosz提出并发展起来的中心理论主要针对篇章结构中的焦点转移以及话语一致性等问题，常用于代词消解研究。\n    - Raghunathan(2010)针对英文数据集构建了一种多轮迭代(MultiSieves)处理的方法，该方法根据准确率由高到低构建了多个筛子（sieve）用来为文本中的表述选取先行语。[Multi-sieves论文：http://www.aclweb.org/anthology/D10-1048](http://www.aclweb.org/anthology/D10-1048)\n\n- <span id=\"ml\">机器学习方法\n\t- 常见的四类框架包括实体-表述模型、表述对模型、表述排序模型和实体排序模型。\n\t- 其中表述对模型是最常见的模型之一，该模型将消解问题看作是表述对之间的二分类问题，判断系统当前处理的两个表述之间是否存在着共指关系。\n\t- 人们在此基础上进行修改，提出了实体-表述模型和表述排序模型。\n\t- 为了组合实体-表述模型和表述排序模型的优势，提出了实体排序模型。\n\t- 关于表述排序模型(Mention-Ranking)可以参考：[Improving Coreference Resolution by Learning Entity-Level Distributed](https://nlp.stanford.edu/pubs/clark2016improving.pdf)\n\n### <span id=\"multisieve\">三. CR——MultiSieves</span>\n\t\n#### 0. <span id=\"idea\">想法\n\n通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。\n\n参考论文：\n- Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.\n- Lee H, Peirsman Y, Chang A, et al. Stanford's multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.\n- Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.\n- Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.\n\n#### 1. <span id=\"ms-md\">Mention Detection\n    \n使用了Stanford的工具包对文本进行预处理\n\n- https://github.com/Lynten/stanford-corenlp\n- https://stanfordnlp.github.io/CoreNLP/#download\n- https://stanfordnlp.github.io/CoreNLP/history.html\n\n**使用示例：**\n\n![image](/组会分享/clipboard.png)\n![image](/组会分享/clipboard_1.png)\n![image](/组会分享/clipboard_2.png)\n\n输入\n> “小明很帅，同学们都很喜欢他”\n\n得到：\n> 明 小明 很帅 小明很帅 同学们 他\n\n#### 2. <span id=\"ms-cr\">Coreference Resolution\n\n- 主要想法：\n通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。\n\n- 参考论文：\n    - Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.\n    - Lee H, Peirsman Y, Chang A, et al. Stanford's multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.\n    - Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.\n    - Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.\n- 常用的sieves\n    ```\n    1. Mention Detection Sieve \n    2. Discourse Processing Sieve\n    3. Exact String Match Sieve \n    4. Relaxed String Match Sieve \n    5. Precise Constructs Sieve \n    6-8. Strict Head Matching Sieves A-C\n    9. Proper HeadWord Match Sieve \n    10. Alias Sieve \n    11. Relaxed Head Matching Sieve \n    12. Lexical Chain Sieve \n    13. Pronouns Sieve\n    ```\n\n### 4. <span id=\"end2end\">CR——End2End</span>\n\n论文名城：End-to-end Neural Coreference Resolution\n\n#### 1. <span id=\"e2e-mission\">目标任务\n\n对于指代消解的任务定义为：\n给定一段文本，找到其中名词/代词，判断这些词语是否存在着指代关系。\n\n因此给定一个包含T个词语的文档D，通过某种方法从中得到N个span(即将这些span作为潜在的表述)。==论文目标是对于每个span 𝑖确定一个先行语y_𝑖==\n\n𝑦_𝑖可能的取值集合: 𝒴(𝑖)={𝜖,1,…,𝑖−1}\n𝜖：表示当前span没有先行语\n\n其中span是由几个单词组成的一个单元，可以理解为一个短语/词组\n\n#### 2. <span id=\"e2e-function\">论文方法\n![image](/组会分享/clipboard_4.png)\n\n顺序 | 任务 | 得到\n---|---|---|\n1 | 对于每个单词，使用300维GloVe embedding和50维Turian embedding构成向量，进行正则化为单元向量。对于每个字符，通过一个卷积网络得到一个8维向量进行表示 | word & character embedding 𝒙\n2 | 将上一步中的向量表示输入双向LSTM网络，对文本中每个词进行编码和计算，得到span 𝑖的边界的表示向量x_(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和x_(𝐸𝑁𝐷(𝑖))^∗ | 𝒙_(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和𝒙_(𝐸𝑁𝐷(𝑖))^∗\n3 | 利用attention机制求取每个候选span的中心词𝑥 ̂_𝑖| span 𝑖 的中心词𝒙 ̂_𝒊\n4 | 构建对于span的向量表示：𝑔_𝑖=[x_𝑆𝑇𝐴𝑅𝑇(𝑖)^ ∗,x_𝐸𝑁𝐷(𝑖)^∗,𝑥 ̂_𝑖,∅(𝑖)] | span的向量表示𝑔_𝑖\n\n![image](/组会分享/clipboard_5.png)\n\n顺序 | 任务 | 得到\n---|---|---|\n1 | 对于每个span 𝑖，计算𝑠_𝑚，其中s_𝑚=𝜔_𝑚∙[𝐹𝐹𝑁𝑁]_𝑚 (𝑔_𝑖) FFNN为一个非线性映射的前馈神经网络。s_𝑚的意义在于对span空间进行剪枝，删去过多的表述。 | 𝑠_𝑚\n2 | 对于span 𝑖，𝑗，计算𝑠_a (𝑖,𝑗)，其中𝑠_a (𝑖,𝑗)=𝜔_𝑎∙[𝐹𝐹𝑁𝑁]_𝑎 ([𝑔_𝑖,𝑔_𝑗,𝑔_𝑖°𝑔_𝑗,∅(𝑖,𝑗)])，s_𝑎 的意义即span 𝑗是span 𝑖的先行词的score | 𝑠_a (𝑖,𝑗)\n3 | 计算S(𝑖,𝑗) | S(𝑖,𝑗)\n\n#### 小结—计算流程\n1. word representation 𝒙\n1. LSTM 𝑥^∗\n1. head 𝑥 ̂_𝑖\n1. span representation 𝑔_𝑖\n1. Mention score s_𝑚(pruning)\n1. Antecedent score 𝑠_a (𝑖,𝑗)\n1. Coreference score s(𝑖,𝑗)\n\n\n#### 3. <span id=\"answer\">讨论\n\n```\n- 以前方法的缺陷\n    在之前的各类方法中，大部分是依赖语法分析树和手工构建的各种特征。\n    存在以下问题：\n        1. 句法分析器的错误可能导致一连串的错误\n        2. 许多手工构建的规则对于新的语言或任务不具有普适性\n- 本论文不依赖于句法分析器，不依赖于手工构建的各类规则\n\n```\n\n<span id=\"strength\">**优点**</span>\n\n1. 本文的模型可以有效提取较长和复杂的词语作为潜在的表述\n2. Attention机制在确定指代关系的工作中有重要作用（通过查找中心词）\n3. 由双向LSTM提供的上下文信息在确定指代关系中也起到了重要作用\n\n \n<span id=\"weakness\">**缺点**</span>\n\n1. 论文可以利用embeedding计算词语之间的相似度，这是基于传统的特征的系统所不能解决的。但是有时在embedding相似的情况下，但是语义并不相近。\n2. 本模型缺少关于现实世界的先验知识\n\n\n<span id=\"improved\">**改进**</span>\n- 将模型与外部世界的知识相结合\n- 用更大量的训练数据克服这些缺陷\n\n\n### 5. <span id=\"urls\">可能有用的Urls\n\n网址 | 备注\n---|---\nhttps://github.com/Lynten/stanford-coren | Stanfordcorenlp项目地址\nhttps://stanfordnlp.github.io/CoreNLP/#download | 最新版CoreNLP模型下载地址\nhttps://stanfordnlp.github.io/CoreNLP/history.html | CoreNLP模型历史版本下载地址\nhttp://allennlp.org/ | 一个基于PyTorch的新的开源NLP研究库。 \nhttps://github.com/kentonl/e2e-coref | End2End CR论文的项目地址\n\n#### 6. <span id=\"recommend\">私货<span>\n\n- [https://weibo.com/52nlp](https://weibo.com/52nlp)\n- [https://weibo.com/u/1657470871](https://weibo.com/u/1657470871)\n- [https://weibo.com/paperweekly](https://weibo.com/paperweekly)\n- [https://weibo.com/fly51fly](https://weibo.com/fly51fly)\n- [https://weibo.com/jietangthu](https://weibo.com/jietangthu)\n- [https://weibo.com/u/1970879995](https://weibo.com/u/1970879995)\n- [https://weibo.com/zibuyu9](https://weibo.com/zibuyu9)\n- [https://weibo.com/clnlp](https://weibo.com/clnlp)\n- [https://weibo.com/nlpjob](https://weibo.com/nlpjob)\n- [https://weibo.com/arnetminer](https://weibo.com/arnetminer)\n- [http://blog.knownsec.com/Knownsec_RD_Checklist/index.html](http://blog.knownsec.com/Knownsec_RD_Checklist/index.html)\n- [https://github.com/dongxiexidian/Chinese](https://github.com/dongxiexidian/Chinese)\n- [https://github.com/justjavac/free-programming-books-zh_CN](https://github.com/justjavac/free-programming-books-zh_CN)\n- [https://www.jianshu.com/u/ZQtGe6](https://www.jianshu.com/u/ZQtGe6)","slug":"组会分享","published":1,"updated":"2019-01-22T04:59:24.956Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjr7ak2od001gscftxounljbz","content":"<h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h2><ol>\n<li><a href=\"#gainian\">一些概念</a><ol>\n<li><a href=\"#01\">指代概念</a></li>\n<li><a href=\"#mention\">表述与实体</a></li>\n<li><a href=\"#mission\">消解任务</a></li>\n</ol>\n</li>\n<li><a href=\"#function\">各类方法</a><ol>\n<li><a href=\"#md\">表述提取</a><ul>\n<li><a href=\"#parse\">利用句法树</a></li>\n<li><a href=\"#nn\">利用神经网络</a></li>\n</ul>\n</li>\n<li><a href=\"#cr\">消解</a><ul>\n<li><a href=\"#language\">语言学</a></li>\n<li><a href=\"#ml\">机器学习</a></li>\n</ul>\n</li>\n</ol>\n</li>\n<li><a href=\"#multisieve\">CR——Multi sieve</a><ol>\n<li><a href=\"#idea\">想法</a></li>\n<li><a href=\"#ms-md\">表述提取</a></li>\n<li><a href=\"#ms-cr/\">消解</a></li>\n</ol>\n</li>\n<li><a href=\"#end2end\">CR——End2End</a><ol>\n<li><a href=\"#e2e-mission\">目标任务</a></li>\n<li><a href=\"#e2e-function\">论文方法</a></li>\n<li><a href=\"#answer\">讨论</a><ul>\n<li><a href=\"#strength\">优点</a></li>\n<li><a href=\"#weakness\">缺点</a></li>\n<li><a href=\"#improved\">改进</a></li>\n</ul>\n</li>\n<li><a href=\"#experiment\">实验</a></li>\n</ol>\n</li>\n<li><a href=\"#url\">可能有用的网址</a></li>\n<li><a href=\"#recommend\">私货</a></li>\n</ol>\n<hr>\n<h3 id=\"一、-一些概念\"><a href=\"#一、-一些概念\" class=\"headerlink\" title=\"一、 一些概念 \"></a><span id=\"gainian\">一、 一些概念 </span></h3><h4 id=\"1-指代概念\"><a href=\"#1-指代概念\" class=\"headerlink\" title=\"1. 指代概念\"></a><span id=\"01\">1. 指代概念</span></h4><p>指代（coreference）为语言学中为了避免已经出现的字词重复出现在文章的句子上，导致语句结构过于赘述和语意不够清晰，所以使用代词（pronouns）或是普通名词（common nouns）来代替已经出现过的字词。</p>\n<p>主要分为两类：<strong>回指</strong>和<strong>共指</strong></p>\n<ul>\n<li>回指：指句子中出现的短语存在着密切的语义关联性，指代同一事物。这种关系依赖上下文语义。其中被指向的语言单位被称为先行语</li>\n<li><p>共指：指句子中出现的短语指向真实世界的同一参照体，这种指代脱离上下文仍然成立。</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">例如：</span><br><span class=\"line\">回指：“小明很帅，同学们都很喜欢他。”</span><br><span class=\"line\">共指：“特朗普就任总统以来第一次访问中国，他也将成为历史上第8位在任时访华的美国总统。”</span><br><span class=\"line\"></span><br><span class=\"line\">句子一中，&apos;他&apos;指代&apos;小明&apos;，&apos;小明&apos;是&apos;他&apos;的先行语。</span><br><span class=\"line\">句子二中，&apos;特朗普&apos;和&apos;美国总统&apos;均指代在美国白宫坐着的那个人类。</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>\n\n</p><h4 id=\"2-表述与实体\"><a href=\"#2-表述与实体\" class=\"headerlink\" title=\"2. 表述与实体\"></a><span id=\"mention\">2. 表述与实体</span></h4><p>实体（Entity）是存在于客观世界的某种事物，是一种切实存在的概念。文本中会出现很多的名词短语以及代词，这些句子成分被称作表述（Mention），是一个事物在文本中的一种体现。</p>\n<blockquote>\n<p>In coreference resolution, an entity is an object or set of objects in the world, while a mention is the textual reference to an entity (Doddington et al., 2004). </p>\n</blockquote>\n<p><img src=\"/2018/04/24/组会分享/mention_and_entity.png\" alt=\"image\"></p>\n<h4 id=\"3-任务描述\"><a href=\"#3-任务描述\" class=\"headerlink\" title=\"3. 任务描述\"></a><span id=\"mission\">3. 任务描述</span></h4><p>指代消解定义：为文中的表述确定其在真实世界中所指向实体的过程。</p>\n<blockquote>\n<p>Coreference resolution, the task of identifying which mentions in a text refer to the same real world entity, is fundamentally a clustering problem. </p>\n</blockquote>\n<p>研究步骤：</p>\n<ol>\n<li>Mention detection/表述提取——对于一段文本，抽取其中的表述。</li>\n<li>Coreference Resolution/指代消解——对上一步得到的表述进行分析，确定它们之间是否存在指代关系。</li>\n</ol>\n<h3 id=\"二-各类方法\"><a href=\"#二-各类方法\" class=\"headerlink\" title=\"二. 各类方法\"></a><span id=\"function\">二. 各类方法</span></h3><h4 id=\"1-表述提取\"><a href=\"#1-表述提取\" class=\"headerlink\" title=\"1. 表述提取\"></a><span id=\"md\">1. 表述提取</span></h4><ul>\n<li><p><span id=\"parse\">利用句法分析树</span></p>\n<p>  对输入的文本进行分词、词性标注等工作，以句子为单位构建句法树。对产生的句法树进行分析，获取其中的各个子树。选取所有的根节点为名词短语（或代词）的子树，抽取其中的词语组合成候选表述</p>\n<p>  输入：</p>\n<blockquote>\n<p>(TOP(IP(NP(QP0) (NP1) (NP2)) (VP(PP3 (IP(VP(ADVP4) (VP5 (NP6))))) (VP7 (NP8))) 9))</p>\n</blockquote>\n<p>  输出：</p>\n<blockquote>\n<p>0、1、01、5、45、8</p>\n</blockquote>\n<p>  <img src=\"/2018/04/24/组会分享/tree.png\" alt=\"image\"></p>\n</li>\n<li><p><span id=\"nn\">利用神经网络</span></p>\n<p>  对输入的文本进行分词工作，将每个词语作为一个基本单位进行保存。通过词嵌入（word embedding）构建包含更多词信息的词向量，然后输入双向LSTM网络获取对应的候选表述。（详见后文）</p>\n</li>\n</ul>\n<h4 id=\"2-消解\"><a href=\"#2-消解\" class=\"headerlink\" title=\"2. 消解\"></a><span id=\"cr\">2. 消解</span></h4><ul>\n<li><p><span id=\"language\">语言学方法</span></p>\n<ul>\n<li>Hobbs算法是最早的代词消解方法之一，主要依赖于句法解析树进行分析。</li>\n<li>Grosz提出并发展起来的中心理论主要针对篇章结构中的焦点转移以及话语一致性等问题，常用于代词消解研究。</li>\n<li>Raghunathan(2010)针对英文数据集构建了一种多轮迭代(MultiSieves)处理的方法，该方法根据准确率由高到低构建了多个筛子（sieve）用来为文本中的表述选取先行语。<a href=\"http://www.aclweb.org/anthology/D10-1048\" target=\"_blank\" rel=\"noopener\">Multi-sieves论文：http://www.aclweb.org/anthology/D10-1048</a></li>\n</ul>\n</li>\n<li><p><span id=\"ml\">机器学习方法</span></p>\n<ul>\n<li>常见的四类框架包括实体-表述模型、表述对模型、表述排序模型和实体排序模型。</li>\n<li>其中表述对模型是最常见的模型之一，该模型将消解问题看作是表述对之间的二分类问题，判断系统当前处理的两个表述之间是否存在着共指关系。</li>\n<li>人们在此基础上进行修改，提出了实体-表述模型和表述排序模型。</li>\n<li>为了组合实体-表述模型和表述排序模型的优势，提出了实体排序模型。</li>\n<li>关于表述排序模型(Mention-Ranking)可以参考：<a href=\"https://nlp.stanford.edu/pubs/clark2016improving.pdf\" target=\"_blank\" rel=\"noopener\">Improving Coreference Resolution by Learning Entity-Level Distributed</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"三-CR——MultiSieves\"><a href=\"#三-CR——MultiSieves\" class=\"headerlink\" title=\"三. CR——MultiSieves\"></a><span id=\"multisieve\">三. CR——MultiSieves</span></h3><h4 id=\"0-想法\"><a href=\"#0-想法\" class=\"headerlink\" title=\"0. 想法\"></a>0. <span id=\"idea\">想法</span></h4><p>通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。</p>\n<p>参考论文：</p>\n<ul>\n<li>Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.</li>\n<li>Lee H, Peirsman Y, Chang A, et al. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.</li>\n<li>Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.</li>\n<li>Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.</li>\n</ul>\n<h4 id=\"1-Mention-Detection\"><a href=\"#1-Mention-Detection\" class=\"headerlink\" title=\"1. Mention Detection\"></a>1. <span id=\"ms-md\">Mention Detection</span></h4><p>使用了Stanford的工具包对文本进行预处理</p>\n<ul>\n<li><a href=\"https://github.com/Lynten/stanford-corenlp\" target=\"_blank\" rel=\"noopener\">https://github.com/Lynten/stanford-corenlp</a></li>\n<li><a href=\"https://stanfordnlp.github.io/CoreNLP/#download\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/#download</a></li>\n<li><a href=\"https://stanfordnlp.github.io/CoreNLP/history.html\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/history.html</a></li>\n</ul>\n<p><strong>使用示例：</strong></p>\n<p><img src=\"/2018/04/24/组会分享/clipboard.png\" alt=\"image\"><br><img src=\"/2018/04/24/组会分享/clipboard_1.png\" alt=\"image\"><br><img src=\"/2018/04/24/组会分享/clipboard_2.png\" alt=\"image\"></p>\n<p>输入</p>\n<blockquote>\n<p>“小明很帅，同学们都很喜欢他”</p>\n</blockquote>\n<p>得到：</p>\n<blockquote>\n<p>明 小明 很帅 小明很帅 同学们 他</p>\n</blockquote>\n<h4 id=\"2-Coreference-Resolution\"><a href=\"#2-Coreference-Resolution\" class=\"headerlink\" title=\"2. Coreference Resolution\"></a>2. <span id=\"ms-cr\">Coreference Resolution</span></h4><ul>\n<li><p>主要想法：<br>通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。</p>\n</li>\n<li><p>参考论文：</p>\n<ul>\n<li>Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.</li>\n<li>Lee H, Peirsman Y, Chang A, et al. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.</li>\n<li>Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.</li>\n<li>Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.</li>\n</ul>\n</li>\n<li>常用的sieves  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. Mention Detection Sieve </span><br><span class=\"line\">2. Discourse Processing Sieve</span><br><span class=\"line\">3. Exact String Match Sieve </span><br><span class=\"line\">4. Relaxed String Match Sieve </span><br><span class=\"line\">5. Precise Constructs Sieve </span><br><span class=\"line\">6-8. Strict Head Matching Sieves A-C</span><br><span class=\"line\">9. Proper HeadWord Match Sieve </span><br><span class=\"line\">10. Alias Sieve </span><br><span class=\"line\">11. Relaxed Head Matching Sieve </span><br><span class=\"line\">12. Lexical Chain Sieve </span><br><span class=\"line\">13. Pronouns Sieve</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"4-CR——End2End\"><a href=\"#4-CR——End2End\" class=\"headerlink\" title=\"4. CR——End2End\"></a>4. <span id=\"end2end\">CR——End2End</span></h3><p>论文名城：End-to-end Neural Coreference Resolution</p>\n<h4 id=\"1-目标任务\"><a href=\"#1-目标任务\" class=\"headerlink\" title=\"1. 目标任务\"></a>1. <span id=\"e2e-mission\">目标任务</span></h4><p>对于指代消解的任务定义为：<br>给定一段文本，找到其中名词/代词，判断这些词语是否存在着指代关系。</p>\n<p>因此给定一个包含T个词语的文档D，通过某种方法从中得到N个span(即将这些span作为潜在的表述)。==论文目标是对于每个span 𝑖确定一个先行语y_𝑖==</p>\n<p>𝑦_𝑖可能的取值集合: 𝒴(𝑖)={𝜖,1,…,𝑖−1}<br>𝜖：表示当前span没有先行语</p>\n<p>其中span是由几个单词组成的一个单元，可以理解为一个短语/词组</p>\n<h4 id=\"2-论文方法\"><a href=\"#2-论文方法\" class=\"headerlink\" title=\"2. 论文方法\"></a>2. <span id=\"e2e-function\">论文方法</span></h4><p><img src=\"/2018/04/24/组会分享/clipboard_4.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>顺序</th>\n<th>任务</th>\n<th>得到</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>对于每个单词，使用300维GloVe embedding和50维Turian embedding构成向量，进行正则化为单元向量。对于每个字符，通过一个卷积网络得到一个8维向量进行表示</td>\n<td>word &amp; character embedding 𝒙</td>\n</tr>\n<tr>\n<td>2</td>\n<td>将上一步中的向量表示输入双向LSTM网络，对文本中每个词进行编码和计算，得到span 𝑖的边界的表示向量x_(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和x_(𝐸𝑁𝐷(𝑖))^∗</td>\n<td>𝒙<em>(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和𝒙</em>(𝐸𝑁𝐷(𝑖))^∗</td>\n</tr>\n<tr>\n<td>3</td>\n<td>利用attention机制求取每个候选span的中心词𝑥 ̂_𝑖</td>\n<td>span 𝑖 的中心词𝒙 ̂_𝒊</td>\n</tr>\n<tr>\n<td>4</td>\n<td>构建对于span的向量表示：𝑔<em>𝑖=[x</em>𝑆𝑇𝐴𝑅𝑇(𝑖)^ ∗,x_𝐸𝑁𝐷(𝑖)^∗,𝑥 ̂_𝑖,∅(𝑖)]</td>\n<td>span的向量表示𝑔_𝑖</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"/2018/04/24/组会分享/clipboard_5.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>顺序</th>\n<th>任务</th>\n<th>得到</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>对于每个span 𝑖，计算𝑠<em>𝑚，其中s</em>𝑚=𝜔<em>𝑚∙[𝐹𝐹𝑁𝑁]</em>𝑚 (𝑔<em>𝑖) FFNN为一个非线性映射的前馈神经网络。s</em>𝑚的意义在于对span空间进行剪枝，删去过多的表述。</td>\n<td>𝑠_𝑚</td>\n</tr>\n<tr>\n<td>2</td>\n<td>对于span 𝑖，𝑗，计算𝑠_a (𝑖,𝑗)，其中𝑠<em>a (𝑖,𝑗)=𝜔</em>𝑎∙[𝐹𝐹𝑁𝑁]<em>𝑎 ([𝑔</em>𝑖,𝑔<em>𝑗,𝑔</em>𝑖°𝑔<em>𝑗,∅(𝑖,𝑗)])，s</em>𝑎 的意义即span 𝑗是span 𝑖的先行词的score</td>\n<td>𝑠_a (𝑖,𝑗)</td>\n</tr>\n<tr>\n<td>3</td>\n<td>计算S(𝑖,𝑗)</td>\n<td>S(𝑖,𝑗)</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"小结—计算流程\"><a href=\"#小结—计算流程\" class=\"headerlink\" title=\"小结—计算流程\"></a>小结—计算流程</h4><ol>\n<li>word representation 𝒙</li>\n<li>LSTM 𝑥^∗</li>\n<li>head 𝑥 ̂_𝑖</li>\n<li>span representation 𝑔_𝑖</li>\n<li>Mention score s_𝑚(pruning)</li>\n<li>Antecedent score 𝑠_a (𝑖,𝑗)</li>\n<li>Coreference score s(𝑖,𝑗)</li>\n</ol>\n<h4 id=\"3-讨论\"><a href=\"#3-讨论\" class=\"headerlink\" title=\"3. 讨论\"></a>3. <span id=\"answer\">讨论</span></h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- 以前方法的缺陷</span><br><span class=\"line\">    在之前的各类方法中，大部分是依赖语法分析树和手工构建的各种特征。</span><br><span class=\"line\">    存在以下问题：</span><br><span class=\"line\">        1. 句法分析器的错误可能导致一连串的错误</span><br><span class=\"line\">        2. 许多手工构建的规则对于新的语言或任务不具有普适性</span><br><span class=\"line\">- 本论文不依赖于句法分析器，不依赖于手工构建的各类规则</span><br></pre></td></tr></table></figure>\n<p><span id=\"strength\"><strong>优点</strong></span></p>\n<ol>\n<li>本文的模型可以有效提取较长和复杂的词语作为潜在的表述</li>\n<li>Attention机制在确定指代关系的工作中有重要作用（通过查找中心词）</li>\n<li>由双向LSTM提供的上下文信息在确定指代关系中也起到了重要作用</li>\n</ol>\n<p><span id=\"weakness\"><strong>缺点</strong></span></p>\n<ol>\n<li>论文可以利用embeedding计算词语之间的相似度，这是基于传统的特征的系统所不能解决的。但是有时在embedding相似的情况下，但是语义并不相近。</li>\n<li>本模型缺少关于现实世界的先验知识</li>\n</ol>\n<p><span id=\"improved\"><strong>改进</strong></span></p>\n<ul>\n<li>将模型与外部世界的知识相结合</li>\n<li>用更大量的训练数据克服这些缺陷</li>\n</ul>\n<h3 id=\"5-可能有用的Urls\"><a href=\"#5-可能有用的Urls\" class=\"headerlink\" title=\"5. 可能有用的Urls\"></a>5. <span id=\"urls\">可能有用的Urls</span></h3><table>\n<thead>\n<tr>\n<th>网址</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/Lynten/stanford-coren\" target=\"_blank\" rel=\"noopener\">https://github.com/Lynten/stanford-coren</a></td>\n<td>Stanfordcorenlp项目地址</td>\n</tr>\n<tr>\n<td><a href=\"https://stanfordnlp.github.io/CoreNLP/#download\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/#download</a></td>\n<td>最新版CoreNLP模型下载地址</td>\n</tr>\n<tr>\n<td><a href=\"https://stanfordnlp.github.io/CoreNLP/history.html\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/history.html</a></td>\n<td>CoreNLP模型历史版本下载地址</td>\n</tr>\n<tr>\n<td><a href=\"http://allennlp.org/\" target=\"_blank\" rel=\"noopener\">http://allennlp.org/</a></td>\n<td>一个基于PyTorch的新的开源NLP研究库。 </td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/kentonl/e2e-coref\" target=\"_blank\" rel=\"noopener\">https://github.com/kentonl/e2e-coref</a></td>\n<td>End2End CR论文的项目地址</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"6-私货\"><a href=\"#6-私货\" class=\"headerlink\" title=\"6. 私货\"></a>6. <span id=\"recommend\">私货<span></span></span></h4><ul>\n<li><a href=\"https://weibo.com/52nlp\" target=\"_blank\" rel=\"noopener\">https://weibo.com/52nlp</a></li>\n<li><a href=\"https://weibo.com/u/1657470871\" target=\"_blank\" rel=\"noopener\">https://weibo.com/u/1657470871</a></li>\n<li><a href=\"https://weibo.com/paperweekly\" target=\"_blank\" rel=\"noopener\">https://weibo.com/paperweekly</a></li>\n<li><a href=\"https://weibo.com/fly51fly\" target=\"_blank\" rel=\"noopener\">https://weibo.com/fly51fly</a></li>\n<li><a href=\"https://weibo.com/jietangthu\" target=\"_blank\" rel=\"noopener\">https://weibo.com/jietangthu</a></li>\n<li><a href=\"https://weibo.com/u/1970879995\" target=\"_blank\" rel=\"noopener\">https://weibo.com/u/1970879995</a></li>\n<li><a href=\"https://weibo.com/zibuyu9\" target=\"_blank\" rel=\"noopener\">https://weibo.com/zibuyu9</a></li>\n<li><a href=\"https://weibo.com/clnlp\" target=\"_blank\" rel=\"noopener\">https://weibo.com/clnlp</a></li>\n<li><a href=\"https://weibo.com/nlpjob\" target=\"_blank\" rel=\"noopener\">https://weibo.com/nlpjob</a></li>\n<li><a href=\"https://weibo.com/arnetminer\" target=\"_blank\" rel=\"noopener\">https://weibo.com/arnetminer</a></li>\n<li><a href=\"http://blog.knownsec.com/Knownsec_RD_Checklist/index.html\" target=\"_blank\" rel=\"noopener\">http://blog.knownsec.com/Knownsec_RD_Checklist/index.html</a></li>\n<li><a href=\"https://github.com/dongxiexidian/Chinese\" target=\"_blank\" rel=\"noopener\">https://github.com/dongxiexidian/Chinese</a></li>\n<li><a href=\"https://github.com/justjavac/free-programming-books-zh_CN\" target=\"_blank\" rel=\"noopener\">https://github.com/justjavac/free-programming-books-zh_CN</a></li>\n<li><a href=\"https://www.jianshu.com/u/ZQtGe6\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/u/ZQtGe6</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h2><ol>\n<li><a href=\"#gainian\">一些概念</a><ol>\n<li><a href=\"#01\">指代概念</a></li>\n<li><a href=\"#mention\">表述与实体</a></li>\n<li><a href=\"#mission\">消解任务</a></li>\n</ol>\n</li>\n<li><a href=\"#function\">各类方法</a><ol>\n<li><a href=\"#md\">表述提取</a><ul>\n<li><a href=\"#parse\">利用句法树</a></li>\n<li><a href=\"#nn\">利用神经网络</a></li>\n</ul>\n</li>\n<li><a href=\"#cr\">消解</a><ul>\n<li><a href=\"#language\">语言学</a></li>\n<li><a href=\"#ml\">机器学习</a></li>\n</ul>\n</li>\n</ol>\n</li>\n<li><a href=\"#multisieve\">CR——Multi sieve</a><ol>\n<li><a href=\"#idea\">想法</a></li>\n<li><a href=\"#ms-md\">表述提取</a></li>\n<li><a href=\"#ms-cr/\">消解</a></li>\n</ol>\n</li>\n<li><a href=\"#end2end\">CR——End2End</a><ol>\n<li><a href=\"#e2e-mission\">目标任务</a></li>\n<li><a href=\"#e2e-function\">论文方法</a></li>\n<li><a href=\"#answer\">讨论</a><ul>\n<li><a href=\"#strength\">优点</a></li>\n<li><a href=\"#weakness\">缺点</a></li>\n<li><a href=\"#improved\">改进</a></li>\n</ul>\n</li>\n<li><a href=\"#experiment\">实验</a></li>\n</ol>\n</li>\n<li><a href=\"#url\">可能有用的网址</a></li>\n<li><a href=\"#recommend\">私货</a></li>\n</ol>\n<hr>\n<h3 id=\"一、-一些概念\"><a href=\"#一、-一些概念\" class=\"headerlink\" title=\"一、 一些概念 \"></a><span id=\"gainian\">一、 一些概念 </span></h3><h4 id=\"1-指代概念\"><a href=\"#1-指代概念\" class=\"headerlink\" title=\"1. 指代概念\"></a><span id=\"01\">1. 指代概念</span></h4><p>指代（coreference）为语言学中为了避免已经出现的字词重复出现在文章的句子上，导致语句结构过于赘述和语意不够清晰，所以使用代词（pronouns）或是普通名词（common nouns）来代替已经出现过的字词。</p>\n<p>主要分为两类：<strong>回指</strong>和<strong>共指</strong></p>\n<ul>\n<li>回指：指句子中出现的短语存在着密切的语义关联性，指代同一事物。这种关系依赖上下文语义。其中被指向的语言单位被称为先行语</li>\n<li><p>共指：指句子中出现的短语指向真实世界的同一参照体，这种指代脱离上下文仍然成立。</p>\n  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">例如：</span><br><span class=\"line\">回指：“小明很帅，同学们都很喜欢他。”</span><br><span class=\"line\">共指：“特朗普就任总统以来第一次访问中国，他也将成为历史上第8位在任时访华的美国总统。”</span><br><span class=\"line\"></span><br><span class=\"line\">句子一中，&apos;他&apos;指代&apos;小明&apos;，&apos;小明&apos;是&apos;他&apos;的先行语。</span><br><span class=\"line\">句子二中，&apos;特朗普&apos;和&apos;美国总统&apos;均指代在美国白宫坐着的那个人类。</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<p>\n\n</p><h4 id=\"2-表述与实体\"><a href=\"#2-表述与实体\" class=\"headerlink\" title=\"2. 表述与实体\"></a><span id=\"mention\">2. 表述与实体</span></h4><p>实体（Entity）是存在于客观世界的某种事物，是一种切实存在的概念。文本中会出现很多的名词短语以及代词，这些句子成分被称作表述（Mention），是一个事物在文本中的一种体现。</p>\n<blockquote>\n<p>In coreference resolution, an entity is an object or set of objects in the world, while a mention is the textual reference to an entity (Doddington et al., 2004). </p>\n</blockquote>\n<p><img src=\"/2018/04/24/组会分享/mention_and_entity.png\" alt=\"image\"></p>\n<h4 id=\"3-任务描述\"><a href=\"#3-任务描述\" class=\"headerlink\" title=\"3. 任务描述\"></a><span id=\"mission\">3. 任务描述</span></h4><p>指代消解定义：为文中的表述确定其在真实世界中所指向实体的过程。</p>\n<blockquote>\n<p>Coreference resolution, the task of identifying which mentions in a text refer to the same real world entity, is fundamentally a clustering problem. </p>\n</blockquote>\n<p>研究步骤：</p>\n<ol>\n<li>Mention detection/表述提取——对于一段文本，抽取其中的表述。</li>\n<li>Coreference Resolution/指代消解——对上一步得到的表述进行分析，确定它们之间是否存在指代关系。</li>\n</ol>\n<h3 id=\"二-各类方法\"><a href=\"#二-各类方法\" class=\"headerlink\" title=\"二. 各类方法\"></a><span id=\"function\">二. 各类方法</span></h3><h4 id=\"1-表述提取\"><a href=\"#1-表述提取\" class=\"headerlink\" title=\"1. 表述提取\"></a><span id=\"md\">1. 表述提取</span></h4><ul>\n<li><p><span id=\"parse\">利用句法分析树</span></p>\n<p>  对输入的文本进行分词、词性标注等工作，以句子为单位构建句法树。对产生的句法树进行分析，获取其中的各个子树。选取所有的根节点为名词短语（或代词）的子树，抽取其中的词语组合成候选表述</p>\n<p>  输入：</p>\n<blockquote>\n<p>(TOP(IP(NP(QP0) (NP1) (NP2)) (VP(PP3 (IP(VP(ADVP4) (VP5 (NP6))))) (VP7 (NP8))) 9))</p>\n</blockquote>\n<p>  输出：</p>\n<blockquote>\n<p>0、1、01、5、45、8</p>\n</blockquote>\n<p>  <img src=\"/2018/04/24/组会分享/tree.png\" alt=\"image\"></p>\n</li>\n<li><p><span id=\"nn\">利用神经网络</span></p>\n<p>  对输入的文本进行分词工作，将每个词语作为一个基本单位进行保存。通过词嵌入（word embedding）构建包含更多词信息的词向量，然后输入双向LSTM网络获取对应的候选表述。（详见后文）</p>\n</li>\n</ul>\n<h4 id=\"2-消解\"><a href=\"#2-消解\" class=\"headerlink\" title=\"2. 消解\"></a><span id=\"cr\">2. 消解</span></h4><ul>\n<li><p><span id=\"language\">语言学方法</span></p>\n<ul>\n<li>Hobbs算法是最早的代词消解方法之一，主要依赖于句法解析树进行分析。</li>\n<li>Grosz提出并发展起来的中心理论主要针对篇章结构中的焦点转移以及话语一致性等问题，常用于代词消解研究。</li>\n<li>Raghunathan(2010)针对英文数据集构建了一种多轮迭代(MultiSieves)处理的方法，该方法根据准确率由高到低构建了多个筛子（sieve）用来为文本中的表述选取先行语。<a href=\"http://www.aclweb.org/anthology/D10-1048\" target=\"_blank\" rel=\"noopener\">Multi-sieves论文：http://www.aclweb.org/anthology/D10-1048</a></li>\n</ul>\n</li>\n<li><p><span id=\"ml\">机器学习方法</span></p>\n<ul>\n<li>常见的四类框架包括实体-表述模型、表述对模型、表述排序模型和实体排序模型。</li>\n<li>其中表述对模型是最常见的模型之一，该模型将消解问题看作是表述对之间的二分类问题，判断系统当前处理的两个表述之间是否存在着共指关系。</li>\n<li>人们在此基础上进行修改，提出了实体-表述模型和表述排序模型。</li>\n<li>为了组合实体-表述模型和表述排序模型的优势，提出了实体排序模型。</li>\n<li>关于表述排序模型(Mention-Ranking)可以参考：<a href=\"https://nlp.stanford.edu/pubs/clark2016improving.pdf\" target=\"_blank\" rel=\"noopener\">Improving Coreference Resolution by Learning Entity-Level Distributed</a></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"三-CR——MultiSieves\"><a href=\"#三-CR——MultiSieves\" class=\"headerlink\" title=\"三. CR——MultiSieves\"></a><span id=\"multisieve\">三. CR——MultiSieves</span></h3><h4 id=\"0-想法\"><a href=\"#0-想法\" class=\"headerlink\" title=\"0. 想法\"></a>0. <span id=\"idea\">想法</span></h4><p>通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。</p>\n<p>参考论文：</p>\n<ul>\n<li>Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.</li>\n<li>Lee H, Peirsman Y, Chang A, et al. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.</li>\n<li>Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.</li>\n<li>Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.</li>\n</ul>\n<h4 id=\"1-Mention-Detection\"><a href=\"#1-Mention-Detection\" class=\"headerlink\" title=\"1. Mention Detection\"></a>1. <span id=\"ms-md\">Mention Detection</span></h4><p>使用了Stanford的工具包对文本进行预处理</p>\n<ul>\n<li><a href=\"https://github.com/Lynten/stanford-corenlp\" target=\"_blank\" rel=\"noopener\">https://github.com/Lynten/stanford-corenlp</a></li>\n<li><a href=\"https://stanfordnlp.github.io/CoreNLP/#download\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/#download</a></li>\n<li><a href=\"https://stanfordnlp.github.io/CoreNLP/history.html\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/history.html</a></li>\n</ul>\n<p><strong>使用示例：</strong></p>\n<p><img src=\"/2018/04/24/组会分享/clipboard.png\" alt=\"image\"><br><img src=\"/2018/04/24/组会分享/clipboard_1.png\" alt=\"image\"><br><img src=\"/2018/04/24/组会分享/clipboard_2.png\" alt=\"image\"></p>\n<p>输入</p>\n<blockquote>\n<p>“小明很帅，同学们都很喜欢他”</p>\n</blockquote>\n<p>得到：</p>\n<blockquote>\n<p>明 小明 很帅 小明很帅 同学们 他</p>\n</blockquote>\n<h4 id=\"2-Coreference-Resolution\"><a href=\"#2-Coreference-Resolution\" class=\"headerlink\" title=\"2. Coreference Resolution\"></a>2. <span id=\"ms-cr\">Coreference Resolution</span></h4><ul>\n<li><p>主要想法：<br>通过某种方法从文本中获取表述，对这些表述进行多轮(sieve)处理，每一轮针对文本不同的特性，处理自己能识别出存在指代关系的表述，然后再将处理的结果传递给下一轮，通过多轮处理后，输出最终的消解结果。这种方案的优势在于构建了一个消解平台，后人可以在基础上很方便的增删改。</p>\n</li>\n<li><p>参考论文：</p>\n<ul>\n<li>Raghunathan K, Lee H, Rangarajan S, et al. A Multi-Pass Sieve for Coreference Resolution.[C]// Conference on Empirical Methods in Natural Language Processing, EMNLP 2010, 9-11 October 2010, Mit Stata Center, Massachusetts, Usa, A Meeting of Sigdat, A Special Interest Group of the ACL. DBLP, 2011:492-501.</li>\n<li>Lee H, Peirsman Y, Chang A, et al. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task[C]// Fifteenth Conference on Computational Natural Language Learning: Shared Task. 2012:28-34.</li>\n<li>Chen C, Ng V. Combining the best of two worlds: a hybrid approach to multilingual coreference resolution[C]// Joint Conference on EMNLP and CoNLL - Shared Task. Association for Computational Linguistics, 2012:56-63.</li>\n<li>Chen C, Ng V. Chinese Noun Phrase Coreference Resolution: Insights into the State of the Art[C]// COLING 2012: Posters. 2013:185-194.</li>\n</ul>\n</li>\n<li>常用的sieves  <figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1. Mention Detection Sieve </span><br><span class=\"line\">2. Discourse Processing Sieve</span><br><span class=\"line\">3. Exact String Match Sieve </span><br><span class=\"line\">4. Relaxed String Match Sieve </span><br><span class=\"line\">5. Precise Constructs Sieve </span><br><span class=\"line\">6-8. Strict Head Matching Sieves A-C</span><br><span class=\"line\">9. Proper HeadWord Match Sieve </span><br><span class=\"line\">10. Alias Sieve </span><br><span class=\"line\">11. Relaxed Head Matching Sieve </span><br><span class=\"line\">12. Lexical Chain Sieve </span><br><span class=\"line\">13. Pronouns Sieve</span><br></pre></td></tr></table></figure>\n</li>\n</ul>\n<h3 id=\"4-CR——End2End\"><a href=\"#4-CR——End2End\" class=\"headerlink\" title=\"4. CR——End2End\"></a>4. <span id=\"end2end\">CR——End2End</span></h3><p>论文名城：End-to-end Neural Coreference Resolution</p>\n<h4 id=\"1-目标任务\"><a href=\"#1-目标任务\" class=\"headerlink\" title=\"1. 目标任务\"></a>1. <span id=\"e2e-mission\">目标任务</span></h4><p>对于指代消解的任务定义为：<br>给定一段文本，找到其中名词/代词，判断这些词语是否存在着指代关系。</p>\n<p>因此给定一个包含T个词语的文档D，通过某种方法从中得到N个span(即将这些span作为潜在的表述)。==论文目标是对于每个span 𝑖确定一个先行语y_𝑖==</p>\n<p>𝑦_𝑖可能的取值集合: 𝒴(𝑖)={𝜖,1,…,𝑖−1}<br>𝜖：表示当前span没有先行语</p>\n<p>其中span是由几个单词组成的一个单元，可以理解为一个短语/词组</p>\n<h4 id=\"2-论文方法\"><a href=\"#2-论文方法\" class=\"headerlink\" title=\"2. 论文方法\"></a>2. <span id=\"e2e-function\">论文方法</span></h4><p><img src=\"/2018/04/24/组会分享/clipboard_4.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>顺序</th>\n<th>任务</th>\n<th>得到</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>对于每个单词，使用300维GloVe embedding和50维Turian embedding构成向量，进行正则化为单元向量。对于每个字符，通过一个卷积网络得到一个8维向量进行表示</td>\n<td>word &amp; character embedding 𝒙</td>\n</tr>\n<tr>\n<td>2</td>\n<td>将上一步中的向量表示输入双向LSTM网络，对文本中每个词进行编码和计算，得到span 𝑖的边界的表示向量x_(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和x_(𝐸𝑁𝐷(𝑖))^∗</td>\n<td>𝒙<em>(𝑆𝑇𝐴𝑅𝑇(𝑖))^∗和𝒙</em>(𝐸𝑁𝐷(𝑖))^∗</td>\n</tr>\n<tr>\n<td>3</td>\n<td>利用attention机制求取每个候选span的中心词𝑥 ̂_𝑖</td>\n<td>span 𝑖 的中心词𝒙 ̂_𝒊</td>\n</tr>\n<tr>\n<td>4</td>\n<td>构建对于span的向量表示：𝑔<em>𝑖=[x</em>𝑆𝑇𝐴𝑅𝑇(𝑖)^ ∗,x_𝐸𝑁𝐷(𝑖)^∗,𝑥 ̂_𝑖,∅(𝑖)]</td>\n<td>span的向量表示𝑔_𝑖</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"/2018/04/24/组会分享/clipboard_5.png\" alt=\"image\"></p>\n<table>\n<thead>\n<tr>\n<th>顺序</th>\n<th>任务</th>\n<th>得到</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>对于每个span 𝑖，计算𝑠<em>𝑚，其中s</em>𝑚=𝜔<em>𝑚∙[𝐹𝐹𝑁𝑁]</em>𝑚 (𝑔<em>𝑖) FFNN为一个非线性映射的前馈神经网络。s</em>𝑚的意义在于对span空间进行剪枝，删去过多的表述。</td>\n<td>𝑠_𝑚</td>\n</tr>\n<tr>\n<td>2</td>\n<td>对于span 𝑖，𝑗，计算𝑠_a (𝑖,𝑗)，其中𝑠<em>a (𝑖,𝑗)=𝜔</em>𝑎∙[𝐹𝐹𝑁𝑁]<em>𝑎 ([𝑔</em>𝑖,𝑔<em>𝑗,𝑔</em>𝑖°𝑔<em>𝑗,∅(𝑖,𝑗)])，s</em>𝑎 的意义即span 𝑗是span 𝑖的先行词的score</td>\n<td>𝑠_a (𝑖,𝑗)</td>\n</tr>\n<tr>\n<td>3</td>\n<td>计算S(𝑖,𝑗)</td>\n<td>S(𝑖,𝑗)</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"小结—计算流程\"><a href=\"#小结—计算流程\" class=\"headerlink\" title=\"小结—计算流程\"></a>小结—计算流程</h4><ol>\n<li>word representation 𝒙</li>\n<li>LSTM 𝑥^∗</li>\n<li>head 𝑥 ̂_𝑖</li>\n<li>span representation 𝑔_𝑖</li>\n<li>Mention score s_𝑚(pruning)</li>\n<li>Antecedent score 𝑠_a (𝑖,𝑗)</li>\n<li>Coreference score s(𝑖,𝑗)</li>\n</ol>\n<h4 id=\"3-讨论\"><a href=\"#3-讨论\" class=\"headerlink\" title=\"3. 讨论\"></a>3. <span id=\"answer\">讨论</span></h4><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">- 以前方法的缺陷</span><br><span class=\"line\">    在之前的各类方法中，大部分是依赖语法分析树和手工构建的各种特征。</span><br><span class=\"line\">    存在以下问题：</span><br><span class=\"line\">        1. 句法分析器的错误可能导致一连串的错误</span><br><span class=\"line\">        2. 许多手工构建的规则对于新的语言或任务不具有普适性</span><br><span class=\"line\">- 本论文不依赖于句法分析器，不依赖于手工构建的各类规则</span><br></pre></td></tr></table></figure>\n<p><span id=\"strength\"><strong>优点</strong></span></p>\n<ol>\n<li>本文的模型可以有效提取较长和复杂的词语作为潜在的表述</li>\n<li>Attention机制在确定指代关系的工作中有重要作用（通过查找中心词）</li>\n<li>由双向LSTM提供的上下文信息在确定指代关系中也起到了重要作用</li>\n</ol>\n<p><span id=\"weakness\"><strong>缺点</strong></span></p>\n<ol>\n<li>论文可以利用embeedding计算词语之间的相似度，这是基于传统的特征的系统所不能解决的。但是有时在embedding相似的情况下，但是语义并不相近。</li>\n<li>本模型缺少关于现实世界的先验知识</li>\n</ol>\n<p><span id=\"improved\"><strong>改进</strong></span></p>\n<ul>\n<li>将模型与外部世界的知识相结合</li>\n<li>用更大量的训练数据克服这些缺陷</li>\n</ul>\n<h3 id=\"5-可能有用的Urls\"><a href=\"#5-可能有用的Urls\" class=\"headerlink\" title=\"5. 可能有用的Urls\"></a>5. <span id=\"urls\">可能有用的Urls</span></h3><table>\n<thead>\n<tr>\n<th>网址</th>\n<th>备注</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"https://github.com/Lynten/stanford-coren\" target=\"_blank\" rel=\"noopener\">https://github.com/Lynten/stanford-coren</a></td>\n<td>Stanfordcorenlp项目地址</td>\n</tr>\n<tr>\n<td><a href=\"https://stanfordnlp.github.io/CoreNLP/#download\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/#download</a></td>\n<td>最新版CoreNLP模型下载地址</td>\n</tr>\n<tr>\n<td><a href=\"https://stanfordnlp.github.io/CoreNLP/history.html\" target=\"_blank\" rel=\"noopener\">https://stanfordnlp.github.io/CoreNLP/history.html</a></td>\n<td>CoreNLP模型历史版本下载地址</td>\n</tr>\n<tr>\n<td><a href=\"http://allennlp.org/\" target=\"_blank\" rel=\"noopener\">http://allennlp.org/</a></td>\n<td>一个基于PyTorch的新的开源NLP研究库。 </td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/kentonl/e2e-coref\" target=\"_blank\" rel=\"noopener\">https://github.com/kentonl/e2e-coref</a></td>\n<td>End2End CR论文的项目地址</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"6-私货\"><a href=\"#6-私货\" class=\"headerlink\" title=\"6. 私货\"></a>6. <span id=\"recommend\">私货<span></span></span></h4><ul>\n<li><a href=\"https://weibo.com/52nlp\" target=\"_blank\" rel=\"noopener\">https://weibo.com/52nlp</a></li>\n<li><a href=\"https://weibo.com/u/1657470871\" target=\"_blank\" rel=\"noopener\">https://weibo.com/u/1657470871</a></li>\n<li><a href=\"https://weibo.com/paperweekly\" target=\"_blank\" rel=\"noopener\">https://weibo.com/paperweekly</a></li>\n<li><a href=\"https://weibo.com/fly51fly\" target=\"_blank\" rel=\"noopener\">https://weibo.com/fly51fly</a></li>\n<li><a href=\"https://weibo.com/jietangthu\" target=\"_blank\" rel=\"noopener\">https://weibo.com/jietangthu</a></li>\n<li><a href=\"https://weibo.com/u/1970879995\" target=\"_blank\" rel=\"noopener\">https://weibo.com/u/1970879995</a></li>\n<li><a href=\"https://weibo.com/zibuyu9\" target=\"_blank\" rel=\"noopener\">https://weibo.com/zibuyu9</a></li>\n<li><a href=\"https://weibo.com/clnlp\" target=\"_blank\" rel=\"noopener\">https://weibo.com/clnlp</a></li>\n<li><a href=\"https://weibo.com/nlpjob\" target=\"_blank\" rel=\"noopener\">https://weibo.com/nlpjob</a></li>\n<li><a href=\"https://weibo.com/arnetminer\" target=\"_blank\" rel=\"noopener\">https://weibo.com/arnetminer</a></li>\n<li><a href=\"http://blog.knownsec.com/Knownsec_RD_Checklist/index.html\" target=\"_blank\" rel=\"noopener\">http://blog.knownsec.com/Knownsec_RD_Checklist/index.html</a></li>\n<li><a href=\"https://github.com/dongxiexidian/Chinese\" target=\"_blank\" rel=\"noopener\">https://github.com/dongxiexidian/Chinese</a></li>\n<li><a href=\"https://github.com/justjavac/free-programming-books-zh_CN\" target=\"_blank\" rel=\"noopener\">https://github.com/justjavac/free-programming-books-zh_CN</a></li>\n<li><a href=\"https://www.jianshu.com/u/ZQtGe6\" target=\"_blank\" rel=\"noopener\">https://www.jianshu.com/u/ZQtGe6</a></li>\n</ul>\n"}],"PostAsset":[{"_id":"source/_posts/Baidu_tieba_spider/psb.png","slug":"psb.png","post":"cjr7ak2lx0003scftus1voevv","modified":1,"renderable":0},{"_id":"source/_posts/algorithm-learning-sort/swap-sort-1.jpg","slug":"swap-sort-1.jpg","post":"cjr7ak2nm0018scftufbzcgjs","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/3.png","slug":"3.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/组会分享/clipboard_4.png","slug":"clipboard_4.png","post":"cjr7ak2od001gscftxounljbz","modified":1,"renderable":0},{"_id":"source/_posts/cnn/3-4.png","slug":"3-4.png","post":"cjr7ak2nn001ascftyuzo6uof","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/6.png","slug":"6.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/tree/1.jpg","slug":"1.jpg","post":"cjr7ak2mq000gscft86om5msh","modified":1,"renderable":0},{"_id":"source/_posts/how-to-use-python-to-build-markdown-calc/1.png","post":"cjr7ak2mj000ascftxer5g1ay","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/how-to-use-python-to-build-markdown-calc/2.JPG","post":"cjr7ak2mj000ascftxer5g1ay","slug":"2.JPG","modified":1,"renderable":1},{"_id":"source/_posts/Baidu_tieba_spider/1.png","post":"cjr7ak2lx0003scftus1voevv","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/Baidu_tieba_spider/psb2.jpg","post":"cjr7ak2lx0003scftus1voevv","slug":"psb2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/learning-tensorflow-linear-regression/1.png","post":"cjr7ak2ml000bscftn84wewas","slug":"1.png","modified":1,"renderable":1},{"_id":"source/_posts/learning-tensorflow-linear-regression/2.JPG","post":"cjr7ak2ml000bscftn84wewas","slug":"2.JPG","modified":1,"renderable":1},{"_id":"source/_posts/learning-tensorflow-linear-regression/3.png","post":"cjr7ak2ml000bscftn84wewas","slug":"3.png","modified":1,"renderable":1},{"_id":"source/_posts/end2end-coreference-resolution/10.png","slug":"10.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/2.png","slug":"2.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/4.png","slug":"4.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/5.png","slug":"5.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/7.png","slug":"7.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/8.png","slug":"8.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/end2end-coreference-resolution/9.png","slug":"9.png","post":"cjr7ak2md0007scft71io7szx","modified":1,"renderable":0},{"_id":"source/_posts/2018-work-schedule/1.JPG","post":"cjr7ak2nj0017scft1j6zn1zj","slug":"1.JPG","modified":1,"renderable":1},{"_id":"source/_posts/cnn/data.JPG","post":"cjr7ak2nn001ascftyuzo6uof","slug":"data.JPG","modified":1,"renderable":1},{"_id":"source/_posts/cnn/vocab.JPG","post":"cjr7ak2nn001ascftyuzo6uof","slug":"vocab.JPG","modified":1,"renderable":1},{"_id":"source/_posts/algorithm-learning-sort/1.JPG","post":"cjr7ak2nm0018scftufbzcgjs","slug":"1.JPG","modified":1,"renderable":1},{"_id":"source/_posts/algorithm-learning-sort/insert-sort-1.jpg","post":"cjr7ak2nm0018scftufbzcgjs","slug":"insert-sort-1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/algorithm-learning-sort/insert-sort-2.jpg","post":"cjr7ak2nm0018scftufbzcgjs","slug":"insert-sort-2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/algorithm-learning-sort/select-sort-1.jpg","post":"cjr7ak2nm0018scftufbzcgjs","slug":"select-sort-1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/algorithm-learning-sort/select-sort-2.jpg","post":"cjr7ak2nm0018scftufbzcgjs","slug":"select-sort-2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/组会分享/clipboard.png","post":"cjr7ak2od001gscftxounljbz","slug":"clipboard.png","modified":1,"renderable":1},{"_id":"source/_posts/组会分享/clipboard_1.png","post":"cjr7ak2od001gscftxounljbz","slug":"clipboard_1.png","modified":1,"renderable":1},{"_id":"source/_posts/组会分享/clipboard_2.png","post":"cjr7ak2od001gscftxounljbz","slug":"clipboard_2.png","modified":1,"renderable":1},{"_id":"source/_posts/组会分享/clipboard_3.png","post":"cjr7ak2od001gscftxounljbz","slug":"clipboard_3.png","modified":1,"renderable":1},{"_id":"source/_posts/组会分享/clipboard_5.png","post":"cjr7ak2od001gscftxounljbz","slug":"clipboard_5.png","modified":1,"renderable":1},{"_id":"source/_posts/组会分享/mention_and_entity.png","slug":"mention_and_entity.png","post":"cjr7ak2od001gscftxounljbz","modified":1,"renderable":0},{"_id":"source/_posts/组会分享/tree.png","post":"cjr7ak2od001gscftxounljbz","slug":"tree.png","modified":1,"renderable":1}],"PostCategory":[],"PostTag":[{"post_id":"cjr7ak2lq0001scftgnf7z2vm","tag_id":"cjr7ak2m10004scftcqhwh4gf","_id":"cjr7ak2mj0009scft2b5hqj2c"},{"post_id":"cjr7ak2lx0003scftus1voevv","tag_id":"cjr7ak2mf0008scftv16hsaej","_id":"cjr7ak2mr000hscfth1ju7ph0"},{"post_id":"cjr7ak2lx0003scftus1voevv","tag_id":"cjr7ak2mm000cscft80t5j6gi","_id":"cjr7ak2ms000jscftebmbvk2y"},{"post_id":"cjr7ak2mr000iscft401prcul","tag_id":"cjr7ak2m10004scftcqhwh4gf","_id":"cjr7ak2mt000lscftct0kz1r7"},{"post_id":"cjr7ak2m30005scftitmw86lh","tag_id":"cjr7ak2mp000fscft1umu5dta","_id":"cjr7ak2mu000nscfth5331imk"},{"post_id":"cjr7ak2m30005scftitmw86lh","tag_id":"cjr7ak2mt000kscftymxt0t4t","_id":"cjr7ak2mu000oscftk9c9298s"},{"post_id":"cjr7ak2m70006scft5ac1dv8q","tag_id":"cjr7ak2mu000mscft89vjh3k7","_id":"cjr7ak2mx000rscftx253o73m"},{"post_id":"cjr7ak2m70006scft5ac1dv8q","tag_id":"cjr7ak2mu000pscftpezpp1n8","_id":"cjr7ak2my000sscftnsqvv3kj"},{"post_id":"cjr7ak2md0007scft71io7szx","tag_id":"cjr7ak2mw000qscft4yo82dee","_id":"cjr7ak2n3000vscftzdfr5581"},{"post_id":"cjr7ak2md0007scft71io7szx","tag_id":"cjr7ak2mt000kscftymxt0t4t","_id":"cjr7ak2n3000wscftlok4w7wf"},{"post_id":"cjr7ak2mj000ascftxer5g1ay","tag_id":"cjr7ak2m10004scftcqhwh4gf","_id":"cjr7ak2n4000yscft4ksy5g70"},{"post_id":"cjr7ak2mj000ascftxer5g1ay","tag_id":"cjr7ak2n2000uscftz55a4ym9","_id":"cjr7ak2n4000zscfthh3uvgje"},{"post_id":"cjr7ak2ml000bscftn84wewas","tag_id":"cjr7ak2n4000xscft08cimflt","_id":"cjr7ak2n50011scfto8ftar7i"},{"post_id":"cjr7ak2mn000dscftpr1nxn7i","tag_id":"cjr7ak2n2000uscftz55a4ym9","_id":"cjr7ak2n60013scftfus9l2ha"},{"post_id":"cjr7ak2mq000gscft86om5msh","tag_id":"cjr7ak2n2000uscftz55a4ym9","_id":"cjr7ak2n80015scftb04kzu0w"},{"post_id":"cjr7ak2mq000gscft86om5msh","tag_id":"cjr7ak2n70014scftcgqz7oi7","_id":"cjr7ak2n80016scftum8m9osp"},{"post_id":"cjr7ak2nj0017scft1j6zn1zj","tag_id":"cjr7ak2m10004scftcqhwh4gf","_id":"cjr7ak2nn0019scftnps5tcfe"},{"post_id":"cjr7ak2nm0018scftufbzcgjs","tag_id":"cjr7ak2n70014scftcgqz7oi7","_id":"cjr7ak2nv001bscft0bpecy9t"},{"post_id":"cjr7ak2nm0018scftufbzcgjs","tag_id":"cjr7ak2n2000uscftz55a4ym9","_id":"cjr7ak2o5001dscftzseznma6"},{"post_id":"cjr7ak2nn001ascftyuzo6uof","tag_id":"cjr7ak2n2000uscftz55a4ym9","_id":"cjr7ak2oc001fscft1s5nuztw"},{"post_id":"cjr7ak2nn001ascftyuzo6uof","tag_id":"cjr7ak2n4000xscft08cimflt","_id":"cjr7ak2ol001hscft4r3f3mcl"},{"post_id":"cjr7ak2nn001ascftyuzo6uof","tag_id":"cjr7ak2mt000kscftymxt0t4t","_id":"cjr7ak2oq001iscfthvoy9t7v"},{"post_id":"cjr7ak2nw001cscftsk0rezd7","tag_id":"cjr7ak2mw000qscft4yo82dee","_id":"cjr7ak2or001jscftirsaavk7"},{"post_id":"cjr7ak2nw001cscftsk0rezd7","tag_id":"cjr7ak2mt000kscftymxt0t4t","_id":"cjr7ak2or001kscfth7t5zx8f"},{"post_id":"cjr7ak2o6001escftou26fis5","tag_id":"cjr7ak2mw000qscft4yo82dee","_id":"cjr7ak2or001lscftrzehck2d"},{"post_id":"cjr7ak2o6001escftou26fis5","tag_id":"cjr7ak2mt000kscftymxt0t4t","_id":"cjr7ak2or001mscftiqjpavd9"},{"post_id":"cjr7ak2od001gscftxounljbz","tag_id":"cjr7ak2mw000qscft4yo82dee","_id":"cjr7ak2or001nscftkhvgricv"},{"post_id":"cjr7ak2od001gscftxounljbz","tag_id":"cjr7ak2mt000kscftymxt0t4t","_id":"cjr7ak2or001oscftgel3phab"}],"Tag":[{"name":"随笔","_id":"cjr7ak2m10004scftcqhwh4gf"},{"name":"爬虫","_id":"cjr7ak2mf0008scftv16hsaej"},{"name":"学习","_id":"cjr7ak2mm000cscft80t5j6gi"},{"name":"CoreNLP","_id":"cjr7ak2mp000fscft1umu5dta"},{"name":"NLP","_id":"cjr7ak2mt000kscftymxt0t4t"},{"name":"笔记","_id":"cjr7ak2mu000mscft89vjh3k7"},{"name":"Java","_id":"cjr7ak2mu000pscftpezpp1n8"},{"name":"CR","_id":"cjr7ak2mw000qscft4yo82dee"},{"name":"python","_id":"cjr7ak2n2000uscftz55a4ym9"},{"name":"Tensorflow","_id":"cjr7ak2n4000xscft08cimflt"},{"name":"Algorithm","_id":"cjr7ak2n70014scftcgqz7oi7"}]}}